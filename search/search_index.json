{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Supercomputers for Starters \u00b6 Some preliminary words Introduction Goals Why supercomputing What it is not A compartmentalised supercomputer Overview of the notes Processors for supercomputers The memory hierarchy Storing data on supercomputers Introduction Problemms with a parallel disk setup Middleware What can we expect? Accelerators What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading Conclusions","title":"Home"},{"location":"#supercomputers-for-starters","text":"Some preliminary words Introduction Goals Why supercomputing What it is not A compartmentalised supercomputer Overview of the notes Processors for supercomputers The memory hierarchy Storing data on supercomputers Introduction Problemms with a parallel disk setup Middleware What can we expect? Accelerators What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading Conclusions","title":"Supercomputers for Starters"},{"location":"0_00_preliminary/","text":"Some preliminary words \u00b6 These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum .","title":"Preliminary words"},{"location":"0_00_preliminary/#some-preliminary-words","text":"These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum .","title":"Some preliminary words"},{"location":"1_Introduction/","text":"Introduction \u00b6 Goals Why supercomputing? What it is not A compartmentalised supercomputer Overview of the notes","title":"Introduction"},{"location":"1_Introduction/#introduction","text":"Goals Why supercomputing? What it is not A compartmentalised supercomputer Overview of the notes","title":"Introduction"},{"location":"1_Introduction/1_01_Goals/","text":"Goals \u00b6 The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a chep PC or smartphone. Whether a supercomputer will be used well dpeends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastrucgture use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the paramters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparant as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_01_Goals/#goals","text":"The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a chep PC or smartphone. Whether a supercomputer will be used well dpeends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastrucgture use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the paramters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparant as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_02_Why/","text":"Why supercomputing? \u00b6 If supercomputing is that complex, why would one want to use it? When a PC or server fails... \u00b6 Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a worstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fiels of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue. Supercomputing jobs \u00b6 From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accomodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demanmd becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the desing of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug desing one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualation of very large data sets or simulation results, were a visualisation workstation may not be enough anymore. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes withs someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be gramatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#why-supercomputing","text":"If supercomputing is that complex, why would one want to use it?","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#when-a-pc-or-server-fails","text":"Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a worstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fiels of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue.","title":"When a PC or server fails..."},{"location":"1_Introduction/1_02_Why/#supercomputing-jobs","text":"From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accomodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demanmd becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the desing of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug desing one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualation of very large data sets or simulation results, were a visualisation workstation may not be enough anymore. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes withs someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be gramatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Supercomputing jobs"},{"location":"1_Introduction/1_03_What_it_is_not/","text":"What supercomputing is not \u00b6 Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous centuty. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '70s, with the advent of so-called vectorsupercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What it is not"},{"location":"1_Introduction/1_03_What_it_is_not/#what-supercomputing-is-not","text":"Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous centuty. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '70s, with the advent of so-called vectorsupercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What supercomputing is not"},{"location":"1_Introduction/1_04_Compartmentalised/","text":"A compartmentalised supercomputer \u00b6 Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly startingh that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compartmentalised supercomputer"},{"location":"1_Introduction/1_04_Compartmentalised/#a-compartmentalised-supercomputer","text":"Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly startingh that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compartmentalised supercomputer"},{"location":"1_Introduction/1_05_Overview/","text":"Overview of the notes \u00b6 A supercomputer is a parallel computer \u00b6 A supercomputer is not a superscaled PC that runs regular PC software much faster, but it is a parallel computer: In a supercomputer, many processors work together to create a fast system, and this is even multi-level parallelism. We will discuss this in the section on Processors . The memory of a supercomputer is also organised in a hierarchy: from fast buffer memory close to the processor to slower memory and then all the way to slow disks. Most of this will be explained in the section on the memory hierarchy . The permanent storage system of a supercomputer consists of many hard disks and solid state drives that are combined to a powerful storage system with the help of software. This is explained in the section on storage for supercomputers . In the current state of technology this is far from transparent. It may be mostly transparent for correctness of the program but it is not al all for performance, which is why one needs properly written software adapted to the specific properties of supercomputers. After all, supercomputing is not about trying to upscale a PC equally in all parameters as that is uneconomical and mostly physically impossible, but it is about building hardware as economical as possible (as the market is too small to develop many non-standard components) and using software to bind all hardware together to a very powerful machine. Yet much of what you learn about supercomputers is also useful for programming PC's. Modern PC's also increasingly rely on parallelism, and this has become very visible with the AMD Ryzen-based PC's and the 12th Gen and later Intel processors. In fact, smartphones arrived there even earlier as 8-core Android smartphones have been around sincce 2013. PC's also have most of the memory hierarchy that you'll find in supercomputers, and taking that hierarchy into account when writing software is as important for PC's as it is for supercomputers. Even the inner workings of a solid state drive has elements in common with how supercomputer storage work, and understanding the behaviour of supercomputer storage helps you also understand why an SSD will not always show the expected peak performance claimed in the specifications. A supercomputer has a layered architecture \u00b6 A supercomputer is much more than some hardware on which you run your application. As already suggested above, between the hardware and your applications sits a lot of other software that binds the hardware into a usable supercomputer. At the lowest level you have the operating system which may be just linux but sometimes is a modified version of linux with some features disabled that may harm the performance of a supercomputer and are not needed by real supercomptuer applications. But on a supercomputer there is a lot of other software that sits between the hardware and basic OS on one hand and your application on the other hand. That software is often called middleware . Most readers of these notes may be only interested in the applications they want to run. However, some understanding of the hardware is needed even for simple things as starting a job, as a supercomputer is a multi-user machine and you need to ask exactly what you need to be able to run your application efficiently. And not all software can run efficiently on all hardware, or even run at all. But it also requires some understanding of the middleware. Your application, if it is really developed to use supercomputers properly, will also use some of that middleware, and not all middleware can be supported on all supercomputers. For developers even better hardware knowledge is required to understand how to write programs efficiently, and you also need a good understanding of the middleware that will be used to develop your program. The sections on processors , memory hierarchy and storage discuss the hardware of a supercomputer. The section on middleware then gives an overview of the most popular middleware used to develop software that exploits parallelism. We cannot discuss much about the application layer in these notes, as that quickly becomes too domain-specific, but we will discuss what you can expect from a supercomputer . In recent years, accelerators have also become a hot topic. As the cost, both the investment cost and energy cost, of supercomputers that are fully based on general-puropse hardware has become too high to sustain the growth of compute capacity, scientists have turned into other technologies, collectively known as accelerators, to futher increase the performance at a lower investment and energy cost, but losing some of the easy-of-use and versatility of general purpose hardware. The most important types of compute accelerators and their programming models (and corresponding middleware) will be discussed in the section on accelerators .","title":"Overview of the notes"},{"location":"1_Introduction/1_05_Overview/#overview-of-the-notes","text":"","title":"Overview of the notes"},{"location":"1_Introduction/1_05_Overview/#a-supercomputer-is-a-parallel-computer","text":"A supercomputer is not a superscaled PC that runs regular PC software much faster, but it is a parallel computer: In a supercomputer, many processors work together to create a fast system, and this is even multi-level parallelism. We will discuss this in the section on Processors . The memory of a supercomputer is also organised in a hierarchy: from fast buffer memory close to the processor to slower memory and then all the way to slow disks. Most of this will be explained in the section on the memory hierarchy . The permanent storage system of a supercomputer consists of many hard disks and solid state drives that are combined to a powerful storage system with the help of software. This is explained in the section on storage for supercomputers . In the current state of technology this is far from transparent. It may be mostly transparent for correctness of the program but it is not al all for performance, which is why one needs properly written software adapted to the specific properties of supercomputers. After all, supercomputing is not about trying to upscale a PC equally in all parameters as that is uneconomical and mostly physically impossible, but it is about building hardware as economical as possible (as the market is too small to develop many non-standard components) and using software to bind all hardware together to a very powerful machine. Yet much of what you learn about supercomputers is also useful for programming PC's. Modern PC's also increasingly rely on parallelism, and this has become very visible with the AMD Ryzen-based PC's and the 12th Gen and later Intel processors. In fact, smartphones arrived there even earlier as 8-core Android smartphones have been around sincce 2013. PC's also have most of the memory hierarchy that you'll find in supercomputers, and taking that hierarchy into account when writing software is as important for PC's as it is for supercomputers. Even the inner workings of a solid state drive has elements in common with how supercomputer storage work, and understanding the behaviour of supercomputer storage helps you also understand why an SSD will not always show the expected peak performance claimed in the specifications.","title":"A supercomputer is a parallel computer"},{"location":"1_Introduction/1_05_Overview/#a-supercomputer-has-a-layered-architecture","text":"A supercomputer is much more than some hardware on which you run your application. As already suggested above, between the hardware and your applications sits a lot of other software that binds the hardware into a usable supercomputer. At the lowest level you have the operating system which may be just linux but sometimes is a modified version of linux with some features disabled that may harm the performance of a supercomputer and are not needed by real supercomptuer applications. But on a supercomputer there is a lot of other software that sits between the hardware and basic OS on one hand and your application on the other hand. That software is often called middleware . Most readers of these notes may be only interested in the applications they want to run. However, some understanding of the hardware is needed even for simple things as starting a job, as a supercomputer is a multi-user machine and you need to ask exactly what you need to be able to run your application efficiently. And not all software can run efficiently on all hardware, or even run at all. But it also requires some understanding of the middleware. Your application, if it is really developed to use supercomputers properly, will also use some of that middleware, and not all middleware can be supported on all supercomputers. For developers even better hardware knowledge is required to understand how to write programs efficiently, and you also need a good understanding of the middleware that will be used to develop your program. The sections on processors , memory hierarchy and storage discuss the hardware of a supercomputer. The section on middleware then gives an overview of the most popular middleware used to develop software that exploits parallelism. We cannot discuss much about the application layer in these notes, as that quickly becomes too domain-specific, but we will discuss what you can expect from a supercomputer . In recent years, accelerators have also become a hot topic. As the cost, both the investment cost and energy cost, of supercomputers that are fully based on general-puropse hardware has become too high to sustain the growth of compute capacity, scientists have turned into other technologies, collectively known as accelerators, to futher increase the performance at a lower investment and energy cost, but losing some of the easy-of-use and versatility of general purpose hardware. The most important types of compute accelerators and their programming models (and corresponding middleware) will be discussed in the section on accelerators .","title":"A supercomputer has a layered architecture"},{"location":"2_Processors/","text":"Processors for supercomputers \u00b6","title":"Processors for supercomputers"},{"location":"2_Processors/#processors-for-supercomputers","text":"","title":"Processors for supercomputers"},{"location":"3_Memory/","text":"The memory hierarchy \u00b6","title":"The memory hierarchy"},{"location":"3_Memory/#the-memory-hierarchy","text":"","title":"The memory hierarchy"},{"location":"4_Storage/","text":"Storing data on supercomputers \u00b6 Introduction Problems with a parallel disk setup","title":"Storing data on supercomputers"},{"location":"4_Storage/#storing-data-on-supercomputers","text":"Introduction Problems with a parallel disk setup","title":"Storing data on supercomputers"},{"location":"4_Storage/4_01_Introduction/","text":"Introduction \u00b6 We've seen that physics makes it impossible to build a single processor core that is a thousand or a million times faster than a regular CPU core in a PC and that we need to use parallelism and lots of processors instead in a supercomputer. The same also holds for storage. It is not possible to make a single hard disk that would spin a thousand times faster and have a capacity a thousand times more than current hard disks for use in a supercomputer. Nor would it be possible to upscale the design of a solid state drive to the capacities needed for supercomputing and at the same time also improve access etc. A storage system for a supercomputer is build in the same way as the supercomputer itself is build from multiple processors: Hundreds or thousands of regular disks or SSDs are combined with the help of some hardware and mostly clever software to appear as one large and very fast disk. In fact, some of this technology is even used in PCs and smartphones as an SSD drive also uses multiple memory chips and exploits parallelism to get more performance out of the drive as would be possible with a single chip. This is why, e.g., the 256 GB hard drive in the M2 MAcBook Pro is slower than the 512 GB one as it simply doesn't contain enough memory chips to get sufficient parallelism and saturate the drive controller. However, just as not all programs can benefit from using multiple processors, not all programs can benefit from a supercomputer disk setup. A parallel disk setup only works when programs access large amounts of data in large files. And the same is true for storage as for computing: The storage of your PC can be faster than the shared storage of a supercomputer if you don't use it in the proper way. But similarly accessing files in the right way may make your already fast PC storage even faster as there are applications that are so badly written that they use the SSD in your PC also at only 5% of its potential data transfer speed...","title":"Introduction"},{"location":"4_Storage/4_01_Introduction/#introduction","text":"We've seen that physics makes it impossible to build a single processor core that is a thousand or a million times faster than a regular CPU core in a PC and that we need to use parallelism and lots of processors instead in a supercomputer. The same also holds for storage. It is not possible to make a single hard disk that would spin a thousand times faster and have a capacity a thousand times more than current hard disks for use in a supercomputer. Nor would it be possible to upscale the design of a solid state drive to the capacities needed for supercomputing and at the same time also improve access etc. A storage system for a supercomputer is build in the same way as the supercomputer itself is build from multiple processors: Hundreds or thousands of regular disks or SSDs are combined with the help of some hardware and mostly clever software to appear as one large and very fast disk. In fact, some of this technology is even used in PCs and smartphones as an SSD drive also uses multiple memory chips and exploits parallelism to get more performance out of the drive as would be possible with a single chip. This is why, e.g., the 256 GB hard drive in the M2 MAcBook Pro is slower than the 512 GB one as it simply doesn't contain enough memory chips to get sufficient parallelism and saturate the drive controller. However, just as not all programs can benefit from using multiple processors, not all programs can benefit from a supercomputer disk setup. A parallel disk setup only works when programs access large amounts of data in large files. And the same is true for storage as for computing: The storage of your PC can be faster than the shared storage of a supercomputer if you don't use it in the proper way. But similarly accessing files in the right way may make your already fast PC storage even faster as there are applications that are so badly written that they use the SSD in your PC also at only 5% of its potential data transfer speed...","title":"Introduction"},{"location":"4_Storage/4_02_Problems/","text":"Problems with a parallel disk setup \u00b6 Disks break \u00b6 Hard drives fail rather often. And though SSDs may not contain moving parts, when not used in the right way they are not that much more reliable. Given that a single drive on average fails after 1 or 2 million hours (which may seem a lot), in a stack of a thousand drives one can expect that a drive might fail every 50 to 100 days. Loosing some data every 50 days is already bad, but if all disks are combined with software to a single giant disks with even average sized files spread out over multiple disks for performance, much more data will be damaged than that single disk can contain. This is clearly unacceptable. Moreover, we don't want to stop a supercomputer every 50 or 100 days because the file system needs repairs. Let alone that big supercomputers have even bigger disk systems... The solution is to use extra disks to store enough information to recover lost data by using error correcting codes. Now hard disks and SSDs are block-oriented media: data is stored in blocks, often 4 kiB in size, so the error correction is done at the block level. A typical setup would be to use 8 drives with 2 drives for the error correction information, which can support up to two drive failures in the group of 10. But that has implications for reading and writing data. This is especially true for writing data, as whenever data is written to a block on one disk, the corresponding blocks on the disks with the error correction information must also be updated. And that requires also first reading data to be able to compute the changes in the error correcting information. Unless of course all corresponding blocks on the 8 disks would be written concurrently, as the we already have all the information to also compute the error correction information. But that makes the optimal block size for writing data effectively 8 times larger... For reading data in such a setup you can actually already benefit as you can read from all drives involved in parallel. This technique is also known as RAID, Redundant Array of Inexpensive Disks, and there exist several variants of this technique, some only to increase performance and others to improve reliability also. Error correction codes are in fact also used to protect RAM memory in servers and supercomputers, or internally in flash drives, and sometimes also in communication protocols (though they may use other techniques also). File system block size \u00b6 A file system organizes files in one or more blocks (which can be different from the blocks on disk). A block in a file system is the smallest element of data that a file system can allocate and manage. On a PC, the block size of the file system used to be 512 bytes though now it is often 4 kiB. However, on a supercomputer this would lead to an enormous amount of blocks which would become impossible to manage. There are two solutions for that. One solution is to simply use a larger block size, which is the case in, e.g., the IBM Spectrum Scale file system. Larger blocks are also a better fit with the RAID techniques used to increase the reliability of the storage system. However, as the block is the smallest amount of storage that can be allocated in a file system, it also implies that very small files will still take a lot of space on such a file system (the size of a block), though some file systems may have a solution for really small files. This can lead to a waste of space if many small files are used like that. The disk setup used at the UAntwerp HPC service until 2020 suffered from this problem. In fact, we once had a user who managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. Now the first IBM-compatible PCs had a memory limit of 640 kiB... The second solution is to use a 2-level hierarchy. Big files are split in a special way in smaller files called objects that are then stored on smaller separate servers called the object servers. As these servers are smaller, they can use a smaller block size. And small files would use only a single object, making the smallest amount of disk space consumed by a file the block size of a single object server. Examples of this approach are the Lustre and BeeGFS file system used in supercomputing. UAntwerp-specific The supercomputer storage of the CalcUA facility of the University of Antwerp used the IBM Spectrum Scale file system (then still known as GPFS) for its disk volumes. The scratch storage had a block size of 128 kiB. One user managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. And that user ran thousands of tests each storing 5 such files... Now remember the first IBM-compatible PCs had a memory limit of 640 kiB and students had to use those to write a complete Ph.D. thesis... The storage that was installed in 2020 at the CalcUA service uses BeeGFS for the file system. Which comes with its own problems as we shall see later. Physics and the network \u00b6","title":"Problems"},{"location":"4_Storage/4_02_Problems/#problems-with-a-parallel-disk-setup","text":"","title":"Problems with a parallel disk setup"},{"location":"4_Storage/4_02_Problems/#disks-break","text":"Hard drives fail rather often. And though SSDs may not contain moving parts, when not used in the right way they are not that much more reliable. Given that a single drive on average fails after 1 or 2 million hours (which may seem a lot), in a stack of a thousand drives one can expect that a drive might fail every 50 to 100 days. Loosing some data every 50 days is already bad, but if all disks are combined with software to a single giant disks with even average sized files spread out over multiple disks for performance, much more data will be damaged than that single disk can contain. This is clearly unacceptable. Moreover, we don't want to stop a supercomputer every 50 or 100 days because the file system needs repairs. Let alone that big supercomputers have even bigger disk systems... The solution is to use extra disks to store enough information to recover lost data by using error correcting codes. Now hard disks and SSDs are block-oriented media: data is stored in blocks, often 4 kiB in size, so the error correction is done at the block level. A typical setup would be to use 8 drives with 2 drives for the error correction information, which can support up to two drive failures in the group of 10. But that has implications for reading and writing data. This is especially true for writing data, as whenever data is written to a block on one disk, the corresponding blocks on the disks with the error correction information must also be updated. And that requires also first reading data to be able to compute the changes in the error correcting information. Unless of course all corresponding blocks on the 8 disks would be written concurrently, as the we already have all the information to also compute the error correction information. But that makes the optimal block size for writing data effectively 8 times larger... For reading data in such a setup you can actually already benefit as you can read from all drives involved in parallel. This technique is also known as RAID, Redundant Array of Inexpensive Disks, and there exist several variants of this technique, some only to increase performance and others to improve reliability also. Error correction codes are in fact also used to protect RAM memory in servers and supercomputers, or internally in flash drives, and sometimes also in communication protocols (though they may use other techniques also).","title":"Disks break"},{"location":"4_Storage/4_02_Problems/#file-system-block-size","text":"A file system organizes files in one or more blocks (which can be different from the blocks on disk). A block in a file system is the smallest element of data that a file system can allocate and manage. On a PC, the block size of the file system used to be 512 bytes though now it is often 4 kiB. However, on a supercomputer this would lead to an enormous amount of blocks which would become impossible to manage. There are two solutions for that. One solution is to simply use a larger block size, which is the case in, e.g., the IBM Spectrum Scale file system. Larger blocks are also a better fit with the RAID techniques used to increase the reliability of the storage system. However, as the block is the smallest amount of storage that can be allocated in a file system, it also implies that very small files will still take a lot of space on such a file system (the size of a block), though some file systems may have a solution for really small files. This can lead to a waste of space if many small files are used like that. The disk setup used at the UAntwerp HPC service until 2020 suffered from this problem. In fact, we once had a user who managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. Now the first IBM-compatible PCs had a memory limit of 640 kiB... The second solution is to use a 2-level hierarchy. Big files are split in a special way in smaller files called objects that are then stored on smaller separate servers called the object servers. As these servers are smaller, they can use a smaller block size. And small files would use only a single object, making the smallest amount of disk space consumed by a file the block size of a single object server. Examples of this approach are the Lustre and BeeGFS file system used in supercomputing. UAntwerp-specific The supercomputer storage of the CalcUA facility of the University of Antwerp used the IBM Spectrum Scale file system (then still known as GPFS) for its disk volumes. The scratch storage had a block size of 128 kiB. One user managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. And that user ran thousands of tests each storing 5 such files... Now remember the first IBM-compatible PCs had a memory limit of 640 kiB and students had to use those to write a complete Ph.D. thesis... The storage that was installed in 2020 at the CalcUA service uses BeeGFS for the file system. Which comes with its own problems as we shall see later.","title":"File system block size"},{"location":"4_Storage/4_02_Problems/#physics-and-the-network","text":"","title":"Physics and the network"},{"location":"5_Middleware/","text":"Middleware: Turning the hardware into a usable supercomputer \u00b6","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"5_Middleware/#middleware-turning-the-hardware-into-a-usable-supercomputer","text":"","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"6_Expectations/","text":"What can we expect? \u00b6","title":"What can we expect?"},{"location":"6_Expectations/#what-can-we-expect","text":"","title":"What can we expect?"},{"location":"7_Accelerators/","text":"Accelerators \u00b6 What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading","title":"Accelerators"},{"location":"7_Accelerators/#accelerators","text":"What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading","title":"Accelerators"},{"location":"7_Accelerators/7_01_What_are_accelerators/","text":"What are accelerators? \u00b6 In short \u00b6 We restrict ourselves to the most common types of compute accelerators used in supercomputing and do not cover, e.g., accelerators in the network to process certain MPI calls. An accelerator is usually a coprocessor that accelerates some computations that a CPU might be capable of doing, but that can be done much faster on more specialised hardware, so that it becomes interesting to offload that work to specialised hardware. An accelerator is not a full-featured general purpose processor that can run a regular operating system, etc. Hence an accelerator always has to work together with the regular CPU of the system, which can lead to very complicated programming with a host program that then offloads certain routines to the accelerator, and also needs to manage the transport of data to and from the accelerator. Some history \u00b6 Accelerators have been around for a long time in large computers, and in particular in mainframes, very specialised machines mostly used for administrative work. However, accelerators also appeared in the PC world starting in the '90s. The first true programmable accelerators probably appeared in the form of high-end sound cards. They contained one or more so-called Digital Signal Processor (DSP) chips for all the digital processing of the sound. Graphic cards were originally very specialised fixed-function hardware that was not really programmable but this changed in the early '00s with graphics cards as the NVIDIA GeForce 3 and ATI Radeon 9300 (ATI was later aquired by AMD which still uses the Radeon brand). It didn't take long before scientist looking for more and cheaper compute power took note and started experimenting with using that programmability to accelerate certain scientific computations. Manufacturers, and certainly NVIDIA, took note and started adding features specifically for broader use. This led to the birth of NVIDIA CUDA 1.0 in 2007, the first succesful platform and programming model for programming graphics cards that were now called Graphics Processing Units (or GPU) as they became real programmable processors. And the term GPGPU for General-Purpose GPU is also used for hardware that is particularly suited to be used for non-graphics work also. GPGPU programming quickly became popular, and even a bit overhyped, as not all applications are suitable for GPGPU computing. Types of accelerators \u00b6 The most popular type of accelerators are accelerators for vector computing . All modern GPUs fall in this family. Examples are NVIDIA Data Center series (previously also called the Tesla series). These started of as basically a more reliable version of the NVIDIA GeForce and Quadro GPUs, but currently, with the Ada Lovelace GPUs and Hopper GPGPUs these lines start to diverge a bit (and even before, the cards really meant for supercomputing had more hardware on them for double precision floating point computations). Strictly speaking the NVIDIA architecture is a single instruction multiple data (SIMD) architecture but not one that uses explicit vector instructions, but the resulting capabilities are not really different from more regular vector computers (but then vector computers with an instruction set that also support scatter/gather instructions and predication, features that are missing from, e.g., the AVX/AVX2 instruction set). AMD has the Instinct series for GPGPU computing. They employ a seperate architecture for their compute cards, called CDNA, while their current graphics cards use various generations of the RDNA architecture. The CDNA architecture is a further evolution of their previous graphics architecture GCN though (used in, e.g., the Vega cards). AMD Instinct GPUs are used in the first USA exaflop computer Frontier (fastest system in the Top500 ranking of June 2022) and in the European LUMI system (fastest European system in the Top500 ranking of June 2022). These computers use the CDNA2 architecture. A future USA exascale system, El Capitan, planned for 2023 (or possibly 2024 with the supply chain disruptions largely due to Covid), will emply a future version of the architecture, RDNA3, which will bring CPU and GPU very close together. Intel is also moving into the market of GPGPU computing with their Xe graphics products. They have supported using their integrated GPUs for computations for many years already, with even support in their C/C++/Fortran compilers, but are now making a separate products for the supercomputing market with the Xe HPC product lines which support additional data formats that are very needed for scientific computing applications. The first product in this line is the GPU code named Ponte Vecchio that will be used in the USA Aurora supercomputer, which should become the second USA exaflop computer. A future European pre-exascale system will also have a compute section with the successor of that chip, Rialto Bridge. The NEC SX Aurora TSUBASA has a more traditional vector computing architecture, but is physically also an expansion card that is put in a regular Intel-compatible server. It is special in the sense that the original idea was that applications would fully run on the vector processor and hence not use a host programming with offloading, while under the hood the OS libraries would offload OS operations to the Intel-compatible server, but in practice it is more and more used as a regular accelerator with a host program running on the Intel-compatible server offloading work to the vector processor. A second type of accelerator that became very popular recently, are accelerators for matrix operations , and in particular matrix multiplication or rank-k update. They were originally designed to speed up operations in certain types of neural networks, but were quickly gained support for additional data types that makes them useful for a range of AI and other HPC applications. Some are integrated on GPGPUs while others are specialised accelerators. The ones integrated in GPUs are most popular for supercomputing though: NVIDIA Tensor cores in the V100 and later generations. AMD matrix cores in the MI100 and later chips. The MI200 generation may be a little behind the similar-generation A100 NVIDIA cards when it comes to low-precision formats used in some AI applications, but it shines in higher-precision data formats (single and double precision floating point). Intel includes their so-called Matrix Engines in the Ponte Vecchio GPGPUs. The NVIDIA tensor cores, AMD matrix cores and Intel matrix engines are all integrated very closely with the vector processing hardware on their GPGPUs, However, there are also dedicated matrix computing accelerators, in particular in accelerators specifically designed for AI, siuch as the Google TPU (Tensor Processing Unit). Most neural network accelerators on smartphone processors also fall in this category as they are usually in a physically distinct area from the GPU hardware in the SOC (though not a separate chip or die). A third and so far less popular accelerator in supercomputing is an FPGA accelerator, which stands for Field Programmable Gate Array. This is hardware designed to be fully configured after manufacturing, allowing the user to create a specialised processor for their application. One could, e.g., imagine creating specialised 2-bit CPUs for working with generic data.","title":"What are accelerators?"},{"location":"7_Accelerators/7_01_What_are_accelerators/#what-are-accelerators","text":"","title":"What are accelerators?"},{"location":"7_Accelerators/7_01_What_are_accelerators/#in-short","text":"We restrict ourselves to the most common types of compute accelerators used in supercomputing and do not cover, e.g., accelerators in the network to process certain MPI calls. An accelerator is usually a coprocessor that accelerates some computations that a CPU might be capable of doing, but that can be done much faster on more specialised hardware, so that it becomes interesting to offload that work to specialised hardware. An accelerator is not a full-featured general purpose processor that can run a regular operating system, etc. Hence an accelerator always has to work together with the regular CPU of the system, which can lead to very complicated programming with a host program that then offloads certain routines to the accelerator, and also needs to manage the transport of data to and from the accelerator.","title":"In short"},{"location":"7_Accelerators/7_01_What_are_accelerators/#some-history","text":"Accelerators have been around for a long time in large computers, and in particular in mainframes, very specialised machines mostly used for administrative work. However, accelerators also appeared in the PC world starting in the '90s. The first true programmable accelerators probably appeared in the form of high-end sound cards. They contained one or more so-called Digital Signal Processor (DSP) chips for all the digital processing of the sound. Graphic cards were originally very specialised fixed-function hardware that was not really programmable but this changed in the early '00s with graphics cards as the NVIDIA GeForce 3 and ATI Radeon 9300 (ATI was later aquired by AMD which still uses the Radeon brand). It didn't take long before scientist looking for more and cheaper compute power took note and started experimenting with using that programmability to accelerate certain scientific computations. Manufacturers, and certainly NVIDIA, took note and started adding features specifically for broader use. This led to the birth of NVIDIA CUDA 1.0 in 2007, the first succesful platform and programming model for programming graphics cards that were now called Graphics Processing Units (or GPU) as they became real programmable processors. And the term GPGPU for General-Purpose GPU is also used for hardware that is particularly suited to be used for non-graphics work also. GPGPU programming quickly became popular, and even a bit overhyped, as not all applications are suitable for GPGPU computing.","title":"Some history"},{"location":"7_Accelerators/7_01_What_are_accelerators/#types-of-accelerators","text":"The most popular type of accelerators are accelerators for vector computing . All modern GPUs fall in this family. Examples are NVIDIA Data Center series (previously also called the Tesla series). These started of as basically a more reliable version of the NVIDIA GeForce and Quadro GPUs, but currently, with the Ada Lovelace GPUs and Hopper GPGPUs these lines start to diverge a bit (and even before, the cards really meant for supercomputing had more hardware on them for double precision floating point computations). Strictly speaking the NVIDIA architecture is a single instruction multiple data (SIMD) architecture but not one that uses explicit vector instructions, but the resulting capabilities are not really different from more regular vector computers (but then vector computers with an instruction set that also support scatter/gather instructions and predication, features that are missing from, e.g., the AVX/AVX2 instruction set). AMD has the Instinct series for GPGPU computing. They employ a seperate architecture for their compute cards, called CDNA, while their current graphics cards use various generations of the RDNA architecture. The CDNA architecture is a further evolution of their previous graphics architecture GCN though (used in, e.g., the Vega cards). AMD Instinct GPUs are used in the first USA exaflop computer Frontier (fastest system in the Top500 ranking of June 2022) and in the European LUMI system (fastest European system in the Top500 ranking of June 2022). These computers use the CDNA2 architecture. A future USA exascale system, El Capitan, planned for 2023 (or possibly 2024 with the supply chain disruptions largely due to Covid), will emply a future version of the architecture, RDNA3, which will bring CPU and GPU very close together. Intel is also moving into the market of GPGPU computing with their Xe graphics products. They have supported using their integrated GPUs for computations for many years already, with even support in their C/C++/Fortran compilers, but are now making a separate products for the supercomputing market with the Xe HPC product lines which support additional data formats that are very needed for scientific computing applications. The first product in this line is the GPU code named Ponte Vecchio that will be used in the USA Aurora supercomputer, which should become the second USA exaflop computer. A future European pre-exascale system will also have a compute section with the successor of that chip, Rialto Bridge. The NEC SX Aurora TSUBASA has a more traditional vector computing architecture, but is physically also an expansion card that is put in a regular Intel-compatible server. It is special in the sense that the original idea was that applications would fully run on the vector processor and hence not use a host programming with offloading, while under the hood the OS libraries would offload OS operations to the Intel-compatible server, but in practice it is more and more used as a regular accelerator with a host program running on the Intel-compatible server offloading work to the vector processor. A second type of accelerator that became very popular recently, are accelerators for matrix operations , and in particular matrix multiplication or rank-k update. They were originally designed to speed up operations in certain types of neural networks, but were quickly gained support for additional data types that makes them useful for a range of AI and other HPC applications. Some are integrated on GPGPUs while others are specialised accelerators. The ones integrated in GPUs are most popular for supercomputing though: NVIDIA Tensor cores in the V100 and later generations. AMD matrix cores in the MI100 and later chips. The MI200 generation may be a little behind the similar-generation A100 NVIDIA cards when it comes to low-precision formats used in some AI applications, but it shines in higher-precision data formats (single and double precision floating point). Intel includes their so-called Matrix Engines in the Ponte Vecchio GPGPUs. The NVIDIA tensor cores, AMD matrix cores and Intel matrix engines are all integrated very closely with the vector processing hardware on their GPGPUs, However, there are also dedicated matrix computing accelerators, in particular in accelerators specifically designed for AI, siuch as the Google TPU (Tensor Processing Unit). Most neural network accelerators on smartphone processors also fall in this category as they are usually in a physically distinct area from the GPU hardware in the SOC (though not a separate chip or die). A third and so far less popular accelerator in supercomputing is an FPGA accelerator, which stands for Field Programmable Gate Array. This is hardware designed to be fully configured after manufacturing, allowing the user to create a specialised processor for their application. One could, e.g., imagine creating specialised 2-bit CPUs for working with generic data.","title":"Types of accelerators"},{"location":"7_Accelerators/7_02_Offloading/","text":"Offloading \u00b6 In early accelerator generations (and this is still the case in 2022) a CPU cannot directly operate on data in the memory of the GPU and vice-versa. Instead data needs to be copied between the memory spaces. Multiple accelerators in a system may or may not share a memory space. NVIDIA NVLINK for instance is a technology that can be used to link graphics cards together and create a shared memory space, but even then it is crucial for performance that data is as much as possible in the memory directly attached to a GPU when being used. Many modern GPUs for scientific computing include support for unified memory where CPU and GPUs in the system can share a single logical address space, but under the hood the data still needs to be copied. This feature has been present in, e.g., the NVIDIA Pascal and later generations, where a page fault mechanism would be used to trigger a migration mechanism. Control-wise the program running on the CPU orchestrates the work but passes control to the accelerator to execute those code blocks that can be accelerated by the accelerator. The main problem with this model is that all the data copying that is needed, whether explicitly triggered by the application or implicitly through the unified memory model of some modern GPUs, can really nullify the gain from the accelerator. This is no different from what we saw for distributed computing. Just as for distributed computing, the fraction of the algorithm that cannot be parallelized limits the speedup, as does the communication overhead, for accelerators the fraction of the application and the overhead to pass data to and from the accelerator will limit the speedup that can be obtained from the accelerator. Hence it is clear that for future generations of accelerators, the main attention will be in making them more versatile to reduce the fraction that cannot be accelerated, and in integrating them closer with the CPU to reduce or eliminate the overhead in passing data between CPU and GPU. The first signs of this evolution were in some USA pre-exascale systems Summit and Sierra that used a IBM POWER9 CPUs and NVIDIA V100 GPUs. Those GPUs were connected to the CPU through NVLink, NVIDIA's interconnect for GPUs, rather than only PCIe links, so that the memory spaces of CPU and GPU become physically merged, with all data directly addressable from GPU and CPU and cache coherency. This technology of the Summit and Sierra supercomputers is carried over to the first three planned exascale systems of the USA. Frontier, and the European supercomputer LUMI, use the MI250X variant of the AMD CDNA2 architecture. The nodes in these systems consist of 4 MI250X GPUs, with each GPU consisting of two dies, and a custom Zen3-based CPU code named Trento. AMD's InfinityFabric is not only used internally in each package to connect the dies (and the zen3 chiplets with the I/O die in the Trento CPU), but also to connect the CPU packages to each other and the CPUs to the GPUs, hence creating a unified coherent memory space. This allows each CPU chiplet to access the memory of the closest attached GPU die with full cache coherency, but it is not clear if coherency is usable over the full system. The Aurora supercomputer which uses Intel Sapphire Rapids CPUs and Ponte Vecchio GPUs will also support a unified and cache-coherent memory space. NVIDIA was lagging a bit with the Ampere generation that has no longer a corresponding CPU that supports its NVLink connection, but will return with its own ARM-based CPU code named Grace and the Hopper GPU generation. The Grace Hopper superchip will combine a Grace CPU die and a Hopper GPU die with a coherent interconnect between them, creating a coherent memory space between the CPU and GPU in the same package, though the available whitepapers suggest no coherency between different CPU-GPU packages in the system. Placing a CPU and GPU in the same package also not only ensures much higher bandwidth between both, but also a much lower energy consumption for the data transfers. Future generation products will go even further. The AMD MI300 is technically speaking no longer a GPU but rather an Accelerated Processing Unit or APU as it will integrate CDNA3 GPU dies, Zen4 CPU dies and memory in a single package. The CPU and GPU dies will no longer have physically distinct memory and AMD claims that copy operations between CPU and GPU in a package will be completely unnecessary. MI300-based products should start appearing towards the end of 2023 or early 2024. Intel has also hinted at the Falcon Shores successor to Ponte Vecchio and Rialto Bridge, likely a 2024 or 2025 product, that will also combine CPU and GPU chiplets in a single package with a fully unified memory space. Note that current Intel or AMD-based PC's with integrated GPUs are not better in this respect. The CPU and integrated GPU share the physical RAM in the system, but each has its own reserved memory space and data still needs to be migrated between those spaces. The Apple M1 and later chips on the other hand so seem to have a truly unified memory space, though given the limited information that Apple provides, it is hard to see if it is indeed a fully hardware based solution. The fact that the M1 can be so fast in photo- and video processing apps though indicates that the M1 does indeed have an architecture that allows to make use of all accelerators in a very efficient way.","title":"Offloading"},{"location":"7_Accelerators/7_02_Offloading/#offloading","text":"In early accelerator generations (and this is still the case in 2022) a CPU cannot directly operate on data in the memory of the GPU and vice-versa. Instead data needs to be copied between the memory spaces. Multiple accelerators in a system may or may not share a memory space. NVIDIA NVLINK for instance is a technology that can be used to link graphics cards together and create a shared memory space, but even then it is crucial for performance that data is as much as possible in the memory directly attached to a GPU when being used. Many modern GPUs for scientific computing include support for unified memory where CPU and GPUs in the system can share a single logical address space, but under the hood the data still needs to be copied. This feature has been present in, e.g., the NVIDIA Pascal and later generations, where a page fault mechanism would be used to trigger a migration mechanism. Control-wise the program running on the CPU orchestrates the work but passes control to the accelerator to execute those code blocks that can be accelerated by the accelerator. The main problem with this model is that all the data copying that is needed, whether explicitly triggered by the application or implicitly through the unified memory model of some modern GPUs, can really nullify the gain from the accelerator. This is no different from what we saw for distributed computing. Just as for distributed computing, the fraction of the algorithm that cannot be parallelized limits the speedup, as does the communication overhead, for accelerators the fraction of the application and the overhead to pass data to and from the accelerator will limit the speedup that can be obtained from the accelerator. Hence it is clear that for future generations of accelerators, the main attention will be in making them more versatile to reduce the fraction that cannot be accelerated, and in integrating them closer with the CPU to reduce or eliminate the overhead in passing data between CPU and GPU. The first signs of this evolution were in some USA pre-exascale systems Summit and Sierra that used a IBM POWER9 CPUs and NVIDIA V100 GPUs. Those GPUs were connected to the CPU through NVLink, NVIDIA's interconnect for GPUs, rather than only PCIe links, so that the memory spaces of CPU and GPU become physically merged, with all data directly addressable from GPU and CPU and cache coherency. This technology of the Summit and Sierra supercomputers is carried over to the first three planned exascale systems of the USA. Frontier, and the European supercomputer LUMI, use the MI250X variant of the AMD CDNA2 architecture. The nodes in these systems consist of 4 MI250X GPUs, with each GPU consisting of two dies, and a custom Zen3-based CPU code named Trento. AMD's InfinityFabric is not only used internally in each package to connect the dies (and the zen3 chiplets with the I/O die in the Trento CPU), but also to connect the CPU packages to each other and the CPUs to the GPUs, hence creating a unified coherent memory space. This allows each CPU chiplet to access the memory of the closest attached GPU die with full cache coherency, but it is not clear if coherency is usable over the full system. The Aurora supercomputer which uses Intel Sapphire Rapids CPUs and Ponte Vecchio GPUs will also support a unified and cache-coherent memory space. NVIDIA was lagging a bit with the Ampere generation that has no longer a corresponding CPU that supports its NVLink connection, but will return with its own ARM-based CPU code named Grace and the Hopper GPU generation. The Grace Hopper superchip will combine a Grace CPU die and a Hopper GPU die with a coherent interconnect between them, creating a coherent memory space between the CPU and GPU in the same package, though the available whitepapers suggest no coherency between different CPU-GPU packages in the system. Placing a CPU and GPU in the same package also not only ensures much higher bandwidth between both, but also a much lower energy consumption for the data transfers. Future generation products will go even further. The AMD MI300 is technically speaking no longer a GPU but rather an Accelerated Processing Unit or APU as it will integrate CDNA3 GPU dies, Zen4 CPU dies and memory in a single package. The CPU and GPU dies will no longer have physically distinct memory and AMD claims that copy operations between CPU and GPU in a package will be completely unnecessary. MI300-based products should start appearing towards the end of 2023 or early 2024. Intel has also hinted at the Falcon Shores successor to Ponte Vecchio and Rialto Bridge, likely a 2024 or 2025 product, that will also combine CPU and GPU chiplets in a single package with a fully unified memory space. Note that current Intel or AMD-based PC's with integrated GPUs are not better in this respect. The CPU and integrated GPU share the physical RAM in the system, but each has its own reserved memory space and data still needs to be migrated between those spaces. The Apple M1 and later chips on the other hand so seem to have a truly unified memory space, though given the limited information that Apple provides, it is hard to see if it is indeed a fully hardware based solution. The fact that the M1 can be so fast in photo- and video processing apps though indicates that the M1 does indeed have an architecture that allows to make use of all accelerators in a very efficient way.","title":"Offloading"},{"location":"7_Accelerators/7_03_CPUs_accelerator_features/","text":"CPUs with accelerator features \u00b6 In the (relatively young) history of personal computers and their microprocessors, succesful accelerators were often integrated in CPUs by means of extensions of the CPU instruction set. Though never as performant as a dedicated accelerator, it was often a \"good enough\" solution to the extent that some accelerators even disappeared from the market, and as these are now part of the instruction set of the CPU, programming is also greatly simplified as there is no need to pass data and control to a coprocessor. Vector accelerators have long had an influence on CPUs. The original Intel MMX instructions (which is rumoured to stand for MultiMedia eXtensions) were designed to compete with the DSPs used in sound cards in the second half of the '90s. They were introduced in an update of the Pentium architecture in 1997. This instruction set reused 64-bit registers from the floating point unit, so both could not be used together. Two years later, in 1999, intel introduced the first version of SSE, which used new 128-bit registers. The MMX and SSE instruction sets made it feasible to process audio on the CPU and quickly erased the market of higher-end soundcards with DSPs. The SSE instruction set continued to evolve for several generations and also adopted support for floating point computing. It became essential to get the full performance out of Intel CPUs in scientific codes. The various editions of the SSE instruction set where superseded by the AVX and later AVX2 instruction sets, both of which use 256-bit registers, defining vector operations working on 4 double precision or 8 single precision floating point number simultaneously. Maybe less known is that Intel's current vector instruction set for scientific computing, AVX512, which as the name implies uses 512-bit registers that can hold 8 double precision or 16 single precision floating point numbers, has its origin in a failed project to build a GPU with a simplified x86 architecture, code-named Larrabee. The Larrabee design was recycled as the Xeon Phi, a chip for supercomputing meant to compete with the NVIDIA GPUs, and in a second generation Xeon Phi product the instruction set was thoroughly revised to become the AVX512 instruction set which is the first x86 vector extension with good support for scatter and gather operations and predication. Another interesting processor design is the ARM-based Fujitsu A64fx which is used in the Japanese Fugaku supercomputer which held the crown of fastest computer in the world from June 2020 until June 2022 when it was finally surpassed by the Frontier supercomputer. The A64fx processor was built specifically for supercomputers. Together with ARM, a new vector extension to the ARM instruction set was developed, Scalable Vector Extensions or SVE, which later became an official part of an update of the ARM architecture. The A64fx combines 48 or 52 cores on a chip with a very high bandwidth but relatively small memory system that uses the same technology as the supercomputer GPUs of NVIDIA. It could be used to build supercomputers that were not only as fast as GPU-based systems, but also almost as power-efficient, and performance was good even on some applications that are less suited for vectorisation but also don't run very good on traditional CPUs due to the lack of memory bandwidth in the latter. Matrix accelerators, although fairly new in the market, are also already starting to influence CPU instruction sets. IBM has added matrix instructions for both AI and linear algebra (the latter requiring single and double precision floating point) to the POWER10 processor. Intel has already some instructions in some CPUs but only for low-precision inference, but will add a new instruction set extension called AMX in Sapphire Rapids, a server CPU that should come out in late 2022 or early 2023. It is still only meant for AI applications, supporting 4 and 8-bit integers and Googles bfloat16 data format. Similarly the ARM V9-A instruction set adds Scalable Matrix Extensions to the architecture. As is the case for Intel, these extensions are also aimed at AI applications, supporting 8-bit integer and bfloat16 data formats. Though these CPU instructions certainly don't make CPUs so fast that they beat GPUs, they also have two advantages over accelerators: there is no need to pass control to an remote processor, which can save some time, and there are no issues with data needing to be moved. Also, one should not forget that a single GPU card for a supercomputer easily costs three times or more as much as a single CPU socket with memory (and even more if the lower end SKUs in the CPU line are used), so a CPU doesn't need to be as fast as a GPU to be the most economical solution.","title":"CPUs with accelerator features"},{"location":"7_Accelerators/7_03_CPUs_accelerator_features/#cpus-with-accelerator-features","text":"In the (relatively young) history of personal computers and their microprocessors, succesful accelerators were often integrated in CPUs by means of extensions of the CPU instruction set. Though never as performant as a dedicated accelerator, it was often a \"good enough\" solution to the extent that some accelerators even disappeared from the market, and as these are now part of the instruction set of the CPU, programming is also greatly simplified as there is no need to pass data and control to a coprocessor. Vector accelerators have long had an influence on CPUs. The original Intel MMX instructions (which is rumoured to stand for MultiMedia eXtensions) were designed to compete with the DSPs used in sound cards in the second half of the '90s. They were introduced in an update of the Pentium architecture in 1997. This instruction set reused 64-bit registers from the floating point unit, so both could not be used together. Two years later, in 1999, intel introduced the first version of SSE, which used new 128-bit registers. The MMX and SSE instruction sets made it feasible to process audio on the CPU and quickly erased the market of higher-end soundcards with DSPs. The SSE instruction set continued to evolve for several generations and also adopted support for floating point computing. It became essential to get the full performance out of Intel CPUs in scientific codes. The various editions of the SSE instruction set where superseded by the AVX and later AVX2 instruction sets, both of which use 256-bit registers, defining vector operations working on 4 double precision or 8 single precision floating point number simultaneously. Maybe less known is that Intel's current vector instruction set for scientific computing, AVX512, which as the name implies uses 512-bit registers that can hold 8 double precision or 16 single precision floating point numbers, has its origin in a failed project to build a GPU with a simplified x86 architecture, code-named Larrabee. The Larrabee design was recycled as the Xeon Phi, a chip for supercomputing meant to compete with the NVIDIA GPUs, and in a second generation Xeon Phi product the instruction set was thoroughly revised to become the AVX512 instruction set which is the first x86 vector extension with good support for scatter and gather operations and predication. Another interesting processor design is the ARM-based Fujitsu A64fx which is used in the Japanese Fugaku supercomputer which held the crown of fastest computer in the world from June 2020 until June 2022 when it was finally surpassed by the Frontier supercomputer. The A64fx processor was built specifically for supercomputers. Together with ARM, a new vector extension to the ARM instruction set was developed, Scalable Vector Extensions or SVE, which later became an official part of an update of the ARM architecture. The A64fx combines 48 or 52 cores on a chip with a very high bandwidth but relatively small memory system that uses the same technology as the supercomputer GPUs of NVIDIA. It could be used to build supercomputers that were not only as fast as GPU-based systems, but also almost as power-efficient, and performance was good even on some applications that are less suited for vectorisation but also don't run very good on traditional CPUs due to the lack of memory bandwidth in the latter. Matrix accelerators, although fairly new in the market, are also already starting to influence CPU instruction sets. IBM has added matrix instructions for both AI and linear algebra (the latter requiring single and double precision floating point) to the POWER10 processor. Intel has already some instructions in some CPUs but only for low-precision inference, but will add a new instruction set extension called AMX in Sapphire Rapids, a server CPU that should come out in late 2022 or early 2023. It is still only meant for AI applications, supporting 4 and 8-bit integers and Googles bfloat16 data format. Similarly the ARM V9-A instruction set adds Scalable Matrix Extensions to the architecture. As is the case for Intel, these extensions are also aimed at AI applications, supporting 8-bit integer and bfloat16 data formats. Though these CPU instructions certainly don't make CPUs so fast that they beat GPUs, they also have two advantages over accelerators: there is no need to pass control to an remote processor, which can save some time, and there are no issues with data needing to be moved. Also, one should not forget that a single GPU card for a supercomputer easily costs three times or more as much as a single CPU socket with memory (and even more if the lower end SKUs in the CPU line are used), so a CPU doesn't need to be as fast as a GPU to be the most economical solution.","title":"CPUs with accelerator features"},{"location":"7_Accelerators/7_04_Programming_accelerators/","text":"Accelerator programming \u00b6 We will restrict ourselves to GPGPU programming as these are currently the most widely used accelerators in supercomputers. The current state \u00b6 Accelerator programming is, also due to the early stage of the technology, still a mess, and standardisation still has to set in. This is not uncommon in the world of supercomputing: The same happened with message passing where there were several proprietary technologies until the needs were sufficiently well understood to come to a standardisation. Currently there are three competing ecosystems growing: NVIDIA was the first to market with a fairly complete ecosystem for GPGPU programming. They chose to make large parts of their technology proprietary to create a vendor lock-in and keep hardware prices high. CUDA is the main NVIDIA ecosystem. NVIDIA together with partners also created a set of compiler pragma's for C/C++ and Fortran for GPGPU programming with an open consortium, but many other compiler vendors are hesitant to pick those up. The NVIDIA toolset also offers some support for open, broadly standardised technologies, but the support is usually inferior to that for their proprietary technologies or standards that are in practice largely controlled by them. AMD is a latecomer to the market. Their software stack is called ROCm, and they open sourced all components to gain more traction in the market. They basically focus on two technologies that we will discuss: HIP and Open MP. Intel calls its ecosystem oneAPI, as it tries to unify CPU and GPGPU programming, with libraries that make it easy to switch between a CPU-only version and a GPGPU-accelerated version of an application. The oneAPI ecosystem is largely based on Data Parallel C++, an extension of SYCL, and Open MP compiler pragmas. The oneAPI is partly open-sourced. In some cases, only the API is available and vendors should make their own implementation, but, e.g., the sources for their clang/LLVM based Data Parallel C++ compiler are available to all and this has been used already to make ports to NVIDIA and AMD GPU hardware. Lower-level models \u00b6 These models use separate code for the host and the accelerator devices that are then linked together to create the application binary. CUDA \u00b6 CUDA is the best known environment in the HPC world. It is however a proprietary NVIDIA technology. It is possible to write GPGPU kernels using subsets of C, C++ and Fortran. CUDA also comes with many libraries with optimised routines for linear algebra, FFT, neural networks, etc. This makes it by far the most extensive of the lower-level models. CUDA, which launched in 2007, has gone through a rapid evolution though it is now reaching a level of maturity with less frequent major updates. HIP \u00b6 HIP or H eterogeneous Computing I nterface for P ortability is AMD's alternative to CUDA. It tries to be as close as possible as legally possible. The HIP API maps one-to-one on the CUDA API, making it possible to recompile HIP code with the CUDA tools using just a header file. In theory this process is without loss of performance, but there is one caveat: A kernel that is efficient on an AMD GPU may not be the most efficient option on a NVIDIA GPU as, e.g., the vector size is different. So careful programming is needed to ensure efficiency on both platforms. HIP is purely based on C++ with currently no support for Fortran GPU kernels. Feature-wise it is roughly comparable to CUDA 7 or 8, but features that cannot yet be supported by AMD hardware are missing. The ROCm platform also comes with a lot of libraries that provide APIs that are also similar to the APIs provided by their CUDA counterpart to further ease porting code from the CUDA ecosystem to the AMD ROCm ecosystem. HIP also comes with two tools that help in reworking CUDA code into HIP code, though both typically need some manual intervention. OpenCL \u00b6 OpenCL or Open Compute Language is a framework for heterogeneous computing developed by the non-prpfit, member-driven technology consortium Khronos Group which manages many standards for GPU software (including also OpenGL and its offsprings and Vulkan). It develops vendor-neutral standards. OpenCL is a C and C++-based technology to develop code that not only runs on GPU accelerators, but the code can also be recompiled for CPU only, and some DSPs and FPGAs are also supported. This makes it a very versatile standard and it was for a long time a very attractive standard for commercial software development. It is by far not as advanced as CUD or not even HIP. The evolution of the standard has been very slow lately, with vendors hardly picking up many of the features of the 2.x versions. This lead to version 3.0 of the standard that defined a clear baseline and made the other features explicitly optional so that programmers at least know what they can expect. OpenCL is in principle supported on NVIDIA, AMD and Intel hardware, but the quality of the implementation is not always spectacular. There are also some open source implementations with varying hardware support. It has been used in supercomputing software though it was not always the only model offered as several packages also contained specialised CUDA code for better performance on NVIDIA hardware. The molecular dynamics package GROMACS is one example, and they will be switching away from OpenCL to newer technologies to support non-NVIDIA hardware. OpenCL is largely superseded by newer technologies developed by the Khronos Group. Vulkan is their effort to create an API that unifies 3D graphics and computing and is mostly oriented towards game programming etc., but less towards supercomputing (as several GPUs used for supercomputing even start omitting graphics-specific circuits) while SYCL, which we will discuss later in these notes, is a new initiative for GPGPU programming. Compiler directives \u00b6 In these models, there are no separate sources for the GPU that are compiled with a separate compiler. Instead, there is a single base of code with compiler directives instructing the compiler how certain parts of the code can be offloaded to an accelerator. These directives appear as comments in a Fortran code to compilers that do not support the technology or use the #pragma directive in C/C++ to make clear that they are directives to compilers that support those. OpenACC \u00b6 OpenACC was the first successful compiler directive model for GPGPU computing. It supports C, C++ and Fortran programming. The first version of standard made its debut at the supercomputing computing conference SC'11 in November 2011. The technology was largely developed by 4 companies: the GPU company NVIDIA, the compiler company Portland Group which was later renamed PGI and later bought by NVIDIA, the supercomputer manufacturer Cray which got bought by HPE and largely lost interest in OpenACC, now more promoting an alternative, and CAPS Entreprises, a French company and university spin-off creating tools for accelerator programming that failed in 2013. The OpenACC standard is currently controlled by the OpenACC Organization, and though it does count other companies that develop GPUs among its members, it still seems rather dominated by NVIDIA which may explain why other companies are somewhat hesitant to pick up the standard. The standard is at the time of writing of this section at version 2.7, released in November 2018. OpenACC is well supported on NVIDIA hardware through the NVIDIA HPC compilers (which is the new name of the PGI compilers adopted after the integration of PGI in NVIDIA). GCC offers some support on NVIDIA and some AMD hardware for version 2.6 of the standard since version 10 of the GCC compilers., but the evolution is slow and performance is often not that great. More promising for the future is the work going on in the clang/LLVM community to support OpenACC, as this effort is largely driven by the USA national labs who want to avoid having to port all OpenACC-based code developed for the NVIDIA based pre-exascale supercomputers to other models to run on the new AMD and Intel based exascale supercomputers. In fact, the clang/LLVM ecosystem is the future for scientific computing and not the GNU Compiler Collection ecosystem as most compiler vendors already base their compilers on that technology. The NVIDIA, AMD and new Intel compilers are all based on LLVM and the clang frontend for C and C++-support, with NVIDIA and AMD still using a Fortran front-end developed by PGI and donated to the LLVM project while Intel is already experimenting with a new community-developed more modern front-end for Fortran. OpenACC support in the LLVM ecosystem will build upon the OpenMP support, the technology that we will discuss next, using extensions for those OpenACC features that still have no equivalent in OpenMP. OpenMP \u00b6 We've already discussed OpenMP as a technology for shared memory parallelism and for vectorisation (the latter since version 4.0 of the standard). But OpenMP is nowadays even more versatile. Since version 4.0 of the standard, released in July 2013, there is also support for offloading to accelerators. That support was greatly improved in version 5.0 of the standard which was released at the SC'18 supercomputer conference. It became a more descriptive and less prescriptive model (offering the compiler enough information to decide what it should do rather then enforcing the compiler to do something in a particular way), with the prescriptive nature being critisized a lot by the OpenACC community who claimed superiority because of this. It also contained much better support for debuggers, performance monitoring tools, etc. OpenMP has since had minor extensions i the form of version 5.1 at SC'20 and 5.2 at SC'21. The standard is controlled by a much larger consortium than the OpenACC standard. OpenMP is an important technology in the AMD ROCm and Intel oneAPI ecosystems. Intel has in fact supported OpenMP offload to some of its own GPUs for many years, long before establishing the oneAPI ecosystem. GCC has had some support for OpenMP offload to NVIDIA hardware since version 7 and to some AMD hardware since version 10. However, as is the case for OpenACC, it may not be the most performant option on the market. The clang/LLVM ecosystem is working hard for full support of the newest OpenMP standards. The AMD and new oneAPI Intel compilers are in fact fully based on clang and LLVM using some of their own plug-ins to offer additional features, and the NVIDIA HPC compiler also largely seems to be based on this technology. NVIDIA and AMD also use the LLVM backend to compile CUDA and HIP kernels respectively, showing once more that this is the compiler ecosystem for the future of scientific computing. C++ extensions \u00b6 SYCL \u00b6 SYCL is a programming model based on recent versions of the C++ standard. The earlier drafts of the standard go back to the 2014-2015 time frame, but SYCL really took off with the 2020 version of the standard. SYCL is also a logical successor to OpenCL, but now making it possible to target CPU, GPU, FPGA and possibly other types of accelerators from a single code base. Though the programmer is of course free to provide alternative implementations for different hardware for better performance, it is not needed by design. SYCL is heavily based on C++ template libraries, but to generate code for accelerators it still needs a specific compiler. There are several compilers in developmentwith varying levels of support for the standard, targeting not only GPUs, but also, e.g., the NEC SX Aurora Tsubasa vector boards. Most if not all of these implemenations are again based on Clang and LLVM. One implementation worth mentioning is hipSYCL, which as its name suggests targets hip for the backend and hence can support both AMD and NVIDIA GPUs, but can also target pure CPU systems and now even contains experimental support for Intel GPUs. DPC++ or Data Parallel C++ \u00b6 Data Parallel C++ is the oneAPI implementation of SYCL. It is basically a project by Intel to bring SYCL into LLVM, and all code is open sourced. The implementation does not only cover CPUs and Intel GPUs, but with the help of others (including the company Codeplay that has since been acquired by Intel) it also adds support for NVIDIA and AMD GPUs and even Intel FPGAs, the latter through a backend based on OpenCL and SPIR. That support is not included in the binaries that Intel distributes though. When DPC++ initially launched, it was really an extension of the then-current SYCL standard, which is why it gets a separate subsection in these notes. However, it is currently promoted as a SYCL 2020 implementation. C++AMP \u00b6 C++AMP or C++ Accelerated Massive Parallelism is a programming model developed by Microsoft. It consists of C++ libraries and a minor extension to the language. There are experimental implementations for non-Microsoft environments, but these are not really popular and the technology is not really taking off in scientific computing. The technology is now deprecated by Microsoft. Yet we want to mention it in these notes as it was a source of inspiration for SYCL. Frameworks \u00b6 There exist also several C++ frameworks or abstraction libraries that support creating code that is portable to regular CPUs and GPU systems of various vendors. They let you exploit all levels of parallelism in a supercomputer except distributed computing. Kokkos is a framework developed by Sandia National Labs and probably the most popular one of the frameworks mentioned here. It was first rteleased in 2011 already but grew to a complete ecosystem with tools to support debuging, profiling and tuning also, and now even some support for distributed computing also. Kokkos already supports backends for CUDA and ROCm, and there are experimental backends that can also support the Intel GPUs that will be used in the Aurora supercomputer. RAJA is a framework developed at Lawrence Livermore National Laboratory, based on standard C++11. Just as Kokkos, RAJA has several backends supporting SIMD, threading through the TBB library or OpenMP, but also GPU computing through NVIDIA CUDA, AMD HIP and OpenMP offload, though not all back-ends are as mature or support all features.In particular the TBB and OpenMP target offload (the latter needed for Intel GPUs) are still experimental at the time this section was written (October 2022). Alpaka is a framework developed by CASUS - Center for Advanced Systems Understanding of the Helmholtz Zentrum Dresden Rossendorf. Alpaka also supports various backends, including a CUDA back-end for NVIDIA GPUs. There is also work going on on a HIP backend for AMD GPUs, with support for Intel GPUs coming through an OpenMP offload backend. Libraries \u00b6 One can often rely on already existing libraries when developing software for GPUs. NVIDIA CUDA comes with a wide range of libraries. MAD ROCm provides several libraries that mimic (subsets of) libraries in the CUDA ecosystem, also easing porting from NVIDIA to AMD hardware. Intel has adapted several of its CPU librararies to use GPU acceleration also in its oneAPI platform. However, there are also several vendor-neutral libraries, e.g., MAGMA which stands for Matrix Algebra on GPU and Multicore Architectures, which is an early example of such a library. heFFTe is a library for FFT","title":"Accelerator prorgramming"},{"location":"7_Accelerators/7_04_Programming_accelerators/#accelerator-programming","text":"We will restrict ourselves to GPGPU programming as these are currently the most widely used accelerators in supercomputers.","title":"Accelerator programming"},{"location":"7_Accelerators/7_04_Programming_accelerators/#the-current-state","text":"Accelerator programming is, also due to the early stage of the technology, still a mess, and standardisation still has to set in. This is not uncommon in the world of supercomputing: The same happened with message passing where there were several proprietary technologies until the needs were sufficiently well understood to come to a standardisation. Currently there are three competing ecosystems growing: NVIDIA was the first to market with a fairly complete ecosystem for GPGPU programming. They chose to make large parts of their technology proprietary to create a vendor lock-in and keep hardware prices high. CUDA is the main NVIDIA ecosystem. NVIDIA together with partners also created a set of compiler pragma's for C/C++ and Fortran for GPGPU programming with an open consortium, but many other compiler vendors are hesitant to pick those up. The NVIDIA toolset also offers some support for open, broadly standardised technologies, but the support is usually inferior to that for their proprietary technologies or standards that are in practice largely controlled by them. AMD is a latecomer to the market. Their software stack is called ROCm, and they open sourced all components to gain more traction in the market. They basically focus on two technologies that we will discuss: HIP and Open MP. Intel calls its ecosystem oneAPI, as it tries to unify CPU and GPGPU programming, with libraries that make it easy to switch between a CPU-only version and a GPGPU-accelerated version of an application. The oneAPI ecosystem is largely based on Data Parallel C++, an extension of SYCL, and Open MP compiler pragmas. The oneAPI is partly open-sourced. In some cases, only the API is available and vendors should make their own implementation, but, e.g., the sources for their clang/LLVM based Data Parallel C++ compiler are available to all and this has been used already to make ports to NVIDIA and AMD GPU hardware.","title":"The current state"},{"location":"7_Accelerators/7_04_Programming_accelerators/#lower-level-models","text":"These models use separate code for the host and the accelerator devices that are then linked together to create the application binary.","title":"Lower-level models"},{"location":"7_Accelerators/7_04_Programming_accelerators/#cuda","text":"CUDA is the best known environment in the HPC world. It is however a proprietary NVIDIA technology. It is possible to write GPGPU kernels using subsets of C, C++ and Fortran. CUDA also comes with many libraries with optimised routines for linear algebra, FFT, neural networks, etc. This makes it by far the most extensive of the lower-level models. CUDA, which launched in 2007, has gone through a rapid evolution though it is now reaching a level of maturity with less frequent major updates.","title":"CUDA"},{"location":"7_Accelerators/7_04_Programming_accelerators/#hip","text":"HIP or H eterogeneous Computing I nterface for P ortability is AMD's alternative to CUDA. It tries to be as close as possible as legally possible. The HIP API maps one-to-one on the CUDA API, making it possible to recompile HIP code with the CUDA tools using just a header file. In theory this process is without loss of performance, but there is one caveat: A kernel that is efficient on an AMD GPU may not be the most efficient option on a NVIDIA GPU as, e.g., the vector size is different. So careful programming is needed to ensure efficiency on both platforms. HIP is purely based on C++ with currently no support for Fortran GPU kernels. Feature-wise it is roughly comparable to CUDA 7 or 8, but features that cannot yet be supported by AMD hardware are missing. The ROCm platform also comes with a lot of libraries that provide APIs that are also similar to the APIs provided by their CUDA counterpart to further ease porting code from the CUDA ecosystem to the AMD ROCm ecosystem. HIP also comes with two tools that help in reworking CUDA code into HIP code, though both typically need some manual intervention.","title":"HIP"},{"location":"7_Accelerators/7_04_Programming_accelerators/#opencl","text":"OpenCL or Open Compute Language is a framework for heterogeneous computing developed by the non-prpfit, member-driven technology consortium Khronos Group which manages many standards for GPU software (including also OpenGL and its offsprings and Vulkan). It develops vendor-neutral standards. OpenCL is a C and C++-based technology to develop code that not only runs on GPU accelerators, but the code can also be recompiled for CPU only, and some DSPs and FPGAs are also supported. This makes it a very versatile standard and it was for a long time a very attractive standard for commercial software development. It is by far not as advanced as CUD or not even HIP. The evolution of the standard has been very slow lately, with vendors hardly picking up many of the features of the 2.x versions. This lead to version 3.0 of the standard that defined a clear baseline and made the other features explicitly optional so that programmers at least know what they can expect. OpenCL is in principle supported on NVIDIA, AMD and Intel hardware, but the quality of the implementation is not always spectacular. There are also some open source implementations with varying hardware support. It has been used in supercomputing software though it was not always the only model offered as several packages also contained specialised CUDA code for better performance on NVIDIA hardware. The molecular dynamics package GROMACS is one example, and they will be switching away from OpenCL to newer technologies to support non-NVIDIA hardware. OpenCL is largely superseded by newer technologies developed by the Khronos Group. Vulkan is their effort to create an API that unifies 3D graphics and computing and is mostly oriented towards game programming etc., but less towards supercomputing (as several GPUs used for supercomputing even start omitting graphics-specific circuits) while SYCL, which we will discuss later in these notes, is a new initiative for GPGPU programming.","title":"OpenCL"},{"location":"7_Accelerators/7_04_Programming_accelerators/#compiler-directives","text":"In these models, there are no separate sources for the GPU that are compiled with a separate compiler. Instead, there is a single base of code with compiler directives instructing the compiler how certain parts of the code can be offloaded to an accelerator. These directives appear as comments in a Fortran code to compilers that do not support the technology or use the #pragma directive in C/C++ to make clear that they are directives to compilers that support those.","title":"Compiler directives"},{"location":"7_Accelerators/7_04_Programming_accelerators/#openacc","text":"OpenACC was the first successful compiler directive model for GPGPU computing. It supports C, C++ and Fortran programming. The first version of standard made its debut at the supercomputing computing conference SC'11 in November 2011. The technology was largely developed by 4 companies: the GPU company NVIDIA, the compiler company Portland Group which was later renamed PGI and later bought by NVIDIA, the supercomputer manufacturer Cray which got bought by HPE and largely lost interest in OpenACC, now more promoting an alternative, and CAPS Entreprises, a French company and university spin-off creating tools for accelerator programming that failed in 2013. The OpenACC standard is currently controlled by the OpenACC Organization, and though it does count other companies that develop GPUs among its members, it still seems rather dominated by NVIDIA which may explain why other companies are somewhat hesitant to pick up the standard. The standard is at the time of writing of this section at version 2.7, released in November 2018. OpenACC is well supported on NVIDIA hardware through the NVIDIA HPC compilers (which is the new name of the PGI compilers adopted after the integration of PGI in NVIDIA). GCC offers some support on NVIDIA and some AMD hardware for version 2.6 of the standard since version 10 of the GCC compilers., but the evolution is slow and performance is often not that great. More promising for the future is the work going on in the clang/LLVM community to support OpenACC, as this effort is largely driven by the USA national labs who want to avoid having to port all OpenACC-based code developed for the NVIDIA based pre-exascale supercomputers to other models to run on the new AMD and Intel based exascale supercomputers. In fact, the clang/LLVM ecosystem is the future for scientific computing and not the GNU Compiler Collection ecosystem as most compiler vendors already base their compilers on that technology. The NVIDIA, AMD and new Intel compilers are all based on LLVM and the clang frontend for C and C++-support, with NVIDIA and AMD still using a Fortran front-end developed by PGI and donated to the LLVM project while Intel is already experimenting with a new community-developed more modern front-end for Fortran. OpenACC support in the LLVM ecosystem will build upon the OpenMP support, the technology that we will discuss next, using extensions for those OpenACC features that still have no equivalent in OpenMP.","title":"OpenACC"},{"location":"7_Accelerators/7_04_Programming_accelerators/#openmp","text":"We've already discussed OpenMP as a technology for shared memory parallelism and for vectorisation (the latter since version 4.0 of the standard). But OpenMP is nowadays even more versatile. Since version 4.0 of the standard, released in July 2013, there is also support for offloading to accelerators. That support was greatly improved in version 5.0 of the standard which was released at the SC'18 supercomputer conference. It became a more descriptive and less prescriptive model (offering the compiler enough information to decide what it should do rather then enforcing the compiler to do something in a particular way), with the prescriptive nature being critisized a lot by the OpenACC community who claimed superiority because of this. It also contained much better support for debuggers, performance monitoring tools, etc. OpenMP has since had minor extensions i the form of version 5.1 at SC'20 and 5.2 at SC'21. The standard is controlled by a much larger consortium than the OpenACC standard. OpenMP is an important technology in the AMD ROCm and Intel oneAPI ecosystems. Intel has in fact supported OpenMP offload to some of its own GPUs for many years, long before establishing the oneAPI ecosystem. GCC has had some support for OpenMP offload to NVIDIA hardware since version 7 and to some AMD hardware since version 10. However, as is the case for OpenACC, it may not be the most performant option on the market. The clang/LLVM ecosystem is working hard for full support of the newest OpenMP standards. The AMD and new oneAPI Intel compilers are in fact fully based on clang and LLVM using some of their own plug-ins to offer additional features, and the NVIDIA HPC compiler also largely seems to be based on this technology. NVIDIA and AMD also use the LLVM backend to compile CUDA and HIP kernels respectively, showing once more that this is the compiler ecosystem for the future of scientific computing.","title":"OpenMP"},{"location":"7_Accelerators/7_04_Programming_accelerators/#c-extensions","text":"","title":"C++ extensions"},{"location":"7_Accelerators/7_04_Programming_accelerators/#sycl","text":"SYCL is a programming model based on recent versions of the C++ standard. The earlier drafts of the standard go back to the 2014-2015 time frame, but SYCL really took off with the 2020 version of the standard. SYCL is also a logical successor to OpenCL, but now making it possible to target CPU, GPU, FPGA and possibly other types of accelerators from a single code base. Though the programmer is of course free to provide alternative implementations for different hardware for better performance, it is not needed by design. SYCL is heavily based on C++ template libraries, but to generate code for accelerators it still needs a specific compiler. There are several compilers in developmentwith varying levels of support for the standard, targeting not only GPUs, but also, e.g., the NEC SX Aurora Tsubasa vector boards. Most if not all of these implemenations are again based on Clang and LLVM. One implementation worth mentioning is hipSYCL, which as its name suggests targets hip for the backend and hence can support both AMD and NVIDIA GPUs, but can also target pure CPU systems and now even contains experimental support for Intel GPUs.","title":"SYCL"},{"location":"7_Accelerators/7_04_Programming_accelerators/#dpc-or-data-parallel-c","text":"Data Parallel C++ is the oneAPI implementation of SYCL. It is basically a project by Intel to bring SYCL into LLVM, and all code is open sourced. The implementation does not only cover CPUs and Intel GPUs, but with the help of others (including the company Codeplay that has since been acquired by Intel) it also adds support for NVIDIA and AMD GPUs and even Intel FPGAs, the latter through a backend based on OpenCL and SPIR. That support is not included in the binaries that Intel distributes though. When DPC++ initially launched, it was really an extension of the then-current SYCL standard, which is why it gets a separate subsection in these notes. However, it is currently promoted as a SYCL 2020 implementation.","title":"DPC++ or Data Parallel C++"},{"location":"7_Accelerators/7_04_Programming_accelerators/#camp","text":"C++AMP or C++ Accelerated Massive Parallelism is a programming model developed by Microsoft. It consists of C++ libraries and a minor extension to the language. There are experimental implementations for non-Microsoft environments, but these are not really popular and the technology is not really taking off in scientific computing. The technology is now deprecated by Microsoft. Yet we want to mention it in these notes as it was a source of inspiration for SYCL.","title":"C++AMP"},{"location":"7_Accelerators/7_04_Programming_accelerators/#frameworks","text":"There exist also several C++ frameworks or abstraction libraries that support creating code that is portable to regular CPUs and GPU systems of various vendors. They let you exploit all levels of parallelism in a supercomputer except distributed computing. Kokkos is a framework developed by Sandia National Labs and probably the most popular one of the frameworks mentioned here. It was first rteleased in 2011 already but grew to a complete ecosystem with tools to support debuging, profiling and tuning also, and now even some support for distributed computing also. Kokkos already supports backends for CUDA and ROCm, and there are experimental backends that can also support the Intel GPUs that will be used in the Aurora supercomputer. RAJA is a framework developed at Lawrence Livermore National Laboratory, based on standard C++11. Just as Kokkos, RAJA has several backends supporting SIMD, threading through the TBB library or OpenMP, but also GPU computing through NVIDIA CUDA, AMD HIP and OpenMP offload, though not all back-ends are as mature or support all features.In particular the TBB and OpenMP target offload (the latter needed for Intel GPUs) are still experimental at the time this section was written (October 2022). Alpaka is a framework developed by CASUS - Center for Advanced Systems Understanding of the Helmholtz Zentrum Dresden Rossendorf. Alpaka also supports various backends, including a CUDA back-end for NVIDIA GPUs. There is also work going on on a HIP backend for AMD GPUs, with support for Intel GPUs coming through an OpenMP offload backend.","title":"Frameworks"},{"location":"7_Accelerators/7_04_Programming_accelerators/#libraries","text":"One can often rely on already existing libraries when developing software for GPUs. NVIDIA CUDA comes with a wide range of libraries. MAD ROCm provides several libraries that mimic (subsets of) libraries in the CUDA ecosystem, also easing porting from NVIDIA to AMD hardware. Intel has adapted several of its CPU librararies to use GPU acceleration also in its oneAPI platform. However, there are also several vendor-neutral libraries, e.g., MAGMA which stands for Matrix Algebra on GPU and Multicore Architectures, which is an early example of such a library. heFFTe is a library for FFT","title":"Libraries"},{"location":"7_Accelerators/7_05_Status_GPU_computing/","text":"Status of GPU computing \u00b6 Too much hype \u00b6 Even though GPU computing, or accelerator computing in general, is definitely here to stay and important for the future of not only supercomputing but computing in general as what can be done with a given amount of Watts of power is important in many markets (also on mobiles that have to get their power from a battery), it does not mean that it works for all applications. Benchmarking of supercomputers with accelerators is often more benchmark et ing. GPU computing is often overhyped using incomplete benchmarks (basically only benchmark that part of the code that can be properly accelerated), marketing by numbers (redefine common terms to get bigger numbers, something that in particular NVIDIA is very good at) and comparing apples and oranges by comparing systems with a very different price or total cost of ownership, e.g., comparing a server with multiple accelerators costing 60k EURO or more with a standard dual socket server costing only 10k EURO and using only one fifth of the power. The NVIDIA vendor lock-in and its success in the market have made accelerators very expensive. At the current price point, GPU computing only makes sense from a price point of view if the speed-up at the application level is a factor of 2.5 or more per accelerator card compared to a standard medium-sized dual socket node. As we have seen, some features of accelerators have been carried over to traditional CPUs in the form of new instructions supporting vector and nowadays even matrix computing, and in some cases they may just be the better choice as CPUs have more memory readily available and as programming is easier. Problems and solutions \u00b6 There are several problems with current GPU designs: The amount of memory that a GPU can address directly and has fast enough access to, is limited. 2020 GPUs were limited to 32-48 GB of memory, in early 2021 a 80 GB GPU appeared on the market, and in 2022 it is technically possible to have 128 GB on a single package. But this is still small to typical memory sizes on a regular dual socket server that is much slower than the GPU. Programming bottleneck: Having to organise all data transport manually and working with separate memory spaces is a pain. The link between the CPU and the GPU is a bottleneck. The PCIe buss that typically links the CPU to the GPU has a rather limited bandwidth compared to either the bandwidth of the CPU memory of the bandwidth of GPU memory. The GPU is rather slow for serial code, so that code often has to be rn on the host. Which is then an issue since it may require additional copying between the CPU and GPU. However, there is hope. The amount of memory that can be used by a GPU will increase a lot the coming years. Both the memory packages are getting bigger by stacking more dies in 3D, and the number of memory packages that can be integrated in the overall GPU package is increasing. As of 2022, 8 memory stacks in a GPU package is feasible as is shown by the AMD MI250(X) GPUs, while in 2023-2024 12 memory stacks in a single GPU package should become a possibility. The two combined may make memory sizes of 192 and likely even 384 GB possible by 2024. The programming bottleneck can already be partially solved by unified memory, using memory pointers that work on both CPU and GPU, and further hardware support for virtual memory that can then trigger software that migrates memory pages under the hood. NVIDIA GPUs have had some of those features since the Pascal generation in 2017. However, one can do even better by physically sharing memory spaces between GPU and CPU, and supporting some level of coherency so that the CPU can access the GPU memory without risk of inconsistent data, or even the GPU can access the memory of the CPU, though that is less interesting as the CPU can never provide the memory bandwidth that the GPU needs to perform well. The physically shared memory spaces were first explored in the Sierra and Summit USA pre-exascale systems (as we discussed before) but is now also seen in the MI250X GPU from AMD which is a special version of the MI200 family connecting to the CPU through InfinityFabric, the same interconnect that AMD uses internally in its sockets to link the CPU dies to the I/O die, and also uses to connect CPU sockets or to connect GPUs to each other in the MI100 and MI200 generations. The Intel Ponte Vecchio GPU combined with the Sapphire Rapids CPU that will be used in the Aurora supercomputer supports a similar feature, as does the NVIDIA Grace CPU and Hopper GPU integrated in a single package. The physical sharing of memory spaces with a level of cache coherency is also the first step in solving the problem of copying data back and forth all the time. E.g., if a CPU can access memory attached to the GPU without risks of coherency problems, then there is no need to copy full memory pages and also not to copy those back, as the link that is used in those GPU systems to connect the CPU to GPU is as fast as the links that are typically used to connect two CPU sockets. The GRACE-Hopper integration shows the next step. By integrating the CPU and GPU in the same package, it is possible to have a much higher bandwidth between both, reducing the copying bottleneck in cases where copying is still needed. IF there is full cache coherency, this really creates a NUMA-like memory model where it starts to matter less where in memory which data is stored. However, with the AMD MI300 and Intel Falcon Shores and without doubt a future unannounced NVIDIA product, we will see even closer integration where CPU and GPU chiplets share the memory controllers. The Apple M series chips give an indication of what can be obtained with such a system, as it performs way better in some applications that use acceleration than one would expect from looking at systems with discrete GPUs with similar theoretical performance. Evolution of GPU nodes \u00b6 Around 2016, a typical GPU compute node consisted of a dual socket server with 1-4 GPUs attached to the CPUs. A typcial design would have been: The red line between the two CPUs denotes a fully cache coherent link between the CPUs. In 2016 this would very likely have been either Intel CPUs or IBM POWER CPUs, and both had proprietary fully cache coherent links to link CPUs in a shared memory system. The red link between the GPUs denotes a similarly proprietary connection between the CPUs for easy data transfer between the CPUs at typically a much higher bandwidth than\\ that offered by the connections between the CPU and GPU. However, not all systems used such a link between graphics cards. A CPU was connected to the GPUs using PCIe, and similarly a network interface would also be connected to a CPU using PCIe. Later designs tried to move the network interconnect closer to the GPUs, as they perform the majority of the calculations and hence also contain the data that should be send to other nodes. A typical 4-GPU node based on the NVIDIA Ampere A100 GPU launched in 2020 would look similar to: There are many variants of quad GPU designs with the A100 GPU, with single and dual socket CPU servers. However, it is often advantageous to have all GPUs connected to a single CPU, but only the single socket AMD Epyc CPU has enough PCIe connections to do so and still be able to also attach one or two network interface cards. The above design solves this in a different way. It uses two PCIe switches (the magenta circles), and each PCIe switch connects the CPU to two of the GPUs and a network interface card (the latter denoted by the dark purple line). This also brings the network very close to the GPU. The next evolution of this design is used in the GPU compute nodes of the Frontier and LUMI supercomputers based on the MI250X GPU. A simplified diagram of these nodes is: The GPU compute nodes of LUMI and Frontier use a special variant of the Zen3-based AMD Epyc processor. In this variant, the PCIe lanes are replaced by InfinityFabric connections. In an MI250X, the 4 GPU packages are connected with each other and with the CPU through InfinitFabric links. Each GPU package connects to its own quarter of the AMD Epyc processor (remember from earlier that the I/O die is subdivided in 4 quarters, in this case each connected to two CPU chiplets with 8 cores each). This creates a fully unified and coherent memory space. Also noteworthy is that the interconnect is no longer connected to the CPU, but the 4 network cards are each connected to a GPU package (through a PCIe interface). This makes this compute node really a GPU-first system, almost a system where the CPU is only used for those parts of the code that cannot be accelerated at all by a GPU and to run the Linux operating system. The true picture of the GPU node is a bit more complicated though. Each GPU package contains two GPU dies, and these are connected to each other through some InfinitFabric links. Each GPU die connects to 4 memory packages, with 64 GB of memory per GPU die. However, the connection between two GPU dies is sufficiently bandwidth-starved that programming wise a single GPU package should be considered as two separate GPUs. Each individual GPU die has its own InfinityFabric link to the CPU and seems to have a preferential CPU chiplet. Even though the connection between the GPU packages appears to be an all-to-all conection, this is not true when one looks at the connections between the GPU dies. The AMD MI250X is really just a transition to the MI300 series and the Intel Falcon Shores architecture. In that generation, expected to come to market in late 2023 in the case of AMD and 2024 or 2025 in the case of Intel, the CPU and GPU will merge completely and share on-package memory. In fact, the reality is that memory outside the package is also starting to limit CPU performance as an increasing number of CPU codes becomes memory bandwidth bound, so even for the CPU it makes sense to switch to smaller but much higher bandwidth memory in the package. The AMD MI300 and Intel Falcon Shores will fully integrate the CPU and GPU chiplets and memory controllers with memory in a single package, which would lead so a supercomputer node similar to Here we see four packages intgrating one or more CPU chiplets, one or more GPU dies and memory in a single packages. The four packages have an all-to-all connection likely using a new generation of InfinityFabric, and each GPYU packages also connects to a network card using PCIe. It is expected that the techniques to connect dies will have evolved enough that the GPU dies in a single package will work as a single GPU. In fact, those improved connections will also be needed to have equal access to memory from the GPU and CPU chiplets.","title":"Status of GPU computing"},{"location":"7_Accelerators/7_05_Status_GPU_computing/#status-of-gpu-computing","text":"","title":"Status of GPU computing"},{"location":"7_Accelerators/7_05_Status_GPU_computing/#too-much-hype","text":"Even though GPU computing, or accelerator computing in general, is definitely here to stay and important for the future of not only supercomputing but computing in general as what can be done with a given amount of Watts of power is important in many markets (also on mobiles that have to get their power from a battery), it does not mean that it works for all applications. Benchmarking of supercomputers with accelerators is often more benchmark et ing. GPU computing is often overhyped using incomplete benchmarks (basically only benchmark that part of the code that can be properly accelerated), marketing by numbers (redefine common terms to get bigger numbers, something that in particular NVIDIA is very good at) and comparing apples and oranges by comparing systems with a very different price or total cost of ownership, e.g., comparing a server with multiple accelerators costing 60k EURO or more with a standard dual socket server costing only 10k EURO and using only one fifth of the power. The NVIDIA vendor lock-in and its success in the market have made accelerators very expensive. At the current price point, GPU computing only makes sense from a price point of view if the speed-up at the application level is a factor of 2.5 or more per accelerator card compared to a standard medium-sized dual socket node. As we have seen, some features of accelerators have been carried over to traditional CPUs in the form of new instructions supporting vector and nowadays even matrix computing, and in some cases they may just be the better choice as CPUs have more memory readily available and as programming is easier.","title":"Too much hype"},{"location":"7_Accelerators/7_05_Status_GPU_computing/#problems-and-solutions","text":"There are several problems with current GPU designs: The amount of memory that a GPU can address directly and has fast enough access to, is limited. 2020 GPUs were limited to 32-48 GB of memory, in early 2021 a 80 GB GPU appeared on the market, and in 2022 it is technically possible to have 128 GB on a single package. But this is still small to typical memory sizes on a regular dual socket server that is much slower than the GPU. Programming bottleneck: Having to organise all data transport manually and working with separate memory spaces is a pain. The link between the CPU and the GPU is a bottleneck. The PCIe buss that typically links the CPU to the GPU has a rather limited bandwidth compared to either the bandwidth of the CPU memory of the bandwidth of GPU memory. The GPU is rather slow for serial code, so that code often has to be rn on the host. Which is then an issue since it may require additional copying between the CPU and GPU. However, there is hope. The amount of memory that can be used by a GPU will increase a lot the coming years. Both the memory packages are getting bigger by stacking more dies in 3D, and the number of memory packages that can be integrated in the overall GPU package is increasing. As of 2022, 8 memory stacks in a GPU package is feasible as is shown by the AMD MI250(X) GPUs, while in 2023-2024 12 memory stacks in a single GPU package should become a possibility. The two combined may make memory sizes of 192 and likely even 384 GB possible by 2024. The programming bottleneck can already be partially solved by unified memory, using memory pointers that work on both CPU and GPU, and further hardware support for virtual memory that can then trigger software that migrates memory pages under the hood. NVIDIA GPUs have had some of those features since the Pascal generation in 2017. However, one can do even better by physically sharing memory spaces between GPU and CPU, and supporting some level of coherency so that the CPU can access the GPU memory without risk of inconsistent data, or even the GPU can access the memory of the CPU, though that is less interesting as the CPU can never provide the memory bandwidth that the GPU needs to perform well. The physically shared memory spaces were first explored in the Sierra and Summit USA pre-exascale systems (as we discussed before) but is now also seen in the MI250X GPU from AMD which is a special version of the MI200 family connecting to the CPU through InfinityFabric, the same interconnect that AMD uses internally in its sockets to link the CPU dies to the I/O die, and also uses to connect CPU sockets or to connect GPUs to each other in the MI100 and MI200 generations. The Intel Ponte Vecchio GPU combined with the Sapphire Rapids CPU that will be used in the Aurora supercomputer supports a similar feature, as does the NVIDIA Grace CPU and Hopper GPU integrated in a single package. The physical sharing of memory spaces with a level of cache coherency is also the first step in solving the problem of copying data back and forth all the time. E.g., if a CPU can access memory attached to the GPU without risks of coherency problems, then there is no need to copy full memory pages and also not to copy those back, as the link that is used in those GPU systems to connect the CPU to GPU is as fast as the links that are typically used to connect two CPU sockets. The GRACE-Hopper integration shows the next step. By integrating the CPU and GPU in the same package, it is possible to have a much higher bandwidth between both, reducing the copying bottleneck in cases where copying is still needed. IF there is full cache coherency, this really creates a NUMA-like memory model where it starts to matter less where in memory which data is stored. However, with the AMD MI300 and Intel Falcon Shores and without doubt a future unannounced NVIDIA product, we will see even closer integration where CPU and GPU chiplets share the memory controllers. The Apple M series chips give an indication of what can be obtained with such a system, as it performs way better in some applications that use acceleration than one would expect from looking at systems with discrete GPUs with similar theoretical performance.","title":"Problems and solutions"},{"location":"7_Accelerators/7_05_Status_GPU_computing/#evolution-of-gpu-nodes","text":"Around 2016, a typical GPU compute node consisted of a dual socket server with 1-4 GPUs attached to the CPUs. A typcial design would have been: The red line between the two CPUs denotes a fully cache coherent link between the CPUs. In 2016 this would very likely have been either Intel CPUs or IBM POWER CPUs, and both had proprietary fully cache coherent links to link CPUs in a shared memory system. The red link between the GPUs denotes a similarly proprietary connection between the CPUs for easy data transfer between the CPUs at typically a much higher bandwidth than\\ that offered by the connections between the CPU and GPU. However, not all systems used such a link between graphics cards. A CPU was connected to the GPUs using PCIe, and similarly a network interface would also be connected to a CPU using PCIe. Later designs tried to move the network interconnect closer to the GPUs, as they perform the majority of the calculations and hence also contain the data that should be send to other nodes. A typical 4-GPU node based on the NVIDIA Ampere A100 GPU launched in 2020 would look similar to: There are many variants of quad GPU designs with the A100 GPU, with single and dual socket CPU servers. However, it is often advantageous to have all GPUs connected to a single CPU, but only the single socket AMD Epyc CPU has enough PCIe connections to do so and still be able to also attach one or two network interface cards. The above design solves this in a different way. It uses two PCIe switches (the magenta circles), and each PCIe switch connects the CPU to two of the GPUs and a network interface card (the latter denoted by the dark purple line). This also brings the network very close to the GPU. The next evolution of this design is used in the GPU compute nodes of the Frontier and LUMI supercomputers based on the MI250X GPU. A simplified diagram of these nodes is: The GPU compute nodes of LUMI and Frontier use a special variant of the Zen3-based AMD Epyc processor. In this variant, the PCIe lanes are replaced by InfinityFabric connections. In an MI250X, the 4 GPU packages are connected with each other and with the CPU through InfinitFabric links. Each GPU package connects to its own quarter of the AMD Epyc processor (remember from earlier that the I/O die is subdivided in 4 quarters, in this case each connected to two CPU chiplets with 8 cores each). This creates a fully unified and coherent memory space. Also noteworthy is that the interconnect is no longer connected to the CPU, but the 4 network cards are each connected to a GPU package (through a PCIe interface). This makes this compute node really a GPU-first system, almost a system where the CPU is only used for those parts of the code that cannot be accelerated at all by a GPU and to run the Linux operating system. The true picture of the GPU node is a bit more complicated though. Each GPU package contains two GPU dies, and these are connected to each other through some InfinitFabric links. Each GPU die connects to 4 memory packages, with 64 GB of memory per GPU die. However, the connection between two GPU dies is sufficiently bandwidth-starved that programming wise a single GPU package should be considered as two separate GPUs. Each individual GPU die has its own InfinityFabric link to the CPU and seems to have a preferential CPU chiplet. Even though the connection between the GPU packages appears to be an all-to-all conection, this is not true when one looks at the connections between the GPU dies. The AMD MI250X is really just a transition to the MI300 series and the Intel Falcon Shores architecture. In that generation, expected to come to market in late 2023 in the case of AMD and 2024 or 2025 in the case of Intel, the CPU and GPU will merge completely and share on-package memory. In fact, the reality is that memory outside the package is also starting to limit CPU performance as an increasing number of CPU codes becomes memory bandwidth bound, so even for the CPU it makes sense to switch to smaller but much higher bandwidth memory in the package. The AMD MI300 and Intel Falcon Shores will fully integrate the CPU and GPU chiplets and memory controllers with memory in a single package, which would lead so a supercomputer node similar to Here we see four packages intgrating one or more CPU chiplets, one or more GPU dies and memory in a single packages. The four packages have an all-to-all connection likely using a new generation of InfinityFabric, and each GPYU packages also connects to a network card using PCIe. It is expected that the techniques to connect dies will have evolved enough that the GPU dies in a single package will work as a single GPU. In fact, those improved connections will also be needed to have equal access to memory from the GPU and CPU chiplets.","title":"Evolution of GPU nodes"},{"location":"7_Accelerators/7_06_Further_reading/","text":"Further reading \u00b6 AMD GPUs Whitepaper: Introducing AMD CDNA TM 2 Architecture which includes information on how the MI250X connects to the CPU, and on the vector and matrix cores of the GPU. HPCwire article on the AMD MI300 APU that will be used in the El Capitan supercomputer. AnandTech article on the MI300 , based on information from AMD's 2022 Inverstors Day. The article shows some not very detailed slides about the memory architecture. NVIDIA GPUs Whitepaper: Summit and Sierra Supercomputers: An Inside Look at the U.S. Department of Energy\u2019s New Pre-Exascale Systems for information on the physically unified memory spece made possible by the NVLink connections between the CPUs and GPUs. Whitepaper: NVIDIA Grace Hopper Superchip Architecture for all information on the combination of the Grace ARM-based CPU and the Hopper GPUs. NVIDIA CUDA NVIDIA CUDA toolkit AMD ROCm AMD ROCm information portal AMD HIP fundamentals HIP API Guides Intel oneAPI Intel oneAPI overview OpenCL Khronos Group OpenCL documentation OpenACC The OpenACC Organization OpenMP OpenMP Architecture Review Board web site SYCL Khronos Group SYCL documentation hipSYCL Intel oneAPI DPC++/C++ compiler Intel oneAPI DPC++ compiler GitHub Kokkos Kokkos ecosystem home page RAJA framework RAJA home page RAJA documentation Alpaka abstraction library Alpaka information on the CASUS web site Alpaka information on GitHub Alpaka training from 2020 on YouTube Libraries MAGMA heFFTe","title":"Further reading"},{"location":"7_Accelerators/7_06_Further_reading/#further-reading","text":"AMD GPUs Whitepaper: Introducing AMD CDNA TM 2 Architecture which includes information on how the MI250X connects to the CPU, and on the vector and matrix cores of the GPU. HPCwire article on the AMD MI300 APU that will be used in the El Capitan supercomputer. AnandTech article on the MI300 , based on information from AMD's 2022 Inverstors Day. The article shows some not very detailed slides about the memory architecture. NVIDIA GPUs Whitepaper: Summit and Sierra Supercomputers: An Inside Look at the U.S. Department of Energy\u2019s New Pre-Exascale Systems for information on the physically unified memory spece made possible by the NVLink connections between the CPUs and GPUs. Whitepaper: NVIDIA Grace Hopper Superchip Architecture for all information on the combination of the Grace ARM-based CPU and the Hopper GPUs. NVIDIA CUDA NVIDIA CUDA toolkit AMD ROCm AMD ROCm information portal AMD HIP fundamentals HIP API Guides Intel oneAPI Intel oneAPI overview OpenCL Khronos Group OpenCL documentation OpenACC The OpenACC Organization OpenMP OpenMP Architecture Review Board web site SYCL Khronos Group SYCL documentation hipSYCL Intel oneAPI DPC++/C++ compiler Intel oneAPI DPC++ compiler GitHub Kokkos Kokkos ecosystem home page RAJA framework RAJA home page RAJA documentation Alpaka abstraction library Alpaka information on the CASUS web site Alpaka information on GitHub Alpaka training from 2020 on YouTube Libraries MAGMA heFFTe","title":"Further reading"},{"location":"8_Conclusions/","text":"Conclusions \u00b6","title":"Conclusions"},{"location":"8_Conclusions/#conclusions","text":"","title":"Conclusions"}]}