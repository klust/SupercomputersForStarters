{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Supercomputers for Starters \u00b6 Some preliminary words","title":"Home"},{"location":"#supercomputers-for-starters","text":"Some preliminary words","title":"Supercomputers for Starters"},{"location":"0_00_preliminary/","text":"Some preliminary words \u00b6 These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum .","title":"Preliminary words"},{"location":"0_00_preliminary/#some-preliminary-words","text":"These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum .","title":"Some preliminary words"},{"location":"1_Introduction/","text":"Introduction \u00b6 Goals Why supercomputing? What it is not A compmartmentalised machine Overview of the notes","title":"Introduction"},{"location":"1_Introduction/#introduction","text":"Goals Why supercomputing? What it is not A compmartmentalised machine Overview of the notes","title":"Introduction"},{"location":"1_Introduction/1_01_Goals/","text":"Goals \u00b6 The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a chep PC or smartphone. Whether a supercomputer will be used well dpeends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastrucgture use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the paramters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparant as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_01_Goals/#goals","text":"The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a chep PC or smartphone. Whether a supercomputer will be used well dpeends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastrucgture use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the paramters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparant as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_02_Why/","text":"Why supercomputing? \u00b6 If supercomputing is that complex, why would one want to use it? When a PC or server fails... \u00b6 Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a worstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fiels of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue. Supercomputing jobs \u00b6 From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accomodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demanmd becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the desing of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug desing one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualation of very large data sets or simulation results, were a visualisation workstation may not be enough anymore. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes withs someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be gramatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#why-supercomputing","text":"If supercomputing is that complex, why would one want to use it?","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#when-a-pc-or-server-fails","text":"Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a worstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fiels of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue.","title":"When a PC or server fails..."},{"location":"1_Introduction/1_02_Why/#supercomputing-jobs","text":"From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accomodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demanmd becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the desing of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug desing one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualation of very large data sets or simulation results, were a visualisation workstation may not be enough anymore. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes withs someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be gramatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Supercomputing jobs"},{"location":"1_Introduction/1_03_What_it_is_not/","text":"What supercomputing is not \u00b6 Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous centuty. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '760s, with the advent of so-called vectorsupercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What it is not"},{"location":"1_Introduction/1_03_What_it_is_not/#what-supercomputing-is-not","text":"Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous centuty. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '760s, with the advent of so-called vectorsupercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What supercomputing is not"},{"location":"1_Introduction/1_04_Compartmentalised/","text":"A compmartmentalised supercomputer \u00b6 Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly startingh that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compmartmentalised computer"},{"location":"1_Introduction/1_04_Compartmentalised/#a-compmartmentalised-supercomputer","text":"Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly startingh that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compmartmentalised supercomputer"},{"location":"1_Introduction/1_05_Overview/","text":"Overview of the notes \u00b6","title":"Overview of the notes"},{"location":"1_Introduction/1_05_Overview/#overview-of-the-notes","text":"","title":"Overview of the notes"},{"location":"2_Processors/","text":"Processors for supercomputers \u00b6","title":"Processors for supercomputers"},{"location":"2_Processors/#processors-for-supercomputers","text":"","title":"Processors for supercomputers"},{"location":"3_Memory/","text":"The memory hierarchy \u00b6","title":"The memory hierarchy"},{"location":"3_Memory/#the-memory-hierarchy","text":"","title":"The memory hierarchy"},{"location":"4_Storage/","text":"Storing data on supercomputers \u00b6","title":"Storing data on supercomputers"},{"location":"4_Storage/#storing-data-on-supercomputers","text":"","title":"Storing data on supercomputers"},{"location":"5_Middleware/","text":"Middleware: Turning the hardware into a usable supercomputer \u00b6","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"5_Middleware/#middleware-turning-the-hardware-into-a-usable-supercomputer","text":"","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"6_Expectations/","text":"What can we expect? \u00b6","title":"What can we expect?"},{"location":"6_Expectations/#what-can-we-expect","text":"","title":"What can we expect?"},{"location":"7_Accelerators/","text":"Accelerators \u00b6","title":"Accelerators"},{"location":"7_Accelerators/#accelerators","text":"","title":"Accelerators"},{"location":"8_Conclusions/","text":"Conclusions \u00b6","title":"Conclusions"},{"location":"8_Conclusions/#conclusions","text":"","title":"Conclusions"}]}