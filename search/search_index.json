{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Supercomputers for Starters","text":"<ul> <li>Some preliminary words</li> <li>Introduction<ul> <li>Goals</li> <li>Why supercomputing</li> <li>What it is not</li> <li>A compartmentalised supercomputer</li> <li>Overview of the notes</li> </ul> </li> <li>Processors for supercomputers<ul> <li>Introduction</li> <li>A very basic CPU</li> <li>Instruction level parallelism 1: Pipelining</li> <li>Instruction level parallelism 2: Superscalar processing</li> <li>Data parallelism through vector computing</li> <li>Shared memory multiprocessing</li> <li>Distributed memory computing</li> <li>Parallelism in a modern (super)computer</li> <li>Node architecture</li> <li>Fast (past) evolution</li> <li>An important difference  between a PC and a supercomputer...</li> <li>Lessons learnt</li> <li>Further reading</li> </ul> </li> <li>The memory hierarchy<ul> <li>The memory performance gap</li> <li>The memory pyramid</li> <li>AMD Rome revisited</li> <li>AMD Milan revisited</li> </ul> </li> <li>Storing data on supercomputers<ul> <li>Introduction</li> <li>Problems with a parallel disk setup</li> <li>Parallel file systems</li> <li>A storage revolution?</li> <li>To remember</li> <li>Further reading</li> </ul> </li> <li>Putting it all together<ul> <li>Scaling</li> <li>Dennard scaling</li> <li>Transistor cost</li> <li>Three keywords: Streaming, Parallelism, and Hierarchy</li> <li>Andy and Bill's law</li> <li>Software first, not hardware</li> <li>Building up the supercomputer</li> </ul> </li> <li>Middleware<ul> <li>Introduction</li> <li>Shared memory computing</li> <li>Vector computing</li> <li>Distributed memory supercomputing</li> <li>Some examples</li> <li>Further reading</li> </ul> </li> <li>What can we expect?<ul> <li>Speed-up</li> <li>Illustration: Matrix-matrix multiplication</li> <li>Speed-up and efficiency of DGEMM</li> <li>Conclusions</li> </ul> </li> <li>Accelerators<ul> <li>What are accelerators?</li> <li>Offloading</li> <li>CPUs with accelerator features</li> <li>Accelerator programming</li> <li>Status of GPU computing</li> <li>Further reading</li> </ul> </li> <li>Conclusions</li> </ul>"},{"location":"C00_S00_preliminary/","title":"Some preliminary words","text":"<p>These course notes are document the material in the CalcUA course \"Supercomputers for Starters\" which itself has grown into a course covering more than just the CalcUA infrastructure and may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so.</p> <p>Disclaimer: This is unofficial documentation that is under continuous evolution.  It is the result of my work at the  CalcUA service of the University of Antwerp for the  Vlaams Supercomputer Centrum. </p> <p>The latest version of the corresponding slides is available on the CalcUA training web page. There are also video recordings from the November 2020 edition with some updates from the spring 2021 edition but these may not be world-visible.</p>"},{"location":"TODO/","title":"TODOs","text":"<ul> <li> <p>DONE 2023-10-16: C07_S02_Matrix_multiplication: Blue block missing in the last picture     that explains how the jki-variant works</p> </li> <li> <p>DONE 2023-10-16: Chapter 2: Lessons learnt page is still empty</p> </li> </ul>"},{"location":"tags/","title":"Tags index","text":"<p>Unfortunately the sorting of the tags is still case-sensitive.</p>"},{"location":"tags/#alpaka","title":"Alpaka","text":"<ul> <li>Accelerator programming</li> </ul>"},{"location":"tags/#beowulf","title":"Beowulf","text":"<ul> <li>Distributed memory</li> </ul>"},{"location":"tags/#cuda","title":"CUDA","text":"<ul> <li>What are accelerators?</li> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#fpga","title":"FPGA","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#hip","title":"HIP","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#ilp-instruction-level-parallelism","title":"ILP (Instruction Level Parallelism)","text":"<ul> <li>ILP I - Pipelining</li> <li>ILP II - Superscalar</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> </ul>"},{"location":"tags/#julia","title":"Julia","text":"<ul> <li>Shared memory programming</li> <li>Distributed memory programming</li> <li>Further reading on middleware</li> <li>Conclusions</li> </ul>"},{"location":"tags/#kokkos","title":"Kokkos","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>Distributed memory</li> <li>Distributed memory programming</li> <li>Further reading on middleware</li> </ul>"},{"location":"tags/#numa","title":"NUMA","text":"<ul> <li>Shared memory</li> <li>A modern (super)computer</li> </ul>"},{"location":"tags/#openacc","title":"OpenACC","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#opencl","title":"OpenCL","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#openmp","title":"OpenMP","text":"<ul> <li>Shared memory programming</li> <li>Vector programming</li> <li>Further reading on middleware</li> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#pgas","title":"PGAS","text":"<ul> <li>Distributed memory programming</li> <li>Further reading on middleware</li> </ul>"},{"location":"tags/#raja","title":"RAJA","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#rocm","title":"ROCm","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#smp","title":"SMP","text":"<ul> <li>Shared memory</li> </ul>"},{"location":"tags/#sycl","title":"SYCL","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#core","title":"core","text":"<ul> <li>Shared memory</li> </ul>"},{"location":"tags/#distributed-memory","title":"distributed memory","text":"<ul> <li>Distributed memory</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> <li>Distributed memory programming</li> </ul>"},{"location":"tags/#matrix-accelerator","title":"matrix accelerator","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#matrix-instructions","title":"matrix instructions","text":"<ul> <li>CPUs with accelerator features</li> </ul>"},{"location":"tags/#oneapi","title":"oneAPI","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#pipelined-execution","title":"pipelined execution","text":"<ul> <li>ILP I - Pipelining</li> </ul>"},{"location":"tags/#shared-memory","title":"shared memory","text":"<ul> <li>Shared memory</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> <li>Shared memory programming</li> </ul>"},{"location":"tags/#superscalar-execution","title":"superscalar execution","text":"<ul> <li>ILP II - Superscalar</li> </ul>"},{"location":"tags/#vector-accelerator","title":"vector accelerator","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#vector-instructions","title":"vector instructions","text":"<ul> <li>Vector computing</li> <li>Processors: Lessons learnt</li> <li>Vector programming</li> <li>CPUs with accelerator features</li> </ul>"},{"location":"C01_Introduction/","title":"Introduction","text":"<ul> <li>Goals</li> <li>Why supercomputing?</li> <li>What it is not</li> <li>A compartmentalised supercomputer</li> <li>Overview of the notes</li> </ul>"},{"location":"C01_Introduction/C01_S01_Goals/","title":"Goals","text":"<p>The goals of these lecture notes are that at the end, the reader should be able to answer questions such as:</p> <ul> <li> <p>Why would I consider using a supercomputer?</p> </li> <li> <p>How does supercomputer hardware influence my choice of software or programming techniques?     And is this only so for supercomputers or does it actually also affect other compute devices     such as a regular PC, a tablet or a smartphone?</p> </li> <li> <p>What can and can we not expect from a supercomputer?</p> </li> </ul> <p>The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a cheap PC or smartphone. Whether a supercomputer will be used well depends on the problem you're trying to solve and on your choice of software.  It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. This requires an understanding of the supercomputer that you are using, the needs of the software you're using and the problem you're trying to solve.  In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, or if your problem is too small, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation.</p> <p>Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC.</p> <p>Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastructure use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries  to explain in short many of the terms (and even some more) that we discuss in these notes.</p> <p></p> <p>The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the parameters of the GROMACS <code>mdrun</code> command for good performance. Most of these will be covered in these lecture nodes.</p> <p>Next, let's have a look at the manual page of the SAMtools <code>sort</code> command.</p> <p></p> <p>Note that the SAMtools <code>sort</code> command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools <code>sort</code> command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2023 (or not even a PC in 2023).</p> <p>Finally, the next picture shows a number of terms copied from a VASP manual</p> <p></p> <p>The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a VASP run may even fail if the parameters used in the simulations do not correspond  to the properties of the hardware used for the run. </p> <p>In general, running software on a supercomputer is not at all as transparent as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence  needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is  done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot  afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that  would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is  \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.</p>"},{"location":"C01_Introduction/C01_S02_Why/","title":"Why supercomputing?","text":"<p>If supercomputing is that complex, why would one want to use it?</p>"},{"location":"C01_Introduction/C01_S02_Why/#when-a-pc-or-server-fails","title":"When a PC or server fails...","text":"<p>Processing of large datasets may require more storage than a workstation or  a departmental server can deliver, or it may require more bandwidth to  memory or disk than a workstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software.</p> <p>Large simulations, e.g., partial differential equations in various fields of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software.</p> <p>But there is also an easier case: applications such as parameter analysis or  Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the  rescue.</p>"},{"location":"C01_Introduction/C01_S02_Why/#supercomputing-jobs","title":"Supercomputing jobs","text":"<p>From the above discussion we already get the feeling that there are two big reasons to use a supercomputer.</p> <ol> <li> <p>We may want to use a supercomputer simply because we want to improve the     turnaround time for a large computation, or because the computation is not     even feasible on a smaller computer because of the required compute time      and memory capacity. This is also called capability computing. In     capability computing one typically thinks in terms of hours per job.</p> </li> <li> <p>We may also want to use a supercomputer to improve throughput if we have to     run a lot of smaller jobs. This is called capacity computing. In     capacity computing one typically thinks in terms of jobs per hour.</p> </li> </ol> <p>Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data  processing jobs we will discuss.</p> <p>Supercomputers are really build for capability computing jobs. They can also accommodate many (but often not all types of) capacity computing jobs,  and often capacity computing jobs could be run equally well or better on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost  (should the user be charged a realistic amount for the compute resources consumed).</p> <p>There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flows requires a huge amount of compute capacity. The demand becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the design of cars.  That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. </p> <p>But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug design one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system.</p> <p>Data processing can also lead to capability computing. One example is the visualisation of very large data sets or simulation results, were a visualisation workstation may not be enough. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a  code is valid but also if it has not been used yet. And a clever abuser may share codes with someone  at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer.</p> <p>Examples of capacity computing for data processing are many of the data mining applications that often  consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing (and one that doesn't always work well on large supercomputers).  Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be grammatically analysed. After the preprocessing, a regular server was enough for the research.</p>"},{"location":"C01_Introduction/C01_S03_What_it_is_not/","title":"What supercomputing is not","text":"<p>Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous century. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use.</p> <p>Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes.</p> <p>The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with  slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '70s, with the advent of so-called vector supercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer.</p> <p>Yet there is no need to be too pessimistic either.  In some cases, in particular capacity computing, the efforts to get your job running efficiently can  be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. Developing code for capability computing is often much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has already done the work for you and good software is already available.</p>"},{"location":"C01_Introduction/C01_S04_Compartmentalised/","title":"A compartmentalised supercomputer","text":"<p>Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments:</p> <ul> <li> <p>When users log on to a supercomputer, they land on the so-called login nodes     (called user access nodes on some HPE computers). These are     one or more servers that each look like a regular Linux machine but should not be used for     big computations. They are used to prepare jobs for the supercomputer: small programs that     tell the supercomputer what to do and how to do it.</p> </li> <li> <p>Each supercomputer also has a management section. This consists of a number of servers that     are not accessible to regular users. The management section is responsible for controlling and     managing the operation of the supercomputer, including deciding when a job may start and properly     starting that job.</p> </li> <li> <p>Each supercomputer also has a shared storage section, a part of the hardware that provides permanent storage     for data or a scratch space that can be used on the complete machine.</p> </li> <li> <p>But the most important part of each supercomputer is of course the compute section, or compute sections     in many cases as most supercomputers provide different types of compute resources to cover different needs     of applications.</p> </li> </ul> <p>In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.</p>"},{"location":"C01_Introduction/C01_S05_Overview/","title":"Overview of the notes","text":""},{"location":"C01_Introduction/C01_S05_Overview/#a-supercomputer-is-a-parallel-computer","title":"A supercomputer is a parallel computer","text":"<p>A supercomputer is not a superscaled PC that runs regular PC software much faster, but it is a parallel computer:</p> <ul> <li> <p>In a supercomputer, many processors work together to create a fast system, and this is even multi-level     parallelism. We will discuss this in the section on Processors.</p> </li> <li> <p>The memory of a supercomputer is also organised in a hierarchy: from fast buffer memory close     to the processor to slower memory and then all the way to slow disks. Most of this     will be explained in the section on the memory hierarchy.</p> </li> <li> <p>The permanent storage system of a supercomputer consists of many hard disks and solid state     drives that are combined to a powerful storage system with the help of software. This is      explained in the section on storage for supercomputers.</p> </li> </ul> <p>In the current state of technology this is far from transparent. It may be mostly transparent for correctness of the program but it is not al all for performance, which is why one needs  properly written software adapted to the specific properties of supercomputers. After all, supercomputing is not about trying to upscale a PC equally in all parameters as that is uneconomical and mostly physically impossible, but it is about building hardware as economical as possible (as the market is too small to develop many non-standard components) and using software to bind all hardware together to a very powerful machine.</p> <p>Yet much of what you learn about supercomputers is also useful for programming PC's. Modern PC's also increasingly rely on parallelism, and this has become very visible with the AMD Ryzen-based PC's and the 12th Gen and later Intel processors. In fact, smartphones arrived there even earlier as 8-core Android smartphones have been around since 2013. PC's also have most of the memory hierarchy that you'll find in supercomputers, and taking that hierarchy into account when writing software is as important for PC's as it is for supercomputers. Even the inner workings of a solid state drive has elements in common with how supercomputer storage work, and understanding the behaviour of supercomputer storage  helps you also understand why an SSD will not always show the expected peak performance claimed in the specifications.</p>"},{"location":"C01_Introduction/C01_S05_Overview/#a-supercomputer-has-a-layered-architecture","title":"A supercomputer has a layered architecture","text":"<p>A supercomputer is much more than some hardware on which you run your application. As already suggested above, between the hardware and your applications sits a lot of other software that binds the hardware into a usable supercomputer. At the lowest level you have the operating system which may be just Linux but sometimes is a modified version of Linux with some features disabled that may harm the performance of a supercomputer and are not needed by real supercomputer applications. But on a supercomputer there is a lot of other software that sits between the hardware and basic OS on one hand and your application on the other hand. That software is often called middleware.</p> <p>Most readers of these notes may be only interested in the applications they want to run. However, some understanding of the hardware is needed even for simple things such as starting a job, as a supercomputer is a multi-user machine and you need to ask exactly what you need to be able to run your application efficiently.  And not all software can run efficiently on all hardware, or even run at all.  But it also requires some understanding of the middleware. Your application, if it is really  developed to use supercomputers properly, will also use some of that middleware, and not all  middleware can be supported on all supercomputers.</p> <p>For developers, even better hardware knowledge is required to understand how to write programs efficiently, and you also need a good understanding of the middleware that will be used to develop your program.</p> <p>The sections on processors, memory hierarchy and storage discuss the hardware of a supercomputer. The section on middleware then gives an overview of the most popular middleware used to develop software that exploits parallelism.  We cannot discuss much about the application layer in these notes, as that quickly becomes too domain-specific, but we will discuss what you can expect from a supercomputer.</p> <p>In recent years, accelerators have also become a hot topic. As the cost, both the investment cost and energy cost, of supercomputers that are fully based on general-purpose hardware has become too high to sustain the growth of compute capacity, scientists have turned into other technologies, collectively known as accelerators, to further increase the performance at a lower investment and energy cost, but losing some of the ease-of-use and versatility of general purpose hardware. The most important types of compute accelerators and their programming models (and corresponding middleware) will be discussed in the section on accelerators.</p>"},{"location":"C02_Processors/","title":"Processors for supercomputers","text":"<ul> <li>Introduction</li> <li>A very basic CPU</li> <li>Instruction level parallelism 1: Pipelining</li> <li>Instruction level parallelism 2: Superscalar processing</li> <li>Data parallelism through vector computing</li> <li>Shared memory multiprocessing</li> <li>Distributed memory computing</li> <li>Parallelism in a modern (super)computer</li> <li>Node architecture</li> <li>Fast (past) evolution</li> <li>An important difference  between a PC and a supercomputer...</li> <li>Lessons learnt</li> <li>Further reading</li> </ul>"},{"location":"C02_Processors/C02_S01_Intro/","title":"Introduction","text":"<p>Long ago supercomputers were build from specialised processors designed specifically for supercomputers. This was in the days that chip technology used large geometries and the speed of even multichip processors was limited by the switching speed of transistors and not by physical distance. Cheaper minicomputers and the early personal computers used single chip processors while mainframes and supercomputers used processors that were build from multiple chips, sometimes several thousands of them, that often also used a faster but more expensive technology (e.g., ECL instead of CMOS).</p> <p>Halfway the '80s things started to change slowly. The processors for PC's that were build from 1 or 2 chip packages (in the case of 2, one for the floating point instructions and one for everything else) became more competitive, not yet in speed, but in price/performance, leading to new supercomputer architectures based on those much cheaper processors. In the course of the '90s processors consisting of multiple chip packages gradually disappeared as the communication between packages became the speed limiting factor. </p> <p>Gradually all supercomputers were no longer being built from very specialised processors, but from processors derived from those used in PCs, and nowadays even from processors derived from those in smartphones, but with enhancements for better reliability.</p> <p>These CPUs have gone through a long evolution.  Both advances in semiconductor technology and advances in architecture made possible by the semiconductor technology advances have made it possible to build ever faster CPUs. The workings of all modern processors is governed by a clock, yet the clockspeed  (which is measured in GHz nowadays rather than in MHz in the '80s) is not a good basis to compare the speed of CPUs. This is because there have been so many architectural advances that have increased the amount of work a CPU can do during each clock cycle. These improvements fall into two categories:</p> <ul> <li>Improvements that enable to execute more instructions per clock cycle: this is called     instruction level parallelism, and</li> <li>improvements that enable the CPU to do more work per instruction, and the main strategy here is     ** vectorization and nowadays even matrix computing**.</li> </ul> <p>Yet those improvements were not enough to satisfy the ever growing need for compute speed of researchers, so two more strategies were used:</p> <ul> <li>Using more CPUs (now called cores) that share memory which is called     shared memory parallel computing, and</li> <li>Using multiple nodes that collaborate not by sharing memory but by sending messages over a     network, which is called distributed memory parallel computing.</li> </ul> <p>All 4 of these strategies - instruction level parallelism, vector and matrix computing, shared memory parallel computing and distributed memory parallel computing - rely on finding and exploiting parallelism in the solution of computational problems, and the further down you go in the list, the more difficult this becomes, but also the more performance you can get.</p> <p>As we shall see, nowadays even your PC and smartphone rely on the first three types of parallelism for performance, so learning to deal properly with parallelism is no longer an option but a necessity to remain competitive.</p>"},{"location":"C02_Processors/C02_S02_Basic_CPU/","title":"A very basic CPU","text":"<p>Let us start with a look at a very basic computer. It has two important components. The processor executes simple instructions, e.g., adding two numbers. The memory  stores the data in a linear structure. A clock governs the pace at which instructions are processed.</p> <p> A basic computer</p> <p>Currently a processor is just as single chip, or in fact, just a small part of a chip, but this was very different in the early days of computers based on chips. Fast processors often consisted of multiple chips, and sometimes even thousands of them.</p> <p>Note</p> <p>The Cray 1, a supercomputer from 1976, had a processor built out of 20,000 chips. The memory was built out of 73,278 chips. The mean time between failure was 50 hours, so whenever you bought such a machine you got a lot of spare parts with it and on-site technicians to replace the parts.</p> <p>Let us now open up our simple processor and have a look inside.</p> <p> A look inside the processor</p> <p>It has several building blocks.  The ALU or Arithmetic and Logical Unit is the unit that does the  actual computations. The registers are a very small block of local memory. On modern  computers, the ALU can only use data that is already in one of the registers and will also write results to the registers. 30 years ago or more this was not always the case and some processors could operate directly on data from memory. Another important block is the Address Generation Unit with load/store unit and and the memory controller. This is the connection between the main memory and the registers. All data passes through the load/store unit with the AGU generating the actual address. The control unit is the part that coordinates all the work.</p> <p>In our very simple processor, instructions are executed one after another. But executing each instruction itself consists of multiple steps. Consider, e.g., this oversimplified execution pattern distinguishing a few important steps for executing an instruction in the ALU:</p> <p></p> <p>Instructions are fetched from memory and decoded in the first step.  In the second step, the operands of the instruction (the numbers on which it works) are transported from the registers to the ALU. In the next step the actual instruction is executed, e.g., two numbers are added. And in the last step the result is stored back in a register. Instructions involving storage would also consist out of multiple steps. The steps are synchronised with a clock. So in this simple example executing an instruction would take 4 clock ticks, so we could also say that we do 0.25 instructions per clock.</p> <p>Note however that different steps use different parts of the logic in the processor, and this is the key to improving performance.</p>"},{"location":"C02_Processors/C02_S03_ILP_I_Pipelining/","title":"Instruction level parallelism 1: Pipelining","text":"<p>Since different steps in the instruction execution process use different parts  of the logic on the chip, one could try to overlap the execution of different steps of different instructions. You can compare this with the way a car assembly line works: cars being assembled move from one station to the next one while the next car to be produced already follows as soon as a station is free.</p> <p></p> <p>In this simple model we can now start or complete (called retire) one  instruction ever clock cycle, so we can process 4 times as many instructions as in the previous case. This however requires that the next instruction  is completely independent from the previous one. If one instruction uses the result of a previous instruction, it has to wait until that instruction as completed. This is called a bubble in the pipeline. So in practice we  will never get one instruction per clock cycle with this design as it  is usually impossible to have enough independent instructions in between two dependent instructions. </p> <p>This is also the first example of so-called instruction-level parallelism: The processor is working on multiple instructions simultaneously. Pipelining has been used in processors for PC's since the mid '80s, with the Intel i386 and i486 the first Intel CPUs to implement some  form of pipelining. The i386 (also known as 80386), launched in 1985,  could not even execute 1 instruction per clock in theory, but the i486  (also known as the 80486) which launched in 1989 implemented pipelining more or less as described here.</p> <p>In supercomputers and other very large computers, pipelining appeared already much earlier. The IBM System/360 Model 1 from 1964 and the CDC 7600 (Control Data Corporation) from 1967 were some of the first examples. (One of the co-founders of CDC was Seymour Cray who later started another  company, Cray Research, the manufacturer of the Cray 1 mentioned in the previous section.)</p>","tags":["pipelined execution","ILP (Instruction Level Parallelism)"]},{"location":"C02_Processors/C02_S04_ILP_II_Superscalar/","title":"Instruction level parallelism 2: Superscalar processing","text":"<p>A second way to improve the instruction throughput in a processor is to simply increase the number of ALU and load-store/address generation units. Of course this only makes sense if the instruction decoder in the control unit is also enhanced so that it can also decode multiple instructions simultaneously to keep those instruction units fed. This is called superscalar execution  and is also a form of instruction level parallelism.</p> <p>Combined with pipelining we would get the following scheme for execution instruction (for the case where we could decode and execute two instructions simultaneously):</p> <p> Pipelined superscalar execution</p> <p>Of course this only works if there are enough independent instructions to execute.</p> <p>With superscalar execution, and certainly when combined with pipelining, we have the potential to execute more than one instruction per clock cycle. </p> <p>It also creates another opportunity: Specialised ALUs. Modern processors will have specialised ALUs for integer instructions and for floating point instructions, and in fact, some of those units may only be capable of executing some of those integer or floating point instructions.</p> <p>In personal computers this technology appeared in the '90s.  The Intel Pentium processor launched in 1993 implements a pipelined superscalar architecture pretty much as on this slide, but both pipelines are not created equal and the compiler had a lot of work to do to keep both pipelines used well. The Pentium Pro, launched in 1995, introduced out-of-order execution: The processor could re-order instructions for execution as it sees fit to get higher instruction throughput, but does so in a way that at the end of the execution the result is the same as if the instructions were executed in-order (also taking into account error conditions etc.) This strategy is currently used by all modern high performance processors, and even by a lot of low power processors. E.g., both the performance and efficiency cores in the  12<sup>th</sup> and 13<sup>th</sup> gen processors code named Alder Lake and Raptor Lake are out-of-order pipelined superscalar processor cores. The ARM Cortex-A53 and Cortex-A55 processors that are often used as the efficiency cores in Android mobile phones are also pipelined superscalar processors, but with in-order execution only (the performance cores from the A-7x series are out-of-order execution cores though).</p> <p>Probably the oldest example of a superscalar design is the CDC 6600 introduced in  1964 (See, e.g., the Wikipedia article on the CDC 6600). This processor had no pipelining and instructions required 10 or more cycles, but it had multiple specialised functional units, some in a single copy and others in two copies. And the IBM/360 Model 91, also from 1964 and already discussed in the section on pipelining, also offered superscalar execution.</p> <p>IPC increase through more and more sophisticated superscalar execution is one of the motors behind speed increases of processors.  E.g., between the Intel Nehalem architecture from 2008 and the Skylake architecture from 2015, IPC for integer workloads improved by 41%. Note that in this period the clock frequency of especially the server variants of processors didn't increase much anymore (rather the contrary).</p> <p>Exploitation of instruction level parallelism is largely done by modern compilers and by the CPU hardware itself. However, whether a compiler and CPU can exploit instruction level parallelism also depends on the program itself. E.g., frequent tests tend to kill ILP as each test leads to two different code paths that the processor would have to follow and then discard the irrelevant one once the result of the test is known, or just take a gamble which way the test will go.</p>","tags":["superscalar execution","ILP (Instruction Level Parallelism)"]},{"location":"C02_Processors/C02_S05_Vectorisation/","title":"Data parallelism through vector computing","text":"<p>Modern superscalar execution, with instruction reordering and other  mechanisms to increase IPC that we have not discussed, requires complicated logic and hence a lot of power. A common case that is well suited for superscalar execution as there can be lots of independent instructions, is working with vectors and matrices.  This led to the design of CPUs with instructions to work on vectors, i.e., with wider ALUs and wider registers that do not contain just a single number but a row of numbers. This has the potential to boost the speed without the full power requirements of a superscalar processor.  This is one example of a single instruction stream, multiple data stream (abbreviated as SIMD) architecture.</p> <p>This was a very popular design in supercomputers in the 70s and 80s, but  then almost disappeared with only two or three vendors still making them (and only one still making them today). Examples are the CDC STAR-100 from 1974 and the Cray-1 from 1976.  The latter was the first vector computer to use a design with registers. The STAR-100 streamed the data directly from and to memory. Currently NEC still makes vector computers though with a slightly different design as before for even faster speeds. Vector registers in the current NEX SX Aurora TSUBASA architecture are 256 double precision numbers wide, and the execution unit itself is 32 double precision numbers wide, executing a 256-number vector instruction in 8 steps.</p> <p>Vector instructions are also returning in general purpose computers, but then with much shorter vector lengths.</p> <p>In the Intel architecture, MMX integer vector instructions with 64-bit wide registers were introduced in 1996 and available in 1997 on an evolution of the Pentium processor. They basically re-used the floating point registers in those processors for storing integer vectors so codes could not combine floating point instructions and integer vector instructions. Just a few years later, in 1999, Intel launched the SSE instruction set which used separate 128-bit registers. Originally they were only meant for single precision floating point numbers (so 4 elements in a vector), and they became available in  the Pentium III processor which was actually an evolution of the Pentium Pro and not of  the original Pentium. The instruction set was extended several times with SSE adding double precision floating point numbers and some integer instructions in the Pentium 4,  and with further extensions in the SSE3 and SSE4.x instruction sets. MMX instructions are not present anymore in modern Intel processors in 64-bit mode, but the SSE instructions are still present.  A further extension came with the AVX instruction set in the Sandy Bridge generation in 2011. The vector registers were extended to 256 bits and could now handle 4 double precision or 8 single precision numbers simultaneously. These processors were also used in the first Tier-1 cluster of the Vlaams Supercomputer Centrum, installed in 2012, and in the hopper cluster of the University of Antwerp. AVX2 extended the AVX instruction set  in the Haswell generation and came at the same time as the FMA3 instructions, the so-called fused multiply-add instructions that combine a multiplication and an addition in the form a*b+c, effectively doubling the number of floating point operations per clock cycle.  A further evolution came in the AVX-512 instruction set using 512-bit wide vectors but also adding other features that make the vector instruction set a lot more useable (and a lot more like those on the traditional vector computers). They were first introduced in the now defunct Xeon Phi processor, a processor specifically designed for supercomputers, but got a new life in the server variant of the Skylake architecture (Skylake X) and the more recent Ice Lake and Sapphire Rapids processors. In fact, Intel keeps adding new instructions for new data formats, especially for AI where some deep learning applications can benefit from lower precision data formats.</p> <p>The ARM instruction set has known the NEON vector instructions for quite a while already. These instructions use 128 bit registers, so can work on 2 double precision or 4 single precision floating point numbers. Even the processors in most mobile phones support these instructions. But the ARM architecture is also evolving. ARM designed the SVE instruction set (Scalable Vector Extension) together with Fujitsu for use in supercomputers. SVE instructions allow for different implementations with vector registers up to 2048 bit wide, which can store 32 single precision or 64 double precision numbers, but also supports implementations with  registers as small as 128 bits. The implementation of Fujitsu which is used in the A64fx processor  of the Fugaku supercomputer (which was the fastest machine in the Top500 lists from June 2020 till November 2021) uses 512-bit registers. These instructions have been extended with SVE2 instructions that are more oriented towards integer computations and making the instruction set more useful for non-scientific applications. SVE2 is now part of the new ARMv9 architecture and used in, e.g., the processors of the Samsung S22 and S23 mobile phones, but also in the brand new NVIDIA Grace CPU that will be used in the GPU-accelerated nodes of the first European exascale supercomputer.</p> <p>AMD also uses vector units int its GCN and CDNA GPUS, with 16-wide single precision  hardware and 64-wide single precision instructions (well, not completely true for CDNA2),  and in its RDNA GPUs, with  32-wide single precision hardware and instructions. The Intel GPUs are also based on vector computing units with 8-wide units for most basic operations.</p>","tags":["vector instructions"]},{"location":"C02_Processors/C02_S05_Vectorisation/#another-type-of-simd-computing","title":"Another type of SIMD computing","text":"<p>There is however another way to reduce the overhead of superscalar execution that works well for vector operations. </p> <p>Rather than widening the ALUs and registers, we can also just use multiple sets of ALU, register and AGU/load-store units, but use only a single control unit so that all those ALU-register-AGU combinations have to execute the same instruction at the same time, or do nothing, but of course can operate on different data.</p> <p>This design is shown in the following figure:</p> <p></p> <p>The historical example of this design is the Thinking Machines connection machine  with the first generation CM-1 machine launched in 1983. However, NVIDIA GPUs are a  modern example of this design combining 10s or even 100 of those SIMD processor (which they call SIMT processor) on a chip.</p> <p>Such processors may seen easy to program as it looks as if you can think in terms of scalar operations rather than vector operations, but in reality it is not easier to program them efficiently that it is to program vector machines. And NVIDIA may claim otherwise, but there is little difference in what the processors can do between this form of SIMD computing and a vector computer with an instruction set that also  supports so-called masking or prediction and scatter-gather operations. Which is in fact also the difference between the AVX-512 and AVX2 instruction  sets.</p>","tags":["vector instructions"]},{"location":"C02_Processors/C02_S05_Vectorisation/#conclusion-on-intra-processor-parallelism","title":"Conclusion on intra-processor parallelism","text":"<p>There are two levels of parallelism in modern processors:</p> <ol> <li> <p>Trying to execute more instructions per clock cycle: instruction-level parallelism, abbreviated ILP.</p> <p>This is mostly hard work for the CPU control logic and for the compiler, and only a limited amount of work for the application developer. Moreover, you can expect a gain from this technology even without recompiling your application for a newer CPU if that CPU supports the same instruction set.</p> </li> <li> <p>Doing more work per instruction: data-level parallelism through vectorisation or other SIMD technologies</p> <p>This is a much harder job for the compiler as most programming languages do not  offer enough information to the compiler. The application developer often needs to help the compiler.</p> <p>Moreover, you will not gain from the addition of vector instructions to a processor design without recompiling for those vector instructions.</p> <p>In some applications a lot of the gain over the various generations of CPUs since the late '90s has come from better and wider vector instruction sets. But these improvements can only be delivered if an application gets recompiled for those new instructions (and then it won't run anymore on the older processors). This is one of the reasons reason why on supercomputers applications are often installed from source code rather than from binaries.</p> </li> </ol>","tags":["vector instructions"]},{"location":"C02_Processors/C02_S06_Shared_memory/","title":"Shared-memory multiprocessing","text":"<p>When making faster processors doesn't deliver enough gains to satisfy the always growing need for more speed, the next step one can look at is to  simply use multiple independent processors. This is called Multiple Instruction, Multiple Data parallelism or MIMD. As every processor is completely independent of every other one at the execution level, they can all follow their own code path and work with their own data.</p> <p>In the first designs, all processors were completely equal which is called symmetric multiprocessing or SMP. This implies that every process or every thread can run equally well on all processors. Also, in the more simple designs all processors also have equal access to all memory. This is called shared memory with uniform memory access. </p> <p> Symmetric multiprocessing and uniform memory access</p> <p>In the most simple designs, all processors are simply connected to a single bus that makes the connection to memory. This was, e.g., the case in the first Intel chips that used this design, e.g., the Core2 Duo which had two processors on a single die (a single piece of silicon) and the Core 2 Quad that had two such dies in a single package. In more sophisticated designs a switch is used, or as switches also have size limits, a mesh or ring network to connect all parts together. E.g., the early generations of multiprocessor Opteron chips used a switch architecture, and current generation Intel CPUs use ring or in some cases (e.g., Sapphire Rapids server chips) a mesh network.</p> <p>Early examples of shared memory supercomputers go back to the early '80s.  E.g., machines such as the CrayX-MP from 1982 where shared memory computers with uniform memory access.</p> <p>Nowadays multiple processors are often integrated on a single piece of silicon or die, together with the network or switch that links them together. This has led to a change of terminology as it became unclear what the processor was. Especially in the PC world, people were used to call the chip, or better, the package containing the chip, the processor and kept using that term once each package effectively contained multiple processors. So nowadays the term core is used for what used to be the processor, the smallest unit on the chip that by itself can execute a program. The name package is often used for the actual part that you buy, which contains one or more dies with one or more cores each, and is soldered to the motherboard or inserted  in what is called the socket. The term processor nowadays typically refers to the package.</p> <p> Uniting multiple cores in a package</p> <p>Remark</p> <p>In the world of NVIDIA GPUs, the word \"core\" has yet another meaning. What NVIDIA calls a CUDA core is actually not a core but an ALU (and one could even argue, just a small part of a wider ALU).</p> <p>This uniform memory access design does have a flaw though:  The more processors you try to link this way, the harder and less economical it becomes to have uniform memory access. The limit seems to be at around 8 to 16 cores, though some CPU manufacturers have succeeded in going further while still giving a near uniform access  of all CPUs to all memory. The connection to the memory does tend to become a bottleneck in such designs (and there are other bottlenecks also that are far too complicated to discuss in this short course).</p> <p>This has led to another design, which has in the Intel world always been used when multiple packages were used in a single shared memory machines since the days of the Nehalem processor which launched in late 2008:</p> <p> Non-Uniform Memory Access</p> <p>Each package, which could itself be a multi-core processor, has part of the system memory attached to it. The processor packages are connected through a special-purpose network that connects the packages in such a way that each core has access to all memory in the same way. These networks go under the name UltraPath or UPI in recent Intel  designs, or Infinity Fabric or xGMI in the case of AMD (AMD uses more and more xGMI as the name for external connections while Infinity Fabric is used for the technology when used inside a package to connect the dies, but they are basically two variants of the same  transport). A new connection type coming up is CXL (which stands for Compute Express Link<sup>TM</sup>) which will also have its variant to connect dies in a package called Universal Chiplet Interconnect Express<sup>TM</sup>, abbreviated UCIe. CXL is itself an extension of the PCIe interface which is already used for expansion boards but adds more features to create a global memory pool with (and this is something that will only be explained in a later chapter) so-called cache coherency (which is still lacking in the first versions of the standard).</p> <p>In this design each core can still directly reach all memory in the computer, but the speed of access can be very different for different parts of the memory. In the case of the figure above, each core in a given package has fast access to memory directly attached to the package  and slower access to the memory attached to the other package. So even though from a logical point of view there is just a single large memory space with a single (physical) address space, but from a performance part of view not all memory is equal. This is called a  Non-Uniform Memory Access or NUMA design. NUMA is transparent with respect to correctness of programs but it is not fully transparent (or sometimes not transparent at all) when it comes to performance, and it is something that has to be taken into account when running software on such  a design.</p> <p>All current multiple-socket server CPUs use a NUMA architecture, e.g., the Intel Xeon, AMD EPYC and IBM POWER processors.  Probably the largest NUMA design in terms of the number of sockets was the SGI Altix UV system which could scale to 64 sockets. After the merger with HPE it was renamed to HPE Superdome Flex. (The current version as of 2023 scales to only 32 sockets.) Currently one may even have NUMA within a chip. This was very pronounced with the  first generation AMD EPYC server processor code-named Naples, which had 4 chiplets each with up to 8 cores and their own memory, and those chiplets were linked together via an internal Infinity Fabric network (and the same network was also used to link two sockets). It is still somewhat present in newer AMD CPUs also, though mostly in a different way that we will return to later in this course. It is also expected to show up strongly in the higher core count Intel Sapphire Rapids CPUs launched in early '2023, but at the time of writing we have no good performance data available yet.</p>","tags":["core","shared memory","SMP","NUMA"]},{"location":"C02_Processors/C02_S06_Shared_memory/#a-look-at-the-software-side","title":"A look at the software side","text":"<p>On the software side, shared memory processors are exploited through the mechanism of processes with threads.</p> <p>A process is an operating system concept. It is basically a program being executed.  It is created when you start an app on your phone, click on a <code>.exe</code> file in Windows or most of the times when you type a command in Linux.  In modern operating systems, each process has an amount of (virtual) memory that processes  cannot access and can have exclusive access to files etc. So it is the entity that owns a part of the shared resources.</p> <p>In the old days, like in the days of MS-DOS, the operating system of most PCs in the '80s,  there was only a single stream of instructions executed in the context of a process (and MS-DOS didn't even have a true notion of multiple processes). The early versions of UNIX had decent support for multiple processes but could support only one instruction stream per process.</p> <p>The problem is that in such designs a single application cannot easily exploit multiple  processors. This was solved by threads. A thread is an instruction stream that is being executed in a process with an execution path that is independent on that of other threads.  Threads of the same process share all resources. Every thread can see all memory of the process. Even though there is some thread-private memory this is not protected from access by other threads. Things like file descriptors (the structure  that is created when you open a file) are also shared in a process.</p> <p>Threads can run on different cores, though one core can also execute multiple threads (just as one core in older operating systems could execute multiple processes) by the mechanism of time sharing: The processor continuously switches between the execution of different threads. In modern operating systems users may have thousands of threads running simultaneously, though typically most of these threads are not very active.</p> <p>Consider, e.g, this thread and process view from the Mac laptop the author of these notes used to use:</p> <p></p> <p>The laptop on which this screenshot was made had only two physical cores, yet was running 315 processes with a total of 1436 threads. Some processes use only a single thread, like the <code>clang</code> process at the top, which is a C compiler. But, e.g., Google Chrome was playing a video at that time and had 50 threads, not counting  the 16 of a helper process. Most threads are really background threads that are waiting for input, e.g., keyboard input,  data arriving from the network, etc.</p> <p>When running scientific software one often needs to take control of the computational threads. These threads can use almost the full capacity of a core and for reasons that will become  clearer later in this course it is important to keep them on separate cores and not use more of those threads than there are cores. </p> <p>Remark</p> <p>NVIDIA also abuses the word \"thread\" as the so-called threads on an NVIDIA GPU are not independent of each other but need to execute in synchrony in groups of 32 called a warp. The warp is a much closer equivalent to a thread on the CPU.</p>","tags":["core","shared memory","SMP","NUMA"]},{"location":"C02_Processors/C02_S06_Shared_memory/#hardware-threads","title":"Hardware threads","text":"<p>Threads in the context of operating systems should not be confused with so-called hardware threads.</p> <p>A problem with large superscalar processing cores is that often a single instruction stream does not contain enough parallelism to exploit all compute resources in such a core. This is certainly the case for code with lots of tests with unpredictable results.  For those applications it may be beneficial to try to run multiple instruction streams simultaneously on a single physical core. So some CPUs can split up a physical core in multiple virtual cores. Each virtual core has its own architectural register set, but the threads running on those virtual cores effectively compete for all resources of the core. To the operating system, the virtual cores are seen as the actual cores on which threads are scheduled. So if, e.g., you have a computer with 1 socket and 8 cores that can each support 2 virtual cores, the operating system will report the computer as having 16 cores (though it may understand that these are actually 8 pairs of two).</p> <p>This technology is known under different names. The generic name is Simultaneous Multi-Threading  or SMT which is also the term used by IBM for its POWER CPUs and by AMD for its Zen architecture CPUs  (Ryzen and EPYC). Intel uses the term hyper-threading instead and Oracle uses the term hardware threading.</p> <p>The gain one can get from enabling SMT depends a lot on the architecture of the CPU and on the application. E.g., on the regular Intel Xeon or Core i7/i9 CPUs the gain is often rather limited. However, on the Xeon Phi (a chip that Intel made for HPC and is a descendant of the original Pentium architecture) in the KNC (Knights Corner) generation you needed to use at least two hardware threads on each core to extract the full performance of the core at it was not capable to fetch and decode instructions from one hardware thread in subsequent  clock cycles. Intel architecture cores in modern PCs that support SMT all support only two virtual cores per physical core. The Xeon Phi however supported four, and some recent IBM POWER processors support 8-way SMT.</p> <p>SMT is fully transparent for software when it comes to correctness of programs, but it is not transparent when it comes to performance. As the benefit for scientific applications is not always that large,  it is not always enabled on supercomputers. But even when it is enabled, with a careful selection of options when you start a process in a job you may still limit your use to a single virtual core per physical core, even though that would still have a (very) small performance penalty compared to a system where SMT is explicitly  disabled.</p>","tags":["core","shared memory","SMP","NUMA"]},{"location":"C02_Processors/C02_S06_Shared_memory/#programming-shared-memory","title":"Programming shared memory","text":"<p>Automatic parallelisation is less successful than automatic vectorisation. So in most cases you'll have to do the work as a programmer. There are several approaches that we will discuss later  in the chapter on middleware.</p> <p>Of course one can exploit shared memory parallelism easily in cases where you have to do a large number of independent runs that don't need that much memory so that you can simply start a run on each core. But in other cases it will require a bit more work to exploit the shared memory level of parallelism, and even a redesign of algorithms as the best sequential one may not be the best in a parallel implementation.</p> <p>And even as a user you need to be aware of shared memory parallelism as you will often need to tell the  program how many threads it should use, or may need to help it a bit to link threads to specific cores in the proper way.</p>","tags":["core","shared memory","SMP","NUMA"]},{"location":"C02_Processors/C02_S07_Distributed_memory/","title":"Distributed memory computing","text":"<p>NUMA shared memory machines also have their scaling limits . From a hardware point of view is is difficult to impossible to build a network that is  fast enough to link together a large number of sockets in a way that memory access is fast enough, and there is another issue that we have not yet discussed that poses an even bigger limit (for those who know something about processor technology: ensuring that the caches of all sockets are coherent, so that every core reads the same value at the same memory address at any given time). There is also a scaling limit in software. Existing operating systems also do not scale well enough to large shared memory systems. It may need more than just changes to  the OS scheduler (the part of the OS that decides at every moment which thread will run on which core) to cure the scalability issues. And lastly there is also an economical reason. Building large networks with low latency to offer transparent memory access is also very expensive.</p> <p>The solution scientists and vendors came up with was to take a number what may look like a pretty standard computer (called the nodes of the supercomputer) and link them together with a fast network (called the interconnect).  The concept is shown in the following figure:</p> <p></p> <p>The picture looks almost exactly as the one for NUMA shared memory, but there is one big difference: the nature of the network. The network cannot support direct memory access (and coherency of caches, something that we have not seen yet) as in the NUMA shared memory case, has a higher latency and lower bandwidth, but can economically cover longer distances. The network is used for communication via sending and receiving software-initiated messages. There is no joint global address space (at least not in hardware, some companies have tried to emulate that in software). There is an evolution towards interconnects with some limited memory semantics that make it easier to create some kind of global address space, but not to the extent that it is as transparent to use as in the case of a  NUMA shared memory machine.</p> <p>As there is no direct access to the memory of another node, it is also not possible to run a process across nodes. So an application will consist of multiple processes, at least one per node, that communicate with each other by sending messages over the network. Such supercomputers are even harder to program than shared memory computers, but are also far more scalable. The largest supercomputers in the world that only use regular processors as we discuss in this chapter (and no accelerators, something that we will discuss towards the end of the course) have more than  5 million cores.</p> <p>Distributed memory computers found their origin in the '80s in research aiming at building cheaper supercomputers using more standard components, as it became quickly clear that CMOS semiconductor technology would soon become the technology of choice to  build powerful microprocessors that could evolve much faster than the large purpose-built supercomputer processors (that also needed to shrink in size as the size became the limiting factor for the clock speed). The Cosmic Cube, built at Caltech in 1981 using standard Intel processors similar to the ones used in the first IBM personal computer, was probably the first example. Intel build and commercialised an evolved version of that design with their series of iPSC supercomputers. However, by  the '90s, the more traditional supercomputer builders also adopted this design, with, e.g., the IBM SP/1 and SP/2 that used POWER processors that were also used in IBM workstations and UNIX servers, Convex Exemplar that used HP PA-RISC processors (which they also used in their workstations and  UNIX servers) and the Cray T3D based on DEC Alpha processors (again a processor for workstations and UNIX servers).</p> <p>Early distributed memory supercomputers in Belgium</p> <p>The department of computer science of the KU Leuven has operated two smaller computers from the iPSC series, basically to be able to develop code to then run on bigger supercomputers of that type elsewhere.</p> <p>In 1994, the central compute facilities of the KU Leuven acquired an IBM SP/2 system.</p> <p>Another evolution got triggered in the early '90s. In 1994 Linux 1.0 was launched which worked well on standard PC hardware of those days. Researchers came up with the idea of combining very standard off-the-shelf PC's or small servers often running Linux with very standard off-the-shelf network technology (Ethernet, in 1994 this was still only 10 Mbits per second, with 100 Mbit/s technology only being standardised in 1995). These clusters were called \"Beowulf\" clusters. The name refers to one of the first such clusters build in 1994 at NASA. However, a 100% Beowulf design had many shortcomings. The Ethernet technology of those days was way too slow to build a system that would be able to run a lot of software in a scalable way on a large cluster. Reliability was also a problem. PC's and workstations still crashed rather often. Now one crash every 50 days may not seem a real problem for a workstation or PC,  but if you build a supercomputer with 1000 such computers you can expect 20 node crashes per day making it impossible to run an application at the full scale of the machine. To build a proper supercomputer you need very fast network technology, so supercomputer companies kept designing  specific network architectures for that purpose, and reliable components.</p> <p>This is shown nicely in the following anecdotes, taken from the  IEEE sepctrum article \"How to kill a supercomputer: Dirty power, cosmic rays, and bad solder\" (you may need to be at an institution with  library access to this journal to read the article).</p> <ul> <li> <p>ASCI Q was a supercomputer built in 2002 at Los Alamos National Laboratory.      It consisted of 2048 4-socket nodes with the DEC Alpha EV-68 (21264C) processor,     probably the fastest single chip processor at that time for scientific computing.     However, the computer had a node crash on average every hour due to sensitivity to     cosmic rays. With better shielding they managed to improve the reliability to one crash     per 6 hours on average. This was due to a design flaw in the DEC Alpha CPU, where there     was a data path that was not protected with error correction.</p> </li> <li> <p>Big Mac was a real Beowulf design built in 2003 at Virginia Tech using 1,100 PowerMac G5 servers.     That machine actually started crashing before it was completely booted. It too was too sensitive     to cosmic rays as it did not use memory with error correction (so-called ECC memory which is     nowadays used in almost all servers).</p> </li> <li> <p>Jaguar, a Cray XT5 system installed at Oak Ridge National Laboratory in 2009, was a very nice     real supercomputer design. It was the largest system in the world at that time,     with 360 TB of memory and 18,774 nodes with 2 AMD quad core processors and custom network     technology. Monitoring showed that there were 350 errors per minute in the memory that could     be corrected by the ECC protection. However, on average there was still an uncorrectable     2-bit error every 24 hours...</p> </li> </ul>","tags":["distributed memory","MPI","Beowulf"]},{"location":"C02_Processors/C02_S07_Distributed_memory/#programming-distributed-memory","title":"Programming distributed memory","text":"<p>Automatic strategies through the compiler have never made it past the research phase.</p> <p>A trivial way to exploit distributed memory computers is to run one or more independent programs on separate nodes. But that is of course not what such a machine is built for as usually you wouldn't need such an expensive network for that use.</p> <p>Developing software for distributed memory computers is rather hard work for the programmer. There are some programming languages that help a little though, but as for share memory computing we will discuss that in a little more detain in the chapter on middleware.</p>","tags":["distributed memory","MPI","Beowulf"]},{"location":"C02_Processors/C02_S08_Modern_supercomputer/","title":"Parallelism in a modern (super)computer","text":"<p>A modern supercomputer combines all those levels of parallelism</p> <ul> <li>Almost all modern supercomputers are distributed memory machines, consisting     of a network of compute nodes and some specialised nodes for special purposes.</li> <li>Each node is a shared memory machine. In most cases these are of the NUMA     type with two (or more) CPU packages (or sockets).      Even single socket nodes often have an on-chip NUMA character, e.g., the     Fujitsu A64fx processor used in Fugaku (and soon also in a European     petascale machine)</li> <li>Each socket contains a multi-core CPU very similar to the ones used in     PC's or in some cases smartphones but with a lot more cores,</li> <li>Each of those cores often operates as two (or in rare cases more) virtual     cores through SMT.</li> <li>Each processor core on such a chip nowadays supports vector instructions</li> <li>and has extensive instruction-level parallelism.</li> <li>And it becomes even more complicated when adding an accelerator, e.g., a GPU     to the mix.</li> </ul> <p>However, most of those levels of parallelism are not exclusive to supercomputers. All modern servers have all but the first level of parallelism in this list and are shared memory machines with in some cases more sockets than a typical  supercomputer node.</p> <p>PC's contain only one socket, but that socket also contains a multi-core CPU, often with SMT, with vector instructions and extensive instruction level parallelism. In fact, some PC's have NUMA-like behaviour, not caused by the memory structure but by the way processor cores are linked to each other and share certain buffers in the system (cache memory), and others are not even uniform. E.g., Intel 12<sup>th</sup> and 13<sup>th</sup> gen Core processors code-named Alder Lake and Raptor Lake  (and later generations) combine two different types of cores, one type optimized for fast sequential execution and the other type optimized for power efficiency and better parallel performance per square centimetre of die. (And unfortunately as the efficiency cores don't support the same vector instruction set the more advanced vector instructions are also disabled in the performance cores). PC's often also use their GPU to accelerate some applications.</p> <p>Smartphones are not that different from modern PC's. Many smartphones contain two or three types or cores, or at least a single type in two different versions, one version more optimised for fast performance and the other for better power efficiency. The GPUs can often also be used for accelerating computations in some apps, and most modern smartphones also have an accelerator for some deep learning AI operations.</p> <p>So much of the things you have to learn to efficiently build software for a supercomputer, or even just to run software, may also help you when you program for servers, PC's or even smartphones, or sometimes also to use them properly. So what you learn for your Ph.D. or postdoc about supercomputers may still be useful later on, certainly when developing software  (even non-scientific applications) and in some cases also when using or managing  infrastructures.</p>","tags":["shared memory","distributed memory","ILP (Instruction Level Parallelism)","NUMA"]},{"location":"C02_Processors/C02_S09_Node_architecture/","title":"Some node architectures","text":"<p>We'll come back to this later also in more detail after discussing memory architectures.</p>"},{"location":"C02_Processors/C02_S09_Node_architecture/#amd-rome-node","title":"AMD Rome node","text":"<p>Examples of clusters that use this processor:</p> <ul> <li>Login nodes and large memory nodes on the EuroHPC pre-exascale system LUMI     (using a 64-core version)</li> <li>Phase 1 of the Flemish Tier-1 system hortense     (using a 64-core version in the regular compute nodes)</li> <li>Older nodes of the UAntwerpen cluster Vaughan,     using the 32-core variant shown in the picture in this section.</li> </ul> <p> 2 socket node with 32-core AMD Rome CPUs</p> <p>Each CPU package of the variant of the Rome CPU used in Vaughan contains 5 chiplets (9 in LUMI and hortense).  4 chiplets (8 in LUMI and hortense) are identical and contain 2 groups of four cores each, with each core supporting 2 hardware threads. The group is also called the Compute Core Complex (CCX), the die the Compute Core Die or CCD. Each CCD then connects to the other die, the I/O die, with an Inifinity Fabric link that is very similar to the links that are also used to connect the I/O dies in the two sockets. The I/O die also contains the memory controllers which in case of Vaughan connect to 128 GB of memory per socket. The sockets are connected through 4 Infinity Fabric or xGMI links. </p> <p>In reality the architecture is a little bit more complex as the following figure suggests:</p> <p> 2 socket node with 32-core AMD Rome CPUs, NUMA-domains</p> <p>The I/O chiplet itself is logically subdivided in 4 quarters with each quarter connecting to one compute die in Vaughan or 2 compute dies in those variants that have 8 CCDs per package, and each quarter also has its own memory controller  connecting to 32 GB of memory. Though it is possible to let the four quarters  function as one, in HPC applications it is often beneficial to let them  operate as if they are 4 separate memory domains, giving slightly better memory performance for memory access via the memory controller attached to the quarter containing the core, and maybe slightly slower to memory in the other quarters. Many supercomputers with these processors have their nodes configured to work in that way so that the operating system might report 8 NUMA domains rather than 2.</p> <p>One of the sockets (and in fact the I/O die in that socket) is then connected to the interconnect network card.</p> <p>The European pre-exascale supercomputer LUMI operated by CSC in Finland, and the newer compute nodes of Hortense use 64-core third generation EPYC processors (code-named Milan) that have one core group of 8 cores per chiplet, so the CCXs and CCDs are equal, but otherwise the design is again similar to the design of the Rome nodes.</p>"},{"location":"C02_Processors/C02_S10_Fast_evolution/","title":"Fast (past) evolution","text":"<p>Micro-electronics and supercomputers have evolved a lot over the past half century.</p> <p>Lets compare an iconic supercomputer from 1977, the Cray 1A system of ECMWF, the European Centre for Medium-term Weather Forecasts, with another iconic device  launched 30 years later, the first generation iPhone. The precise specs of the latter are derived from the little information available as Apple is rather secretive about the processors it uses, but as this was not yet a custom design information was rather easy to find.</p> 1977: Cray 1A (ECMWF) 2007: Apple iPhone CPU 80 MHz CPU, 160-250 Mflops 412 MHz, 412 MFlops RAM memory 8 MB 128 MB Permanent storage 2.4 GB Up to 8GB Weight 5,500 kg 0.135 kg Power required 115 kW A few Watts Estimated cost $8.86M $500 MTBF 50 hours Weeks without reboots, years without repairs Extra Needed a CDC as front-end HAs a built-in monitor <p>(MTBF = Mean Time Between Failures)</p> <p>We see that the first iPhone was at least on paper a faster machine than the computer used 30 years earlier for weather forecasts in Europe, and it was certainly more reliable.</p> <p>Also launched in 1977 was the VAX-11/780, a mini-computer with 5 5 MHz 32-bit CPU and  2MB of memory (later models had 8 MB). Another machine from 1977 was the PDP-11/60 (which was an evolved model of the PDP-11 series, itself a predecessor of the VAX-11). One of these was acquired by the Department of  Computer Science of the KU Leuven 2 years later in 1979 and costed back then 3.7 million BEF, which is just over 90,000 EURO at the conversion rate between EURO and belgian francs of 2000. That machine had only 256 kB of memory but given the high cost had to be shared by many researchers.</p> <p>Machines more oriented at home use or other personal use in 1977 were the Tandy TRS-80 model 1 and  the Apple II. They had processors with a clock speed of 1 to 2 MHz and no floating point hardware so floating point operations had to be emulated entirely in software. Because of that speeds were measured in flops per second rather than kiloflops or megaflops per second (and the speed of floating point emulation was probably on the order of 100-200 flops in double precision). The TRS-80 costed $400 (On the order of 2000 EURO in 2023 money) at launch without the monitor  and had 4 kB of ROM memory that stored the permanent software and 4 kB of RAM memory (but extensions up to first 16 kb and then 48 kB appeared quickly)  in its early version. The CPU ran at 1.7 MHz. The only permanent storage consisted of tape which in fact was just a regular cassette that could also be used for music.</p> <p></p> <p>The Apple II, also launched in 1977, had very similar specs, with the MOS Technology 6502 CPU running at 1 MHz and a similar memory capacity, but was a lot more expensive.</p> <p>Another comparison to make is one between a thin-and-light laptop from 2014 (a MacBook Air) with probably the fastest supercomputer in Belgium 20 years earlier, the IBM SP2 system at the KU Leuven. The SP2 was a distributed memory system with 16 nodes. We added the memory of all nodes together in the table below.</p> 1994: SP2 @ KU Leuven 2014: Thin-and-light MacBook Air CPU 16 66MHz IBM POWER2+ CPUs 1 dual-core Intel CPU @ 1.3 GHz base Speed 4.25 Gflops DP 41.6 Gflops DP (more with turbo boost) RAM memory 3 GB 8 GB RAM Permanent storage 80 GB 256 GB on SSD Weight 1,000 kg estimated Less than 2 kg with power supply Power required &gt;10 kW estimated 45 W Estimated cost Roughly 1 M EURO in 1994 money 1300 EURO <p>The laptop in 2014 was definitely a lot more powerful than the supercomputer in 1994. And the latter again had to be shared with many researchers...</p> <p>The IBM SP2 of the KU Leuven entered at position 198 in the Top500 list of November 1994,  a list that ranks supercomputers based on their performance for solving very large dense systems of linear equations. One year later it was only at position 409. 20 years later, in 2014, you needed a machine running the benchmark at 364 Tflops to make it to position 198 in tht list. And one and a half year later you would have dropped from the list with that performance.</p> <p>So we do note a fast evolution. But it is not by accident that the comparison was with a computer from 2014. After that the evolution started to slow down a bit, but more about that later in the  chapter \"Putting it all together\"...</p>"},{"location":"C02_Processors/C02_S11_Difference/","title":"An important difference between a PC and a supercomputer...","text":"<p>As we have seen, many supercomputers share a lot of similarities with  PC's. Yet there is an important difference that cannot be underestimated:</p> <p>The answer to the question \"Can a PC be faster than a supercomputer today?\" may actually surprise some, but the correct answer is \"Yes\". </p> <p>That is because they are optimised for very different things. Hence software that is written with PC hardware in mind does not always perform as well on supercomputers, and there are some science domains where the step from PC or small server to a supercomputer is becoming a big problem.</p> <p>We will discuss one cause now, while another big problem when moving from a PC or small server will be discussed in the  chapter on storage.</p> <p>The power consumption of a chip does not scale linearly with the clock speed of a chip. Doubling the clock speed for a given core architecture requires far more than twice the power. In fact, when close enough to the limits of the  design a factor of 4 or more is not uncommon. Now for ease of computing assume a factor of 4. So assume that for a given core architecture we would be able to run 4 cores at 4 GHz using a total of 100W of power. Then with the same amount of power we would be able to run 16 cores at 2 GHz. Now to make computations easy we also assume  that the IPC would be the same at both clock speeds (which is not the case as  the wait time for data from main memory doesn't change much with the clock speed), then each core in the first configuration, with the cores running at 4 GHz, would be twice as fast as each core in the second configuration. A single-threaded application would run twice as fast on the first configuration as on the second. However, the total amount of instructions that the whole package could process is twice as high in the second configuration. Even though each core is only half  as fast, we have four times the number of cores. So a parallel application that can use all cores effectively would be twice as fast in the second configuration.</p> <p>The reality is a bit more complex as nowadays CPUs don't run at a fixed clock frequency but have a so-called turbo mode that can run some cores at a higher clock speed when the other cores are not used, or can even temporarily increase the clock  speed and power consumption under a heavy load (or, e.g., when a program doesn't make heavy use of vector instructions). But even then we still see important differences between the chips used in PC's and in supercomputer nodes.</p> <p>Processors for your desktop PC and to some extent laptops are optimised to run moderately parallel and serial applications well. So the choice was made to use fewer cores but run those at a higher clock speed. Moreover, if not all cores are in use, the cores that are in active use can be clocked even higher and laptop chips may even have an extremely high single core boost frequency compared to their base frequency, the frequency they run at  in normal use with a normal power consumption. That turbo boost allows to run single  threaded applications at higher speed, while keeping some of the benefits of more but slower clocked cores for better parallel throughput per Watt.</p> <p>Processors for supercomputers and servers are optimised for a different application profile. They use way more cores, but run those cores at a lower clock speed and the turbo boost algorithms are also tuned differently, not boosting to nearly as high frequencies as in the PC chips. </p> <p>E.g., consider the latest chip generation of Intel in February 2023. The top version of the 13<sup>th</sup> gen \"raptor lake\" CPU, the i9-13900K, has two types of cores, the 8 performance  cores that support very fast single-thread execution, and the 16 efficiency cores that  improve parallel performance at a given power consumption, and are also used when the high speed is not needed. The chip has a base power of 125 W, but when clocking up some or all cores its power consumption can temporarily increase to 253 W until the chip becomes too hot. For the performance cores the base frequency is 3 GHz but one core can clock up to 5.8 GHz  to run single threaded applications at very high speed. Intel's top server chip at that time was the 4<sup>th</sup> generation Xeon Scalable processor code named Sapphire Rapids.  The core is actually very similar to the cores used in Raptor Lake, though the latter have  the AVX-512 instructions disabled as these instructions are not supported by the  efficiency cores. The top-in-the-line variant, with 60 cores, runs those cores at a base frequency of only 1.9 GHz, 35% slower than the PC processor, and only allows the frequency  to boost to 3.5 GHz, only 15% higher than the base frequency of the PC processor. </p> <p>Another illustration is the table below, for two older processor types before the days of turbo boost etc. in server processors, when it was much easier to analyse performance:</p> <p> E5-2623v3 E5-2660v3 E5-2637v4 E5-2698v4 cores 4 10 4 20 clock 3 GHz 2.6 GHz 3.5 GHz 2.3 GHz power 105 W 105 W 135 W 135 W Gflops peak DP/core 48 41.6 56 36.8 Gflops peak DP/socket 192 416 224 736 <p></p> <p>The first two processors in this table from the generation code-named Haswell,  are two 105 W parts, which was a level of power often used for supercomputer nodes in those days. We see that for that same amount of power we could run 4 cores at 2.6 GHz, but 10 cores at a speed that was less than 15% lower.  The 4 core chip of course offers slightly higher single thread performance, but if we look at the total amount of double precision floating point  operations the chip was in theory capable of, then the 10-core chip is over twice as fast at the same power level. The last two columns in the table are for two 135 W parts from the next generation, code-named Broadwell, and are two 135 W parts (there was one part with even more cores but that one is left out of the comparison as there is no low core count variant that runs at the same power). The 4-core part can now run at 3.5 GHz which is considerably faster than the 20-core part that runs at only 2.3 GHz, 35% lower. This of course translates into the single thread performance. But now the total number of  double precision floating point operations the 20-core chip can do per second is  more than three times that of the 4-core part at the same quoted power level.</p> <p>This shows that if your software or at least your workflow is not parallel, running on a supercomputer is a waste of money. Then there is no faster machine to run your application than a high-end gaming PC (though you can omit the expensive graphics card as your software will probably not be able to use that one properly either).  And incidentally, these PC's also tend to have disk storage that is way faster than a supercomputer can offer for disk access patterns with lots of small random reads, something that we will discuss in the  chapter of storage.</p> <p>Supercomputers need appropriate software to function as a supercomputer!  In fact, one key point of supercomputing since the mid '80s has been adapting  software to be able to use cheaper hardware at scale rather than investing in extremely expensive hardware that still cannot do the job as well as good software can do.</p>"},{"location":"C02_Processors/C02_S12_Lessons_learnt/","title":"Lessons learnt","text":"<p>We have seen that there are four levels of parallelism in supercomputer, and  three of them are also relevant for PCs or even smartphones:</p> <ol> <li> <p>Instruction level parallelism through pipelining and superscalar execution,     present to some degree in virtually all processor designs these days, even     very low power ones for smartphones.</p> </li> <li> <p>Vector instructions (and now coming up, matrix instructions) to do more work     per instruction. These are also used in even a lot of low power designs,     have been crucial to get additional performance from even PC processors in the      past 20 years, and are even useful in lots of integer based compute tasks     on modern CPUs (AI inference, image and sound processing, ...)</p> </li> <li> <p>Shared memory parallelism: Many cores sharing a single memory space.      This type of parallelism is also present in PCs and even smartphones,     but supercomputers take it to a larger scale, reaching limits of scalability     of operating systems.</p> </li> <li> <p>Distributed memory parallelism. This is the only level of parallelism that is     not found in your PC or smartphone. Both may more and more rely on distributed     apps where some of the functionality comes from remote servers, but this kind     of distribution uses very different techniques from what is needed to run, e.g.,     a simulation code on a distributed memory infrastructure.</p> </li> </ol> <p>The processors in a supercomputer are optimised for parallel performance and not for sequential performance. Hence a single threaded application may run a lot faster on a high-end PC then on a supercomputer.</p> <p>It is also important to realise that computers and compilers have evolved over the years. As part of the performance growth of processors comes from the addition of new (mostly vector) instructions, a ten year old binary won't run efficiently. Due to other changes in the system architecture, it might not even run at all. Code that remained  unmodified for 20 years probably will not be able to extract all parallelism of modern supercomputers, even when properly recompiled. It might in fact not even be efficient on your PC anymore.</p> <p>It is also important to realise that supercomputers contain 1000's of times  more parts than a regular PC. Even though care is taken to add some redundancy where affordable and even though there is a focus on using rather reliable components, a supercomputer is not as reliable as your PC. Crashes are often contained to a small part of the supercomputer (e.g., single nodes failing), but if you run on a large number of nodes you will experience a lot more problems than you are used to from  running small problems on a PC. It is not a good idea to run a large multi-day job without making sure that it stores enough data from time to time to restart an interrupted computation.</p>","tags":["shared memory","distributed memory","ILP (Instruction Level Parallelism)","vector instructions"]},{"location":"C02_Processors/C02_S13_Further_reading/","title":"Further reading","text":"<ul> <li>Vectorisation<ul> <li>Article in HPC wire: \"Vectors: How the Old Became New Again in Supercomputing\"       on the re-emergence of vector instructions</li> </ul> </li> <li>Vector instruction extensions for CPUs<ul> <li>AVX instruction set:<ul> <li>Intel AVX whitepaper</li> </ul> </li> <li>AVX512<ul> <li>Talk about the AVX512 history (by Tom Forsyth who works/worked at Intel) and     corresponding slides</li> </ul> </li> <li>AVX512 has been revised in an improved specification AVX10 as all the extensions have     turned it into a mess.<ul> <li>Intel AVX10 whitepaper</li> <li>Article \"AVX10/128 is a silly idea and should be completely removed from the specification\"     also contains nice information about the evolution of vector instruction sets in     the x86 architecture.</li> </ul> </li> <li>YouTube recording of a talk on the history of vector instructions in x86</li> </ul> </li> <li>Interconnects to link processor packages together, or dies in a package,     preserving global memory:<ul> <li>UPI - Intel Ulta Path Interconnect on WikiPedia</li> <li>CXL - Compute Express Link</li> <li>UCIe - Unified Chiplet Interconnect Express</li> <li>AMD Infinity Fabric<ul> <li>Very technical article on WikiChip</li> </ul> </li> </ul> </li> <li>YouTube playlist for this course      (with many of the videos for other chapters of these notes)</li> </ul>"},{"location":"C03_Memory/","title":"The memory hierarchy","text":"<ul> <li> <p>The memory performance gap</p> </li> <li> <p>The memory pyramid</p> </li> <li> <p>AMD Rome revisited</p> </li> <li> <p>AMD Milan revisited</p> </li> </ul>"},{"location":"C03_Memory/C03_S01_Performance_gap/","title":"The memory performance gap","text":""},{"location":"C03_Memory/C03_S01_Performance_gap/#what-is-the-memory-performance-gap","title":"What is the memory performance gap?","text":"<p>Memory bandwidth and latency haven't improved at the same rate as processor performance.</p> <p>Let us compare a workstation-class Intel architecture processor from  1996 (that would have been used in distributed memory supercomputers based on the Intel architecture) with the high-end AMD Rome processor used in  several big supercomputers around 2023.</p> <p>The 1996 processor would have been an Intel Pentium Pro processor running at a  clock speed of 200 MHz and capable of 1 floating point operation per cycle, so a floating point performance of 0.2 Gflops on its single core. Its memory technology would have  been PC100 SDRAM. The memory latency would have been on the order of 300 ns, and the processor had a 64-bit memory bus running at 66 MHz, so was capable of a theoretical peak memory bandwidth of 0.52 GB/s.</p> <p>The 2023 Intel architecture processor would have been the AMD EPYC 9654 processor, a processor from the Genoa family. This beast has 96 cores that  with proper cooling can run at 2.4 GHz (guaranteed base clock with proper cooling).  When counting the so-called fused multiply-add instruction as two floating point operations (a fused  multiply-add instruction is an instruction that computes \\(a*b+c\\) in a single pass), and taking into account that each core has 2 256-bit vector units, the chip is capable of 16 floating point operations per cycle per core which translates into a theoretical peak floating point performance  of 3686 Gflops. Its memory architecture is DDR5, with a memory latency of somewhere aroudn 110-120 ns.  You'll find different and lower numbers also, but it all depends on how you measure (to the core, to the socket, first byte or the whole cache line, ...) and whether you take best case or average performance. Each socket has 12 64-bit memory channels running at a data rate of 4800 MHz, so the theoretical peak memory bandwidth is 461 GB/s.</p> 1996 Pentium Pro 2023 AMD EPYC 9654 Change Peak flops 0.2 Gflops 3686 Gflops \\(\\times 18,430\\) Peak memory bandwidth 0.52 GB/s 461 GB/s \\(\\times 890\\) Memory latency 300 ns 80-120 ns \\(/ 2.5 - 4\\) <p>While peak flops have exploded over this period - a factor of over 18,000 on a per  socket basis between chips that are meant for technical computing - the memory bandwidth has not followed that pace as it grew only with a factor of 890. And we actually took the first member of a new generation that usually gives higher bandwidth per core or per flop for the 2023 chip. And even worse, memory latency only decreased with a  factor around 3 and has in fact been stagnant for many years now. This is an excellent example of how parameters of a computer system evolve at vastly different rates.</p> <p>Memory bandwidth is clearly a problem. The 1996 Pentium Pro could produce 1.6 GB/s  of double precision floating point results which is three times the bandwidth of the memory system. And this does not yet take into account that to produce those results, one needs input too. So clearly this chip could not work at full speed at all if all data has to come from main memory. However, for the 2023 AMD EPYC processor the situation is even much worse. That chip can in theory produce 29,488 GB/s of results with is over 60 times the memory bandwidth (and its predecessor was even worse as there was a big generational jump in memory bandwidth). So clearly this chip would be running at an even lower fraction of its peak speed if all data had to come from memory.</p> <p>The memory latency is an even bigger problem. 300ns latency on the Pentium Pro means that 60 floating point operations could have been done in that period.  However, the 120ns latency on the AMD EPYC from 2023 corresponds to more than 400,000 floating point operations. Obviously this is an exaggeration as the 2023 chip is a multi-core chip and in fact multiple cores can use different parts of the main memory in parallel, but still...</p>"},{"location":"C03_Memory/C03_S01_Performance_gap/#speed-limiting-factors-that-cause-the-memory-performance-gap","title":"Speed-limiting factors that cause the memory performance gap","text":"<p>Memory speed and latency are limited by both physical and economical constraints.</p> <p>Larger memories are usually slower due to physical constraints: If a memory device becomes physically larger, it has to be put further away from the CPU so the travel distance for electric signals to the memory becomes longer, but as the memory device becomes larger travel distances inside the memory device also become longer. Longer  distances come with expensive signal regeneration introducing additional delays and/or lower transfer speeds.</p> <p>The number of connections one can make from the CPU to other devices is also limited by both technological and economical constraints. Those connections are expensive, so the bandwidth one can reach to devices external to the CPU socket is also limited. Also, there are faster memory types than the DRAM technology currently used, but  they are expensive and also difficult to scale economically to large capacities.</p> <p>There are however two other possibilities.</p> <p>On-die memory can be very fast, but is is limited in capacity due to size and price constraints. For that one uses a different type of memory cell that takes more space per bit. Moreover, very wide and high-speed data paths are possible to that memory. (Though here too problems are starting to appear as the size of the memory cell doesn't shrink nicely with new process technologies.)</p> <p>Second, modern packaging technologies enable the integration of memory in the package that also contains the CPU dies. As that memory sits closer to the CPU, higher transfer speeds are often possible, and it is also possible to use a much wider data path the memory. Some CPUs and GPUs use this to add additional very fast memory of the type discussed in the  previous paragraph, but it is now also a popular technique to add memory that uses the same DRAM technology as main memory. It is in fact a standard practice in smartphones. The Apple Silicon M-series processors also use this technology, with a data path of 512 bits wide (8 64-bit channels) in the Max-series, resulting in a memory bandwidth to the CPU and integrated GPU that is higher than the per-socket memory bandwidth in many supercomputer CPUs.  It is also a standard technology in GPUs for scientific computing. E.g., the AMD MI100 and NVIDIA V100 GPUs for scientific computing and the Fujitsu A64fx CPU for supercomputers all have a 4096-bit wide data bus to memory (in fact, 4 1024-bit buses and but at a slower speed). The NVIDIA A100 and regular H100 have 5120 bit wide buses (in fact, 5 1024 bit buses),  the NEC SX-Aurora TSUBASA first and second gen vector accelerators have a 6144 bit wide bus (in fact, 6 1024-bit buses) and the AMD MI250X GPU used in LUMI has even an 8192 wide bus (in fact, 8 1024-bit buses). However, that memory is limited in capacity. E.g., the MI250X is limited to 128 GB. All these processors or accelerators use various generations of so-called HBM memory.</p> <p>E.g., the theoretical peak memory bandwidth of an MI250X is 3.2 TB/s and the peak memory bandwidth of an NVIDIA H100 compute GPU is around 2 TB/s while an NVIDIA 4090 gaming card, which does not have the memory integrated in the package, has a theoretical peak memory bandwidth around 1 TB/s.</p>"},{"location":"C03_Memory/C03_S01_Performance_gap/#what-can-we-do-to-deal-with-the-memory-performance-gap","title":"What can we do to deal with the memory performance gap?","text":"<p>The previous discussion already gives a key to the solution: We have access to different memory  technologies with different speed and capacity trade-offs, so the solution is to go for a hierarchical setup that combines different technologies, preferably in a way that is transparent to the user when it comes to correctness of the program so that the processor remains compatible with previous generations. This is exactly what happens: All modern processors implement so-called  caches, buffer memory using a very fast memory technology that is managed by the processor hardware (with most processors also giving some level of user-control).</p> <p>In fact, even for that memory a hierarchy is used, typically with three levels:</p> <ul> <li> <p>Level 1 or L1 caches are very small and very specialised as they sit very close to     specific parts of the processors. Processors will often have one for instructions and     one for integer data. Floating point data is not always stored in that cache. The typical     size is in the range of 32 to 64 kB.</p> </li> <li> <p>The level 2 or L2 cache is larger. On processors for scientific computing it usually still     at the core level and not shared between cores, but it can now contain both instructions and     all data types. Sizes range from 256 kB to 2 MB on modern CPUs.</p> </li> <li> <p>The level 3 or L3 cache is typically shared by a number of cores (but not always by all cores).     Both the total capacity and capacity per core is all over the place in modern CPUs. E.g.,     on AMD Milan and Genoa EPYC CPUs the L3 cache is shared by all cores of a CCD (chiplet) and     is 32 MB or 96 MB per CCD.</p> </li> </ul> <p>Some vendors are starting to experiment with even different technologies. E.g, Intel now has some processors for scientific computing from the \"Sapphire Rapids\" generation that also embed  so-called HBM memory on the socket. These chips are currently known as the Intel Xeon CPU Max 9xxx series. They contain 4 HBM stacks, for a theoretical memory bandwidth of 1.6 TB/s and a practical memory bandwidth of 1 TB/s. They can be used either as cache for the larger external memory, or directly as memory and in fact the processor could even run without external DRAM memory attached.</p> <p>Though caches - at least on CPUs - are transparent with respect to correctness, they need to be taken into account to get a good performance. It is very important to organize data access in code so that data  accesses become local (i.e., the next memory cell you need is close to the one you just accessed) and predictable in nature and not random. Ideally you'd stream data through a processor and do as much  computations as possible with a bit of data before progressing to the next bit. We're not talking about  a performance doubling here, but rather about factors of 200x or even 2000x for the best codes compared to a bad code for essentially the same algorithm.</p> <p>Luckily we don't need to write all that code. Many scientific codes spend most of their time in a few pretty standard routines for linear algebra, FFT, etc. There are very good optimised libraries for linear algebra, FFT, image processing and other core operations of popular algorithms. It is important to use those libraries and not think that we can easily do better. One example is the BLAS library for basic vector and matrix operations. This library is used in many other linear algebra libraries. The reference implementation in  Fortran sucks on modern systems, but there are many excellent commercial and free implementations that give great performance: Intel MKL, AMD Core Math Library, OpenBLAS, Bliss, Atlas, ... Basically any microprocessor company that builds processors that may be used in scientific applications, will support an optimised BLAS library. For other types of applications, e.g., solving PDEs, there exist frameworks that will help you in organising data access in a proper way.</p> <p>So even if you only use code, it is important that you realise that not all code is created equal and that big performance differences do occur. Moreover, code from books such as numerical recipes typically sucks from a  performance point of view. </p>"},{"location":"C03_Memory/C03_S02_Memory_pyramid/","title":"The memory pyramid","text":"<p>We've seen several types of memory now, and that is structured in a hierarchy. The typical hierarchy for a regular CPu is depicted above.</p> <ul> <li> <p>Each core has registers for integer data, floating point data and/or vectors,     and some special registers. The architectural register set is rather small but as this     limits parallel execution inside the processor, modern processors have a lot of     hidden register space that is used through an automatic mechanism called      \"register renaming\". E.g., the AMD Rome or Milan processors have less than 1 kB      of register space in the instruction set, but the total physical register space     (used with register renaming) is over 6 kB.</p> </li> <li> <p>Processors typically have separate L1 caches for instructions and for data     (where the data one is sometimes bypassed when using vector instructions).</p> </li> <li> <p>The L2 cache in processors for scientific computing is typically on a per core     basis.</p> </li> <li> <p>The L3 cache is shared by a number of cores, sometimes even by all cores in      a socket</p> </li> <li> <p>The next level in the hierarchy is the main RAM memory, which due to the NUMA     architecture may appear as multiple levels with different latency and bandwidth</p> </li> <li> <p>The last level is one that will actually be de-activated on many supercomputers     as it is too slow: When RAM memory is exhausted, the OS can swap some of the     content to disk storage. But even without swapping enabled we can still see     the disk as an additional storage space, but then one that we need to      manage ourselves in software.</p> </li> </ul> <p>Even this picture is still oversimplified. Processors have other types of small caches, but those are harder to exploit by our programming style so we should not really take them into account here. RAM memory may also exist out of multiple layers instead of the one depicted in the picture.</p> <ul> <li> <p>The Intel Xeon Max 9xxx series has 64 GB of high-bandwidth RAM on each socket and then     up to 1 TB or more of lower bandwidth memory connected externally to the socket.</p> </li> <li> <p>Recently for large memory computers, and especially for database systems, a new type     of RAM package was proposed that is connected to an expansion card on the CXL-bus,     an evolution of the PCIe bus (and IBM has its own alternative in its CPUs).      Obviously this memory sits even further away from the CPU so we can expect a higher     latency and less bandwidth. Given that scientific computing applications are often     already memory latency and/or memory bandwidth limited, this memory type may never     become very useful in supercomputers.</p> </li> </ul> <p>Let us compare the memory hierarchy for the \"Leibniz\" cluster and AMD Rome section of Vaughan of the UAntwerpen infrastructure for 2023, and then with Hawk, one of the largest CPU-only systems in Europe, at HLRS in Germany, and also based on the AMD Rome CPU</p> Leibniz \"Broadwell\" Vaughan \"Rome\" Hawk \"Rome\" architectural registers &lt;1 kiB/HW thread &lt;1 kiB/HW thread &lt;1 kiB/HW thread physical registers ~7 kiB ~6 kiB ~6 kiB L1 cache 2 x 32 kiB/core (I+D) 2 x 32 kiB/core (I+D) 2 x 32 kiB/core (I+D) L2 cache 256 kiB/core 512 kiB/core 512 kiB/core L3 cache 35 MiB/socket 128 MiB/socket 256 MiB/socket L3 organization 35 MiB/socket 16 MiB / 4 cores 16 MiB / 4 cores RAM 128 or 256 GiB/node 256 GiB/node 256 GiB/node disk swap disabled disabled disabled disk storage 600 TB 600 TB 26 PB size cluster 152 nodes 152 nodes 5632 nodes <p>So we see that there is in fact very little difference in the hierarchy for a university level  supercomputer as Leibniz and Vaughan and a very large national or international supercomputer. However, the total size is very different.</p>"},{"location":"C03_Memory/C03_S03_AMD_Rome/","title":"AMD Rome revisited","text":"<p>The older nodes of the UAntwerpen cluster Vaughan, the VSC Tier-1 system Hortense  and the login nodes and data analytics nodes of the European pre-exascale supercomputer LUMI are equipped with AMD Rome processors. These CPUs have a very complex hierarchy:</p> <p>The picture below depicts the one of the older nodes of the UAntwerpen cluster Vaughan with 2 32-core AMD Rome generation CPUs:</p> <p></p> <p>Each socket in the 32-core version has 4 chiplets with 8 cores each (those chiplets are also called the CCDs) and one I/O chiplet. Each CCD has two groups of 4 cores the L3 cache is shared within a group but not between groups. These groups of 4 cores are also called a CCX or core complex. The two CCXs on a CCD share the connection to the I/O die with the memory controllers. Each I/O die has 4 memory controllers, and each CCD has its preferred memory controller through which memory access is  slightly faster. I.e., we have a mild NUMA architecture on the socket.</p> <p>The picture for the 64-core variant used in many large supercomputers is not that  different. In that variant there are 8 CCDs with 2 CCXs each, and each CCD has its  own connection to the I/O die, but there are still only 4 memory controllers, now with 2 preferred CCDs or 4 preferred CCXs each.</p> <p>All this leads to the following hierarchy with up to 6 levels:</p> <p></p> <ol> <li> <p>Each core supports two hardware threads. These two threads share the L1 and L2 caches.     They are very close together and can communicate through those caches resulting in      a short transfer delay and high data transfer bandwidth.</p> </li> <li> <p>There are 4 cores per CCX, and these cores share the L3 cache.     This puts them a little further apart, where communication now has to go all     the way up to the L3 cache.</p> </li> <li> <p>There are 2 CCXs per CCD. These two CCXs share a link to the I/O die.</p> </li> <li> <p>When using NUMA-in-a-socket (typical for HPC systems), there is only one     CCD per NUMA domain in the 32-core variant (so not really an extra level)     but 2 CCDs per NUMA domain in the 64-core variant. They share a memory     controller with 2 64-bit lanes to memory. They also share some PCIe lanes     but that is less important in this context.</p> </li> <li> <p>4 NUMA nodes share a socket. These NUMA nodes also share a connection     to the second socket (though technically speaking the situation is a bit     more complex and too complex for this course).</p> </li> <li> <p>At the last level, a node has two sockets. On Vaughan and Hortense the two     sockets share the inter-node interconnect.</p> </li> </ol> <p>The further one moves down along this list, the higher the distance between executing threads, so the lower the bandwidth at which they can communicate  and the higher the latency.</p> <p>To get good performance on AMD Rome processors, it is important to map the application properly onto this hierarchy. If not, very erratic performance is possible and we have seen huge performance differences between runs. The problem is that what is a good mapping, depends on the application and we have no way to give a recipe that works for every application.  You need to understand the behaviour of your application to do a proper mapping and it depends a lot on how your code accesses memory.</p>"},{"location":"C03_Memory/C03_S04_AMD_Milan/","title":"AMD Milan revisited","text":"<p>The AMD Milan generation that succeeded the Rome generation offers a slight simplification of the picture. Now the size of the CCX is 8 cores so CCX and CCD are the same. Otherwise the picture looks fairly identical, but with levels 2 and 3 merged. Cores within a CCD now share both the L3 cache and link to the I/O die.</p> <p></p> <p>But here again a proper mapping of an application onto resources can turn out to be essential for good performance.</p> <p>Doing that mapping on a node that is shared with multiple users is impossible as the Slurm scheduler used on many clusters does not fully support the hierarchy and does not offer enough control over partial allocations on a node. After all, supercomputers and their system software are made in the first place for big applications that can fill one or more nodes...</p> <p>The Milan CPU is used in some of the newer nodes of the UAntwerpen cluster Vaughan, most phase 2 nodes of the VSC Tier-1 system hortense, and the regular compute nodes and GPU nodes of the EuroHPC pre-exascale system LUMI.</p>"},{"location":"C04_Storage/","title":"Storing data on supercomputers","text":"<ul> <li>Introduction</li> <li>Problems with a parallel disk setup</li> <li>Parallel file systems</li> <li>A storage revoulution?</li> <li>To remember</li> <li>Further reading</li> </ul>"},{"location":"C04_Storage/C04_S01_Introduction/","title":"Introduction","text":"<p>We've seen that physics makes it impossible to build a single processor core that is a thousand or a million times faster than a regular CPU core in a PC and that we need to use parallelism and lots of processors instead in a supercomputer. The same also holds for storage. It is not possible to make a single hard disk that would spin a thousand times faster and have a capacity a thousand times more than current hard disks for use in a supercomputer. Nor would it be possible to upscale the design of a solid state drive to the capacities needed for supercomputing and at the same time also improve access etc.</p> <p>A storage system for a supercomputer is build in the same way as the supercomputer itself is build from multiple processors: Hundreds or thousands of regular disks or SSDs are combined with the help of some hardware and mostly clever software to appear as one large and very fast disk. </p> <p>In fact, some of this technology is even used in PCs and smartphones as an SSD drive also uses multiple memory chips and exploits parallelism to get more performance out of the drive as would be possible with a single chip. This is why, e.g., the 256 GB hard drive in the M2 MacBook Pro is slower than the 512 GB one as it simply doesn't contain enough memory chips to get sufficient parallelism and saturate the drive controller.</p> <p>However, just as not all programs can benefit from using multiple processors, not all programs can benefit from a supercomputer disk setup. A parallel disk setup only works when programs access large amounts of data in large files. It can improve bandwidth a lot, but the latency of each individual device still restricts the latency of the system as a whole. And the same is true for storage as for computing: The storage of your PC can be faster than the shared storage of a supercomputer if you don't use  the supercomputer storage in the proper way. But similarly accessing files in the right way may make your already fast PC storage even faster as there are applications that are so badly written that they use the SSD in your PC also at only 5% or 10% of its potential data transfer speed...</p>"},{"location":"C04_Storage/C04_S02_Problems/","title":"Problems with a parallel disk setup","text":""},{"location":"C04_Storage/C04_S02_Problems/#disks-break","title":"Disks break","text":"<p>Hard drives fail rather often. And though SSDs may not contain moving parts, when not used in the right way they are not that much more reliable. Given that a single drive on average fails after 1 or 2 million hours (which may seem a lot), in a stack of  a thousand drives one can expect that a drive might fail every 50 to 100 days. Loosing some data every 50 days is already bad, but if all disks are combined with software to a single giant disks with even average sized files spread out over  multiple disks for performance, much more data will be damaged than that single disk can contain. This is clearly unacceptable. Moreover, we don't want to stop a supercomputer every 50 or 100 days because the file system needs repairs.  Let alone that big supercomputers have even bigger disk systems...</p> <p>The solution is to use extra disks to store enough information to recover lost data by using error correcting codes. Now hard disks and SSDs are block-oriented media: data is stored in blocks, often 4 kiB in size, so the error correction is done at the block level. A typical setup would be to use 8 drives with 2 drives for the error  correction information, which can support up to two drive failures in the group of 10. But that has implications for reading and writing data. This is especially true for writing data, as whenever data is written to a block on one disk, the corresponding blocks on the disks with the error correction information must also be updated. And that requires also first reading data to be able to compute the changes in the error correcting information. Unless of course all corresponding blocks on the 8 disks would be written concurrently, as the we already have all the information to also compute the error correction  information. But that makes the optimal block size for writing data effectively 8 times  larger...</p> <p>For reading data in such a setup you can actually already benefit as you can read from all drives involved in parallel.</p> <p>This technique is also known as RAID, Redundant Array of Inexpensive Disks, and there exist several variants of this technique, some only to increase performance and others to improve reliability also.</p> <p>Error correction codes are in fact also used to protect RAM memory in servers and supercomputers, or internally in flash drives, and sometimes also in communication protocols (though they may use other techniques also).</p>"},{"location":"C04_Storage/C04_S02_Problems/#file-system-block-size","title":"File system block size","text":"<p>A file system organizes files in one or more blocks (which can be different from the blocks on disk). A block in a file system is the smallest element of data that a file system can  allocate and manage.</p> <p>On a PC, the block size of the file system used to be 512 bytes though now it is often 4 kiB. However, on a supercomputer this would lead to an enormous amount of blocks which would become impossible to manage. </p> <p>There are two solutions for that. One solution is to simply use a larger block size, which is the case in, e.g., the IBM Spectrum Scale file system. Larger blocks are also a better fit with the RAID techniques used to increase the reliability of the storage system. However, as the block is the smallest amount of storage that can be allocated in a file system, it also implies that very small files will still take a lot of space on such a file system (the size of a block), though some file systems may have a solution for really small files. This can lead to a waste of space if many small files are used like that. The disk setup used at the UAntwerp HPC service until 2020 suffered from this problem. In fact, we once had a user who managed to store 36 bytes worth of data while consuming over 640 kiB on the file  system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. Now the first  IBM-compatible PCs had a memory limit of 640 kiB...</p> <p>The second solution is to use a 2-level hierarchy. Big files are split in a special way in smaller files called objects that are then stored on smaller separate servers called the object servers.  As these servers are smaller, they can use a smaller block size. And small files would use only a single object, making the smallest amount of disk space consumed by a file the block size of a single object server.  Examples of this approach are the Lustre and  BeeGFS file system used in supercomputing. However, in such systems it often turns out to be necessary for optimal peformance to tell the system how to distribute the data across objects as that depends strongly on the size of the files and how the files are used.</p> UAntwerp-specific <p>The supercomputer storage of the CalcUA facility of the University of Antwerp used the IBM Spectrum Scale file system (then still known as GPFS) for its disk volumes. The scratch storage had a block size of 128 kiB. One user managed to store 36 bytes worth of data while consuming over 640 kiB on the file  system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file.  And that user ran thousands of tests each storing 5 such files... Now remember the first IBM-compatible PCs had a memory limit of 640 kiB and students had to use those to write a complete Ph.D. thesis...</p> <p>The storage that was installed in 2020 at the CalcUA service uses BeeGFS for the file system. Which comes with its own problems as we shall see later.</p>"},{"location":"C04_Storage/C04_S02_Problems/#physics-and-the-network","title":"Physics and the network","text":"<p>On your PC, the storage sits both physically and logically very close to the processor.  The fastest SSDs, NVMe drives, are often even directly connected to the CPU, and on the M-series MAc this is even taken one step further, with part of the drive (the controller)  integrated into the CPU (though this probably saves more on power consumption than it gives additional speed). Moreover, at the logical level, there is only one file system in between your program and the drive. The program talks directly to the OS which talks  directly to the drive.</p> <p>With shared storage on a supercomputer the picture is very different. The storage is both physically and logically further away from your application. Physically because there are (at least) two processors and a network involved (and on the server side disks usually not be as close to the processor as on your PC, except in some of the most expensive storage systems). The physical delay caused by the network may not be that important with hard disk storage, but it is important when accessing SSDs or cached storage.  The software adds even more delays. After all, your program talks to a network file system which then sends the request to the server where it also has to pass through multiple layers of software: through the network stack, to the file server software, to the file system (which may be similar to that on your PC but doesn't have to), and back through the network stack before the answer to the request is off again to your application, where it also first has to pass through the network stack and network file system again.</p> <p>Parallel file systems often have an optimised and shorter route for reading and writing data, but often at the cost of more costly open and close operations and hence a very high cost for access to small files. And the path is still much, much longer than that to a local drive.</p> <p>This comes with a number of consequences:</p> <ul> <li> <p>Programs that open and close hundreds of small files in a short time may work slower than     on a PC. This is particularly true if all data access comes from a single thread as your     program will be waiting for data all the time. That software also puts a very high load on\\     the file systems of a supercomputer, to the extent that several supercomputer centres nowadays     take measures to keep that software off the supercomputers.</p> </li> <li> <p>Unpredictable file access patterns may make things even worse as any logic to prefetch data\\     and hide latency will fail.</p> </li> </ul> <p>One may wonder why supercomputers don't always provide local drives to cope with the slowness of shared storage. There are many reasons:</p> <ul> <li> <p>From a management point of view, there are several problems. The management environment has     to clean them at the end of a job, but it is not always clear which files can be removed if     multiple jobs are allowed to run on a single node of the supercomputer. And the software that     needs to local storage most, also tends to be the software that cannot use a full node of a     supercomputer efficiently.</p> <p>Modern Linux does have a solution to the cleaning problem (namespaces), but that solution then comes with other restrictions that users may have trouble living with, e.g., starting processes on other nodes should always be done through the resource manager software to ensure that the processes start in the right namespace. Which implies that, e.g., <code>ssh</code>, a very popular mechanism to start a session on a different host, should not be used. </p> <p>Moreover, the data becomes inaccessible to the user when the job ends, so if the so-called job script, the script that instructs the supercomputer what to do, is ended because the resources expire or because of a crash, the data on the local drive will be lost to the user. For reading one also loses time as the data needs to be copied first to the local drive.</p> </li> <li> <p>It is physically also not easy to add local drives to a supercomputer node. Supercomputer nodes     are built with very high density as it is important to keep all links in a supercomputer as short     as possible to minimise communication delays between nodes. Modern CPUs and GPUs run very hot,     while storage prefers a lower temperature. </p> <p>On an air cooled node, the storage has to be early in the air flow through the node as once the air has gone through the CPU or GPU coolers, it is way too hot to cool the storage sufficiently. But given the small size of a supercomputer node, those storage devices may hinder the air flow through the node and hence make the cooling less effective. </p> <p>On a water cooled node, things aren't that much easier though the situation is improving. As  M-type SSDs (those that you insert in a slot on the motherboard close to the CPU) nowadays even need cooling in a regular PC, they have been made more friendly to the addition of cooling elements.</p> </li> <li> <p>However, reliability of SSD drives is also an issue. SSD drives based on flash memory (which is     currently practically any drive still in production) have a limited life span under a heavy write     load, while the use suggested here, as a temporary buffer for the duration of a job, is precisely     a scenario with a high write load. </p> <p>Replacing broken hardware is an issue, made worst because of the dense construction of a supercomputer.</p> </li> </ul> <p>One may wonder why local drives are so much more common in cloud infrastructure. The constraints in cloud infrastructure are different. </p> <ul> <li> <p>Supercomputers, with the exception of some commercial clusters, are built to be as cost-effective as     possible. So one tends to solve problems with better software rather than adding more hardware.</p> <p>Cloud infrastructures on the other hand are usually commercial offerings. They are built at a very large scale, with often an even more custom hardware design, and they are simply overprovisioned. E.g., a server may have two SSD drives where the software will simply switch over to the second drive when the first one breaks, but the broken one will never be replaced.</p> </li> <li> <p>The management model of a cloud infrastructure is also very different. Cloud is based on virtualisation     technologies to isolate users, and let users built a virtual network of servers in which regular Linux     access methods can be used. These layers of software add additional overhead (even with hardware features     to support virtualisation) which is undesirable on a supercomputer where each additional 100 nanoseconds of     communication delay may limit how far a job can scale on the computer. Note though that the virtualisation     overhead, thanks to hardware support, has become low enough that small supercomputer jobs can run very     well on some cloud infrastructures.</p> </li> </ul>"},{"location":"C04_Storage/C04_S02_Problems/#metadata","title":"Metadata","text":"<p>Each file contains both the actual data in the file, and so-called metadata such as the name of the file, access rights to the file, the date of creation and of last use, ...  This metadata is stored in a directory which on a traditional file system with folder and subfolder structure is a special type of file for each (sub)directory.  This implies that if you do many small disk accesses to files in the same directory, or store a lot of files in a single directory and access them simultaneously, you create a bottleneck as many updates are needed  to that directory. Most file systems are not very good at parallelising that directory access.</p> <p>Bad patterns of metadata access are probably the most common source for performance problems on supercomputer file systems. A typical scenario is when in a distributed memory application, each process creates its own set of files in the same shared directory, rather than use a feature called parallel file I/O to create a few giant files to which data is written in orchestrated multi-node operations. This can bring almost every supercomputer file system to its knees. Building a file system that can cope with this load might be technologically impossible or would at least make storage an order of magnitude more expensive.</p> <p>An equally stupid idea is to open a file before every write and then close it again to ensure that data is written to disk immediately. This is already an expensive operation on a PC with local storage that will slow down your program a lot if those writes are very frequent, but it will kill almost any networked file system and is even worse for the so-called parallel file systems on supercomputers as they tend to have more expensive open and close operations.</p>"},{"location":"C04_Storage/C04_S03_Parallel_filesystems/","title":"Parallel file systems","text":"<p>On PCs and many more regular file server file systems, each block on disk can  be used for either metadata or data. (Well, in practice there will be a zone that is used exclusively for metadata but that zone is extended transparently when needed.) These file systems are very flexible and support small file sizes very well. But at the same time it is very difficult to ge very high bandwidth, as a file will be managed by a single storage server, so even with the fastest and most expensive storage attached to the storage servers, the  single server would ultimately be a performance bottleneck.</p> <p>Larger supercomputers need a different kind of file system for higher performance, one where multiple servers can be used concurrently to access a single file. This is called a parallel file system. </p> Popular examples and their use at the VSC <ul> <li> <p>Lustre is probably the most used file parallel file system on large supercomputers.     It is used at KU Leuven, the VSC Tier-1 system Hortense at UGent and on the      LUMI system in Finland, a pre-exascale     supercomputer project to which Belgium and Flanders participate.</p> <p>Lustre uses two layers. It is implemented on top of another file system that is used for the actual disk accesses.</p> <p>Lustre is open-source but with companies offering support and doing most of the development.</p> </li> <li> <p>BeeGFS has a very similar architecture as Lustre     and is used for the scratch file system on the clusters of the University     of Antwerp.</p> <p>BeeGFS is open-source, but most of the development is done by  a spin-off of a German research laboratory, and they also offer (paid) support.</p> </li> <li> <p>In the early days of the VSC, IBM Spectrum Scale,     then known as GPFS,     was used on all clusters. However, increasing licensing costs made this impossible.</p> <p>Spectrum Scale/GPFS doesn't have the same two-layer architecture as Lustre or BeeGFS (or at least, it is not visible), but does the full management of the disks itself.</p> </li> <li> <p>Panasas PanFS is a storage system that has its roots in the     same research project as Lustre. It is a commercial offering consisting of software and     dedicated hardware. It is currently not in use at the VSC.</p> </li> <li> <p>WEKA is also a fully commercial offering, but one running on more     standard file server hardware. It requires a full SSD system though which makes it a rather     expensive offering. They claim to offer better small file performance than their competitors     and claim to have solved many of the metadata problems that we will mention below.     It is currently not in use at the VSC.</p> </li> </ul> <p>In a parallel file system, the metadata is separated from the actual data. The picture below shows the (simplified) setup for Lustre or BeeGFS:</p> <p></p> <p>A Lustre of BeeGFS system consists of three components, all connected via a high performance network (often the interconnect that is also used for MPI, etc.):</p> <ul> <li> <p>File system clients run on all compute and login nodes of the cluster. Applications talk to the      client software which then talks to the servers.</p> </li> <li> <p>The metadata servers (MDS)     take care of all metadata operations, which include controlling the process of opening and closing     files. </p> </li> <li> <p>The Object Storage Servers (OSS) store the actual data of files. That data can be spread over multiple     object storage servers.</p> <p>Note that this should not be confused with object storage such as Amazon S3 or Ceph.</p> </li> </ul> <p>To open a file for reading or writing, the file system clients talk to the metadata server. The metadata server will then return information about the location and layout of the file (which object storage servers contain which parts of the file).  However, after that the content of the file can be served by all involved object storage servers and the metadata server is no longer involved in the process of reading the file. Hence multiple clients in a parallel job can talk to multiple object storage servers simultaneously and large files can be read with very high bandwidth if the file is indeed accessed in large read (or write) operations so that this parallelism can be exploited.</p> <p>Opening and closing files are expensive operations since they do not only involve the metadata server, but that metadata server also has to talk to the object storage servers for the file.  However, subsequent reads and writes have again a normal cost compared to regular file systems, and much higher bandwidths are possible as files are served from multiple servers.  Depending on the specifics of the file system, some other operations may also be rather expensive. E.g., on Lustre, doing an <code>ls -l</code> is an expensive operation as the size of the file is not  stored on the metadata server. The metadata server needs to request all object storage servers used by the file what the size of the chunk stored on that object storage server is.</p> Storage Targets and Storage Servers <p>The real picture is even a bit more complicated than above. Each object storage server can have multiple drive pools attached, called the Object Storage Targets (OST), and files are actually distributed across Object Storage Targets. Similarly, each metadata server can have multiple Metadata Targets (MDT).</p> <p>More redundancy reasons, each OST can be attached to two OSS so that if one OSS fails, the other can take over.</p> <p>A parallel file system can produce very high bandwidth for large read and write operations coming from  optimised parallel software using the right libraries to optimise that data transport, and this at a very reasonable cost. However, it has trouble dealing with lots of small files, and metadata access, certainly to files in one directory, can be a bottleneck. Moreover, the amount of storage space for metadata is determined when the system is purchased or  configured as it is on separate servers, so there is also a strict limit to the number of files such a system can store.</p> <p>It is not uncommon nowadays for supercomputer centres to impose strict limits on what users can do on the shared file systems. Just as software needs to adapt to the processing model of a supercomputer, software also needs to adapt to the way large storage systems on supercomputers work. You cannot simply run hundreds of instances of that naive PC program that accesses thousands of files in a short time on your supercomputer, but you have to organise your data in a better way to scale to supercomputer storage.</p> The storage on the clusters at the University of Antwerp <p>At the CalcUA supercomputer service, we have different storage systems. The Linux home directory is kept small deliberately so that it can be served from a few SSD drives in a very high reliability setup to prevent data loss. The system is small, hence a broken drive should not break the bank, while there are clear advantages to using SSD drives for home directories. The home directories are on a regular Linux file system served to the  compute nodes via NFS as that better fits the typical data access pattern to the home directory than a parallel file system.</p> <p>The centrally installed applications are also installed on SSD drives. As program files  are relatively small to very small and as packages like Python and R need thousands of  very small files, it is served by a regular Linux file system exported to the compute nodes via NFS as this setup is better at dealing with small files than a parallel file system, certainly on a cluster the size of the one at the University of Antwerp.</p> <p>Next there is a 50TB file system on regular hard drives in a RAID setup with two redundant drives for every 8 drives. It uses a regular Linux file system served through NFS. We made this choice to also have a somewhat larger file system that can deal better with small files than a parallel file system can. It is, e.g., a  good file system for users who want to build their own Python or R installation.</p> <p>The largest file system is a roughly 0.6 PB scratch file system. That uses 105 regular hard drives spread over 7 object servers for the storage of the data, and a number of SSDs for the metadata. The low latency of SSDs is very important to get good performance from the metadata servers, and again a broken SSD won't break the bank as there are only few. But for the object servers hard disks are  used as they are an order of magnitude cheaper than SSDs. </p>"},{"location":"C04_Storage/C04_S04_Revolution/","title":"A storage revolution?","text":""},{"location":"C04_Storage/C04_S04_Revolution/#why-are-flash-based-ssds-not-the-solution","title":"Why are flash-based SSDs (not) the solution?","text":"University of Antwerp-specific <p>The joint bandwidth on the BeeGFS scratch file system at the CalcUA compute service in use in 2022 is on the order of 7-8 GB/s. At the same time, some NVMe SSDs for PCIe 4 also claim to offer a read bandwidth of 7 GB/s and a write bandwidth that is not that much lower, and the even newer PCIe 5 generation can be even faster (at least with the proper I/O  pattern as with a bad I/O pattern bandwidth can be as low as only  100 MB/s).</p> <p>So one could wonder if we shouldn't use 120 of those drives instead.</p> <p>There is of course the cost of the drives. But also a lot more server hardware would be needed simply to connect the drives and also to support the bandwidth over the interconnect. And as SSDs internally also get their speed partially from parallelism, they only come close to their  speed promises with the right file access pattern.</p> <p>The following table shows prices and properties for some drives available in early 2022, with a price update made in September 2023:</p> Seagate Exos X20 Seagate Nytro 3732 Seagate  Nytro 3332 Samsung 980 Pro Samsung 970 EVO Plus Samsung 870 QVO Technology spinning magnetic disks 3D eTLC NAND flash 3D eTLC NAND flash TLC V-NAND flash TLC V-NAND flash QLC V-NAND Flash Market datacenter (SAS) datacenter (SAS) datacenter (2xSAS) prosumer (NVMe) consumer (NVMe) consumer (SATA) Capacity 20 TB 3.2 TB 15.36 TB 2 TB 2 TB 8 TB Read speed 0.28 GB/s 1.1 GB/s 1.05-2.1 GB/s 7 GB/s 3.5 GB/s 0.56 GB/s Write speed 0.28 GB/s 1 GB/s 0.95-1 GB/s 5.1 GB/s 3.3 GB/s 0.53 GB/s Latency 4,16 ms 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? Endurance ? 58.4 PB 28 PB 1.2 PB 1.2 PB 2.88 PB DWPD ? 10 1 0,33 0.33 0.2 (@5 year) Data written/day ? 32 TB/day 15.3 TB/day 0.66 TB/day 0.66 TB/day 1.5 TB/day Time needed 8h50m 4h15m 2m9s 3m20 s 50 m Price 0.025-0.05 \u20ac/GB 0,85 \u20ac/GB 0,31 \u20ac/GB 0.08 \u20ac/GB 0.06 \u20ac/GB 0.04 \u20ac/GB <p>In this table we compare a popular high-quality hard drive for use in the datacenter with several NAND flash based drives, ordered from the highest to the lowest quality measured in durability first and speed second.</p> <p>The Nytro 3732 is a very high endurance drive but only exists in relatively small capacities. It uses a SAS interface which is very popular in servers as it is a good interface to build large disk systems, where the drives are also further away from the CPU or in this case the drive controller. The 3332 is a somewhat similar drive but with much higher capacity but lower endurance. It has two SAS interfaces that can be used to get double the bandwidth. The Samsung 980 Pro is a NVMe drive in M.2-format, meant to be put in a PCIe 4 slot on the motherboard. This is a much faster interface than the one used in the two Nytro drives, which also explains its very high speed. But it is also a less scalable interface as long distance connections would be expensive, as are the switches that would be needed if the CPU does ot provide enough PCIe connections itself. The Samsung 970 EVO Plus is a slightly lower-end drive with a slower interface. All these drives use so-called TCL NAND, which stands for Triple Level Cell NAND, meaning that it stores 3 bits per memory cell.  The last drive, the Samsung 870 QVO differs in two aspects from the other SAMSUNG drives and the datacenter drives: It uses QLC NAND, which stores 4 bits per cell, making it a bit cheaper, but also uses a much slower interface, SATA, which is an older interface that was very popular for hard disks in PCs. It also has a hard disk form factor and is not one that you plug in on the motherboard as the other two Samsung drives.</p> <p>For each of the drive series, we compare the larger capacity SKUs that one can get as after all we are interested in building big storage systems and as they also tend to be relatively cheaper. For SSDs, the larger capacity drives are sometimes also a bit faster than the lower capacity ones in the same series. This is because a flash drive itself is already a highly parallel device using parallelism over multiple banks of flash memory chips to increase the speed. The smaller drives in a given series may not have enough banks of flash memory to fully exploit all the parallelism that the controller chip of the drive (itself a processor more powerful than those in the first smartphones) is capable of using.</p> <p>The main weakness of hard drives and strength of flash drives becomes immediately clean when we look at the rows for (peak) read and write speed and for latency: Hard disks have  much lower peak read and write speeds and a latency that is orders of magnitude higher. Note that we did not find precise latency numbers for the flash drives in the table, but the numbers are a reasonable estimate based on other drives for which the data was available. One should be careful interpreting the write bandwidth. Hard disks become slower for both reading and writing as they fill up partly because of mechanical reasons as  the outer zones of the drive are also used and the read and write heads have to move more, and partly because of a phenomenon known as drive fragmentation, where data that belongs together gets spread all over the disk instead of stored in a single zone of the disk.  There is software to try to correct the latter. But SSDs also get slower the more data is already on them, and this is more pronounced the more bits are stored per memory cell.  SSDs also have another problem: Just as regular hard drives, data is written in blocks. But data cannot be erased or overwritten in single blocks. Before overwriting data, the block has to be erased, but this can only be done in clusters of blocks. Therefore, if a drive is getting full and data is written and rewritten all the time, the drive has to reorganise its storage all the time which can lead to write speeds that are  much lower than on a well-organised hard disk. Some modern hard disks with very high capacity also have a similar problem, but those are only used for archiving while slightly lower capacity drives that don't have this problem are used for more active storage. And another problem is with drives that store multiple bits per cell, which is currently basically all drives. When the drive is rather empty and only one bit needs to be stored per cell, write speeds are much higher than when the drive is filling up and 3 or 4 bits are stored per cell.</p> <p>The table does however clearly show another problem of SSDs: endurance. Unfortunately  endurance for hard disks and SSDs is measured differently so that it is hard to compare in the table. But basically, the Seagate Exos is suitable for near continuous reading and writing and will last for the 5 years of its warranty period. For SSDs the story  is very different. In the early years, an SSD memory cell could be erased and rewritten several thousands of times without failing. However, both the miniaturisation and the use of multiple bits per cell have severely lowered that life span. Some flash memory cell can be erased and rewritten only 150 to 500 times without failing. Drives try to compensate for that by very clever write and erase strategies and by keeping some spare storage on the drive that can be used to replace worn out parts, so that a fixed size can be reported to the operating system. This technique is called wear levelling. This complex management of data on the drive to appear as a regular drive to the operating system is also one of the reasons why flash drives have actually rather powerful processors. Endurance of SSDs is  measured in Drive Writes Per Day, abbreviated DWPD. In the table above, these have all been normalised to a 5-year life span of the drive. Another measure is the amount of data that can be written per day to have a 5 year life span of the drive. It is obvious that the larger the drive, the more data can be written also. We compare this with how long it would take to write that daily capacity to the drive in the (invalid) assumption that we could  write at the maximum write bandwidth. Now we see that the Nytro 3732 is a very good drive. It supports 10 DWPD and even at its relatively small capacity this is still so much data per day  that one could probably write almost continuously to it. It is certainly a drive suitable for a scenario with high write loads, as is the case for supercomputer scratch storage. For the Nytro 3332 which is 1 DWPD it is really only the capacity that saves us. It is slow enough that one can still write a lot of data to it. But if we would use a smaller and cheaper 1 DWPD in, e.g., a compute node, we would probably run into problems as that local drive in the compute node would typically be used to contain a temporary copy of large data sets, and hence  have a lot of data written to it in each job. The Samsung NVMe drives are meant for a totally different access scenario though. They can only sustain 0.33 DWPD, and if you could write that data at the full write speed, this really means that you could only write to them for a few  minutes per day. That high speed is certainly not meant to put a higher write load on the drives. On the contrary, these drives are meant for use in PCs under a typical workload for a PC, where much of the data on the drive is very static and consists of files that may be read often, or are archived for long times. The QVO drive can sustain only 0.2 DWPD, but still ingest quite a lot of data per day due to its high capacity. And due to its slow write speed due  largely to the slow SATA interface, it also looks as if one can write for quite some time to it, but also given how much quad level cells can slow down when filling up, it is really a drive for archiving, and for storing data where the speed of the write operations doesn't really matter.</p> <p>This brings us to the last line of the table, the cost of the drives and here we see the  big problem of SSDs. The cheaper Samsung drives are starting to approach the price level of  the enterprise quality hard drive that we are comparing with, but this is really an apples-and-oranges comparison. Those cheaper SSDs are not really suitable for generic  datacenter use. One could imagine building a storage system with high capacity for high read load scenarios from the QVO drives, and some companies build such storage, but these drives are not suited for the typical load on supercomputer file systems, neither for a local drive or for, e.g., the scratch file system. Some SSDs are suitable (and the Nytro 3732 certainly is) but then the cost is easily 10 times or more higher per byte than for good quality hard drives. This is one of the reasons why SSDs are not used more in supercomputer storage. After all, in supercomputer storage we need storage with a high capacity that can deal with a high write load.</p> <p>Moreover, to benefit from the higher bandwidth that an SSD can deliver, it is not enough to  simply replace hard disks with SSDs. You'll need more and more powerful servers to move the data around between the network interfaces and the drives. Getting the full performance of modern SSDs would require an very expensive storage architecture so in fact any storage system would still be a compromise between bandwidth, capacity and cost.</p> <p>One should also not be blinded by the bandwidth advantage of SSDs. It is true that in  equal circumstances, an SSD will usually be faster than its hard disk cousin for an operation. However, SSDs are also very sensitive to the file access pattern. Some very high bandwidth SSDs actually slow down to 1% of their maximum bandwidth when doing small random serialised accesses (the latter occurring when synchronous I/O from a single thread is used, which is not uncommon in applications that were not written by more experienced programmers).</p> Nice to know <p>Around 2010 a file storage expert at KU Leuven looked at how storage was used on their cluster to determine which type of storage should be bought. It turned out that the load was 90% write and only 10% read, which means that most of the data written to disk was even never read... It is hoped that this text makes clear that writing data is all but free.</p>"},{"location":"C04_Storage/C04_S04_Revolution/#new-memory-types-for-a-revolution","title":"New memory types for a revolution?","text":"<p>As we can see from the previous discussion, flash memory comes with a higher price tag than one would think from looking at prices for drives for PCs, and it has some other issues also, like durability issues as each memory cell has a very short lifespan in terms of number of rewrites, and the unpredictable slow-down especially under a random small writes load when the drive starts to fill up.</p> <p>Several companies have worked on other types of solid state memory for permanent storage that does not have those issues. Two memory types have been considered that would allow byte level access instead of block level, have much better write endurance and that also could be rewritten at the byte level rather than having to erase whole blocks and copying data round in the drive to free such a big block. HPE (then still called HP) and Sandisk explored a type of memory cell called  memristor.  Intel and Micron worked together on a type of memory cell which they called 3D-XPoint,  sometimes marketed by Intel as Optane (though that name was later recycled for other types of storage also). The memristor never made it to market. 3D-XPoint did result in a couple of products, but they were only competitive for some very special markets.</p> <p>3D-XPoint memory appeared in several forms. It was used to build datacenter solid state drives with excellent endurance. Even though the I/O performance wasn't stellar on paper, the fine print in the specs really mattered: 3D-XPoint drives coped much better with read and write loads that lacked enough parallelism, while most flash drives must look at a larger number of  read and write requests simultaneously and reorder them to get close to their quoted speed. They were also used as a cache for flash drives, buffering writes and keeping some data that was read a lot also available as in some cases it could be read faster from the 3D-XPoint cache. And finally, it could also be plugged in some servers instead of RAM. However, it didn't act as  RAM at all as it is still a lot slower than regular RAM, and as its write endurance doesn't come close to that of RAM memory. Instead, it was marketed as memory for large database servers where much of the database could be kept in 3D-XPoint memory yet accessed as if it were RAM, at a lower cost as a fully RAM-equipped system and at a higher reliability in case of, e.g., power problems. The last usage scenario is an example of so-called Storage Class Memory (sometimes abbreviated as SCM): Memory that can operate as (slower) RAM but that just as regular disks storage maintains its state when the system is powered off.</p> <p>However, developing a new memory technology to compete with an already established and very far developed technology is hard and requires extremely deep pockets. In the end, technological evolution created good enough alternatives for many of the use cases of 3D-XPoint memory, or people simply didn't see enough benefits to pay for it, and the development was stopped in 2021 by Micron and 2022 by Intel.</p> <p>High-endurance SSDs are simply much cheaper than 3D-XPoint drives and are a good alternative for all but a few cases where software has a really bad drive access pattern. But then for bigger companies it is probably cheaper to simply rework the software to shine on cheaper drives. The benefit of 3D-XPoint as a cache for SSDs was questionable because of the small size and also because of the way it was implemented, increasing the complexity of the system, and nowadays some drive use some SLC (single bit per cell) flash memory as a cache. The RAM capacity per socket has also increased a lot with more memory channels per socket and larger memory modules. </p> <p>Another technology that would allow larger RAM memories is also coming up.  Compute eXpress Link (\\CXL) is an open standard that build upon the PCIe standards to provide an interface that would be suitable for connecting several kinds of components in large systems: CPUs, GPUs, other accelerators with compute capability, additional RAM memory, ... It also builds on the experience with other technologies, as IBM's OpenCAPI, that tried to reach some of these goals though it is not compatible with any of those. It would even be possible to build networks of CXL connections to build a reconfigurable computer: A user can select the number of CPUs, GPUs, memory blocks, etc., and those are connected on the fly the the CXL fabric. </p> <p>This may sound nice but it remains to be seen how useful this will be in practice. It is not possible to make very large switched fabrics as the latency would simply be too large to use this in a way  current memory and accelerators are used. On the contrary, as we shall also see in  the chapter on accelerators and as we have already seen to some extent in the chapter on memory technology, the needs for many supercomputer applications but also regular applications are exactly the opposite. Large memories are useless if they also come with much higher latency unless applications are reworked to hide the latency and make clever use of the  memory hierarchy with nearby faster RAM and more remote larger but slower RAM. As we shall see, the  performance improvement that one can obtain from using accelerators can also be limited by the data transfers to and from the accelerators. Not all applications can be reworked to cope with the much higher latency in such a CXL-based reconfigurable system or even just an ordinary server with a large bank of  slower memory. In fact, many applications need in fact the opposite, a much closer integration of  memory, CPU and accelerators. This is precisely the reason why, e.g., the Apple M-series processors  sometimes provide much better performance than one would expect from the chip in applications.</p>"},{"location":"C04_Storage/C04_S04_Revolution/#local-storage-in-supercomputer-nodes","title":"Local storage in supercomputer nodes","text":"<p>As the speed difference between the processing capacity of a supercomputer node and the storage keeps increasing, there is a renewed interest in adding local storage again to compute nodes, something that certainly the large supercomputers avoided because of reliability and management issues.</p> <p>Modern high-end SSDs have become fairly reliable and as shapes have mostly standardised, it  does become possible to build them into water cooled nodes without having a negative impact  on the cooling of other components or a performance impact because of a too high temperature.</p> <p>Manufacturers are also working on software to make them more manageable in a supercomputer context and more useful also to parallel programs as those local SSDs cannot be accessed directly from other compute nodes.</p> <p>Intel DAOS was originally developed for the much delayed USA Aurora exascale system where it would work with 3D-XPoint drives in the compute nodes. It is also  designed to integrate with the Lustre file system that will be used on Aurora. It is not clear how Intel envisions using DAOS though as it does rely on storage class memory and not only NVMe drives, and was really designed with 3D-XPoint in mind and its server processors with built-in support for that memory.</p> <p>HPE is working a what they call a  near-node storage system code-named Rabbits for the third USA exascale computer, El Capitan.  It consists of a storage server that sits close to a number of compute nodes with fast dedicated PCIe connection to each of them. The server has its own processor so can work independently from the compute nodes to, e.g., transfer data that was written by the job to the larger remote Lustre file system. Each server has 16 SSDs but also two spares so that it can reconfigure automatically when an SSD fails. These SSDs can be accessed as if they are a directly attached drive, essentially operating as an SSD in the node, or as a network drive acting as a cache to the larger remote Lustre file system. It will work in conjunction with a new scheduler as Slurm cannot easily be made sufficiently aware of the architecture of the attached software to manage it and allocate proper resources.</p>"},{"location":"C04_Storage/C04_S05_To_remember/","title":"To remember from this chapter","text":"<p>Supercomputer file systems, just as their compute capacity, are build from relatively standard but better reliability components. Their speed does not come from novel physics or so but just from clever software.</p> <p>Supercomputers like large files and large reads or writes. Just as with memory, streaming data to and from a file is much faster than random access to data. It may in fact seem that a PC is less sensitive to this but this is only partially true. Even a PC SSD will have much higher bandwidth when streaming sufficiently large amounts of data to it then when using random access inside files and to many files. It is certainly essential to avoid writing many small files. E.g., when running 1000s of small jobs for a parameter study, it would be much better to accumulate the data in  fewer files and/or to use a database to process the results.</p> <p>Opening and closing files is also very expensive on a parallel file system and should be avoided where possible.</p> <p>A good step towards improved use of supercomputer file systems is to use appropriate  data formats. Libraries such as HDF5, netCDF, ADIOS and SIONlib help to organise large amounts of data in a small number of files that work well on supercomputers.  For some compression formats libraries exist that can be used to read data from the compressed archive directly into memory which may already improve performance when\\ a lot of small files would otherwise be read in, but of course it requires adapting the code to use such a library rather than the functions calls to read from the  file system. You can think of such libraries as creating a hierarchy to manage the complexity:  the file system manages datasets as a single file for each dataset, and the file library creates a kind of a file system in the application to store the different  data objects that compose the dataset, also in a format that can be optimised for that specific dataset.</p> <p>Another remark that we have not yet really discussed is that it is not a good idea to read or write numeric data in text (ASCII or UTF) format. There is hardly a portability advantage to using text to store numbers as binary data formats have mostly been standardised with really two options remaining and many libraries and tools supporting both (little endian and big endian which defines the byte ordering for multi-byte numbers, but almost all CPUs nowadays use little endian). Storing numbers as ASCII text expands the needed capacity with a factor of approximately three and the conversion to and from binary data is very slow.</p> <p>When discussing scaling a storage system from the size of a PC storage system to the size of a  supercomputer storage system, one should realise:</p> <ul> <li> <p>Scaling capacity is cheap. Often one only needs to add disks and disk enclosures and not so     much servers when using drives with interfaces that are specifically made for that (and in     particular SAS drives as we have seen in the drive technology comparison table).</p> </li> <li> <p>Scaling bandwidth is harder and more expensive. Simply adding disks is not enough as we need     more bandwidth between the disks and the servers and more bandwidth from the server to      the interconnect. As both are limited, we will need to add more servers.</p> <p>Joint bandwidth to a number of files is also easier to scale (at least if those files are spread over the whole system and a sufficient number of requests come in in parallel). The strength of many supercomputers is that they also offer extremely scalable bandwidth to large files. But this also requires large parallel applications using a properly written parallel I/O framework.</p> </li> <li> <p>Scaling the number of I/O operations that the system can process per second (called IOPS) is     extremely hard and expensive and is sometimes even physically impossible. This has several causes.</p> <ul> <li> <p>Metadata access is much harder to parallelise, especially for access to a single (sub)directory by a single user.</p> </li> <li> <p>Due to the larger physical and logical distance of the application to the storage, the latency     of I/O operations will be one or more magnitudes higher. This also means that if the I/O operations     are launched sequentially and one has to finish before the next one can start, the application will     run a lot slower than on a PC with fast local storage.</p> <p>In fact, it is doable to make supercomputer storage with very high IOPS when there are a lot of  parallel I/O requests, from many users to files all over the file system so that metadata and data access can be spread over many servers. But it is not possible to improve IOPS for a single threaded application with synchronous file access.</p> </li> </ul> </li> </ul>"},{"location":"C04_Storage/C04_S06_Further_reading/","title":"Further reading","text":""},{"location":"C04_Storage/C04_S06_Further_reading/#scientific-data-file-format-libraries","title":"Scientific data file format libraries","text":"<ul> <li>HDF5,</li> <li>netCDF,</li> <li>SIONlib</li> <li>ADIOS<ul> <li>ADIOS2 documentation </li> </ul> </li> </ul>"},{"location":"C04_Storage/C04_S06_Further_reading/#related-libraries","title":"Related libraries","text":"<ul> <li>openPMD C++ and Python API library for implementing     the openPMD standard for meta data and     naming schemes for particle-mesh data files, useable with several backends.</li> </ul>"},{"location":"C05_Summary1/","title":"Putting it all together","text":"<ul> <li>Scaling</li> <li>Dennard scaling</li> <li>Transistor cost</li> <li>Three keywords: Streaming, Parallelism, and Hierarchy</li> <li>Andy and Bill's law</li> <li>Software first, not hardware</li> <li>Building up the supercomputer</li> </ul>"},{"location":"C05_Summary1/C05_S01_Scaling/","title":"Scaling of technology","text":"<p>In this chapter we summarise the previous three chapters on processor, memory and storage technology. We also want to stress that HPC stands for High Performance Computing and that a supercomputer is not  like a very High-end Personal Computer.</p>"},{"location":"C05_Summary1/C05_S01_Scaling/#scaling","title":"Scaling","text":"<p>The performance of a computer cannot be understood from a single parameter. Instead many parameters characterise the performance of computer. The clock speed of a CPU is only one of those parameters. There are also many lantencies that need to be taken into account: memory latencies at the various levels of the memory hierarchy and of storage, communication latency, but if you would study into more detail than is possible in these lecture notes how to obtain maximal performance from a computer, also latencies in instruction execution. And there is also the bandwidth of several components that has to be taken into account: bandwidth to the various levels of the memory hierarchy and to storage, bandwidth between CPUs in a shared memory system and bandwidth of the interconnect. And the number of instructions a CPU can execute simultaneously is also a kind of bandwidth. When you're interested in solving big problems, you're also interested in the capacity  of memory and storage.</p> <p>Not all these parameters are as cheap to scale, or improve over time at the same rate. As we will also discuss a bit further in this chapter, physical limitations have put a bound to improvements in CPU clock speed and latencies. The finite speed of light (30 cm/ns in vacuum and roughly 20 cm/ns in glasfiber) and speed of signals in copper wires is just one of those limitations. The growth of the bandwidth of memory, disks and network connections tends to be slower than the growth of the theoretical peak performance of a computer system.</p> <p>As a result of these restrictions, it is simply not possible to build a supercomputer were all these parameters would be, e.g., 100 times better than in your PC or smartphone so that your PC software would simply run and run 100x faster.  In fact, the opposite may be true. For some applications a High-end PC is unbeatable because of  its compact size and (though less so nowadays) thin software layer as it is a personal device, as this guarantees minimal latencies.  As we have seen in several examples, \"bigger\" often means higher latencies that need to be hidden. E.g., a bigger memory has to sit physically further from the CPU so access will be slower.  A bigger disk system needs a different way of managing it then the SSD that sits just next to the processor in your PC and will be slower. We no longer build processors cores out of multiple chips, not only because it is not really needed anymore, but if we did the clock speed would be low as signals would travel too slowly to the other end of the processor core.</p>"},{"location":"C05_Summary1/C05_S02_Dennard_scaling/","title":"Dennard scaling","text":"<p>For a long time, with every new generation of chip technology, roughly every two years, linear dimensions decreased by 30% (x0.7), and surface dimensions decreased by 50% (x0.7<sup>2</sup>), i.e., transistor density doubled. Moreover the power density remained practically the same as voltage and currents needed to drive the circuits also lowered proportional to linear dimensions. Circuit delays went down by 30% (x0.7), so frequencies went up by 40% (0.7<sup>-1</sup>).</p> <p>Though the first cracks already appeared some generations earlier (see, e.g., the problems with the Prescott Pentium 4 variant from 2004, built  on a 90\u00a0nm process), this really broke down around 2006.  No longer did all dimensions of elements on an integrated circuit scale as well and hence transistor density did not grow as fast anymore. Moreover, the threshold voltage of semiconductors, the minimum voltage to let them switch, became more relevant and put a lower boundary on how much voltages can be reduced. Another element in the power consumption, leakage power, became more and more important if not dominant.  And capacitances and inductances are such that the clock frequencies don't go up as fast anymore either. </p> <p>As a result of the breakdown of Dennard scaling, chips have become very hot and power consumption of supercomputers has become a major concern. Moreover, there is hardly any further speed increase anymore just from further  reducing the component size, and in fact chip designers usually need to chose between a slight speed increase and a slight lowering of the power consumption per transistor. As a result of this designers need to look much harder for architectural improvements than before for further speed increases. The breakdown of dennard scaling is also part of the reason why latencies of various components and subsystems do no longer improve much.</p> <p>Transferring data has become the major source of power consumption in computers, more than doing the actual computations. Nowadays it takes more power to transfer two numbers from  one end of a chip to the other end than do a simple arithmetic operation with those numbers.</p> <p>PCs already operate their hardware in the domain of Dennard scaling breakdown so there is no  hope that one can design a single processor core that is much faster than the one in a PC in the current state of technology.</p>"},{"location":"C05_Summary1/C05_S03_Cost_transistor/","title":"Cost of transistors","text":"<p>The cost per transistor may not be the ideal number to compare chip process generations as chips are actually paid per wafer. And the cost per useful transistor on a wafer will also depend on the die size, how tolerant individual chips on the wafer are to defects, and the defect rate. It is however an easy number to work with.</p> <p>The following graph is based on numbers found in a report of the Marvell Technology Inc 2020 Investor Day.  (Marvell Technology Inc is a fabless semiconductor company.)</p> <p></p> <p>For a long time the cost per gate (or for that reason, transistor) decreased rapidly with each  new process generation. In fact, the cost per wafer did not increase much while the gate density doubled with each new process generation. This ended with the 28nm generation, From the 20nm generation on (which TSMC launched in 2014) the cost per gate did not longer decrease. Instead, the price per wafer grew  faster with every new generation than the transistor density increased. This is also the moment that gates on a chip could no longer be made as flat 2D structures, but 3D structures were needed instead to control leakage, requiring more and more process steps as the miniaturisation further progressed.</p> <p>Nowadays processors and computers still become faster with every generation though not at as fast a rate as before (see the breakdown of Dennard scaling), but the cost starts to rise again. It is no coincidence that every new generation of high-end graphics cards or almost every new generation of high-end smartphones is more  expensive than the previous one. The cost of high end processors for servers and supercomputers is also exploding the last couple of years. And the price per flop is hardly going down anymore. The only way this can still improve is by  architectural innovations that make processors more efficient so that more work can be done per transistor.</p> <p>This also implies that we cannot expect a drastic growth in supercomputer performance in the near future without a matching growth of budgets. </p> <p>It also implies that the only way to get more research results on supercomputers without growth in funding is paying more attention to the quality of the software that is used and the way the supercomputer is used.</p>"},{"location":"C05_Summary1/C05_S04_Keywords/","title":"Three keywords: Parallelism, hierarchy, and streaming","text":"<p>There are three keywords in programming for supercomputers: parallelism, hierarchy, and streaming, and we will now discuss each of them separately.</p>"},{"location":"C05_Summary1/C05_S04_Keywords/#parallelism","title":"Parallelism","text":"<p>Parallelism is important at all levels in supercomputers. We have thoroughly discussed the 4 levels of parallelism in a the processing units in a supercomputer: Instruction Level Parallelism and vectorisation (and in fact, recently matrix computations) in the core, the use of multiple cores in shared memory setup, and distributed memory parallel computing. We've also seen that parallelism is important in the design of storage systems for supercomputers, with even specialised shared storage systems. However, as we have mentioned, in fact all but the most basic SSDs in PCs and smartphones also rely on parallelism to reach the high bandwidth  and high number of IO operations per second they promise to be capable of. We haven't discussed main memory in that much detail, but in fact modern computer memory also relies on parallelism. modern processor packages offer multiple memory controllers that work in parallel, and usually each memory controller already controls more than one memory channel. And even within a modern memory module there is already a level of parallelism.</p> <p>Most parallelism is not automatic. Instruction level parallelism does not require much work from a user though there are some programming techniques that improve and others that harm instruction level parallelism. Exploiting vector computation already requires more work from the programmer as we shall see in the following chapter, and matrix units are even harder to exploit. Shared and distributed memory parallelism almost always requires a significant effort from the programmer. Both can of course be exploited by simply running multiple binaries in a capacity computing context but that does not improve the time-to-solution for a single problem. Similarly, good performance of central storage is all but a given and requires careful thinking about how data is stored. Some storage systems may be more forgiving than other storage systems, but even on an SSD in a PC a bad data access pattern will leave a lot of potential of the storage system unused.</p> <p>This is also not a new lesson. Supercomputers have relied on forms of parallelism that require help from the programmer since the '70s. In PCs some form of vector computing returned in the '90s and became essential for floating point performance with the Pentium 4 processor in 2002.  The first regular PCs with more than one core appeared already in 2006, and nowadays 4 cores or more are common in even laptops and the cheapest smartphones.</p> <p>In PCs single thread performance still improves with every generation but at the expense of steeply rising power consumption. In servers and supercomputers the single thread performance has been  stagnating for several years already as they are more optimised for performance per Watt. Even though every new generation of core claims to do more work per clock cycle (better instruction level parallelism),  the clock speed is often also lowered so that the net gain is practically zero.</p> <p>Just as streaming is almost as important on PCs as on supercomputers, the same also holds for parallelism.  There is only one level of parallelism that is specific to supercomputers: distributed memory parallelism. And at the file system levels PCs also tend to be a lot more forgiving for bad access patterns, even though you will still be exploiting only 10 percent or less of the potential of your storage.</p>"},{"location":"C05_Summary1/C05_S04_Keywords/#hierarchy","title":"Hierarchy","text":"<p>Hierarchies appear in different ways in supercomputers, but also more and more in regular PCs.</p> <p>Memory is organised in a hierarchical way with typically 3 levels of cache where the first two levels tend to be organised per core while the third level is a cache that is shared by multiple if not all cores, and then often two or more levels of RAM memory. Many supercomputers are built out of building blocks with two processor sockets and accessing memory that is physically attached to the other socket is slower than accessing local main memory, but even within a socket not all memory might be equal. The latter is already the case on the AMD Epyc server processors and will also happen on some versions of the Intel Sapphire Rapids series that became available in early 2023. PCs also have the same cache hierarchy, but the main memory has only one level unless you opt for some workstation-class systems that are really built using variants of processors for servers.</p> <p>There is also a hierarchy in the levels of parallelism for processing. Instruction level parallelism and vectorisation is parallelism at a very fine scale as it is done in the instruction stream itself and does not require any special communication or synchronisation. Shared memory parallelism is  the next level in the hierarchy. It comes with a startup cost and cost to synchronise the threads for some operations. It requires bigger chunks of work to be executed in parallel before the gain from parallelism offsets the startup and synchronisation costs. Distributed memory parallelism tends to be even coarser as the startup cost is higher and the communication between the processes is a lot more complicated then the communication between threads in a shared memory program.</p> <p>Discussing the hardware architecture and software parallelism models of GPUs in much detail is  outside the scope of this two-lecture introduction. However, both the hardware and the lower-level programming models for GPUs are very hierarchical.</p> <p>We can expect that parallel storage will also become more hierarchical than it is today as  supercomputer manufacturers are looking for ways to bring some storage again closer to the processing elements without losing too much of the manageability, and as flash storage will remain too expensive in the foreseeable future to build an all flash file system and hence can only be used for an extra level in the hierarchy.</p> <p>Exploiting the memory hierarchy is extremely important for performance as will also be illustrated later in this tutorial. Mapping threads and processes in shared and distributed memory parallel computing on that hierarchy is for many codes also important for performance. One may of course wish that it would be different, but in practice some understanding of the hardware architecture is needed even if you are only using parallel computers and not programming them.</p>"},{"location":"C05_Summary1/C05_S04_Keywords/#streaming","title":"Streaming","text":"<p>We have already mentioned several problems with data access on computers, not only supercomputers.</p> <ul> <li>Data access requires a lot of power. The further the data is from the processing units, the     more power it costs to get the data to the processing unit. But unfortunately we can only     store a limited amount of data really close to a processing unit.</li> <li>The further the memory is from the processing element, the higher the latency to get it      there to process which was the main reason why a memory hierarchy was created and      cache memory was introduced (as it was introduced     long before the power associated with data transport became a problem).</li> </ul> <p>Hence getting the data flowing smoothly through the memory hierarchy in the computer,  all the way from permanent storage to processing, is key to performance. An important way to deal with latency is the use of caches. Some are fully managed in hardware, like the caches between CPU and RAM memory, while others are managed in software, e.g., file systems also have a caching mechanism. It is important to ensure that those caches can work efficiently, and that requires:</p> <ul> <li>predictable data accesses, so that prefetching mechanisms can fetch data into the cache     before it is needed so that part of the latency can be hidden, and</li> <li>data accesses in sufficiently large chunks to avoid that data in caches is never used and     hence wasted and so that the effective bandwidth is not too much reduced by latency.     If you'd fetch individual 4-byte numbers from RAM memory and if the memory latency would     be 100ns and if there would be no way to have those data accesses overlap as you cannot     predict what the next piece of data would be, then you effectively get at most a bandwidth of     4 bytes / 100ns / 1024^3 = 0,037 GByte/s which is only a fraction of the bandwidth      one can get from RAM memory. </li> </ul> <p>Random access to small blocks of data is bad at all levels in computers, not only supercomputers.  Only the definition of \"small\" varies depending on the type of memory or storage. For RAM memory \"small\" is on the order of a cache line or 64 bytes on many popular CPUs, for permanent storage \"small\" is measured in kilobytes or even 10s or 100s of kilobytes for shared parallel file systems. We haven't discussed the architecture of main memory in much detail, but there also streaming is  important to get the fastest performance as internally memory is also accessed and buffered in larger blocks than a single request, and a subsequent request to data that is already in that  larger buffer will be quicker than an access to data that is not.</p> <p>This is also not a new message. Some level of streaming has been important in supercomputers ever since the 70s, and when it comes to permanent storage it has been important on PCs ever since the  first PCs were build. One may have the impression that it has become less important with modern flash memory based SSDs, but that is only partly true. The latency is extremely small compared to any storage device that uses mechanically moving parts, but even then if you only access data through small files with size in the order of kilobytes so that the caching mechanisms in file systems cannot work, you will only reach a fraction of the theoretical bandwidth of those  devices, and this again becomes more and more pronounced with every new generation as the  peak bandwidth of SSDs improves quickly while the latency stays about the same. </p>"},{"location":"C05_Summary1/C05_S05_Andy_Bill/","title":"Andy and Bill's law","text":"<p>What Andy giveth, Bill taketh away</p> <p>The law originates from a humorous one-liner told in the 1990s during computing conferences. Andy in this law is Andy Grove, CEO of Intel and later Chairman of the board between 1987 and 2004.  In those days Dennard scaling and the rapid evolution of semiconductor technology made a rapid  growth of processing power for PCs possible and the single chip designs from Intel were quickly catching up with the much more expensive often multi-chip designs for workstation and server processors. Bill in this law is Bill Gates, founder of Micorosft and its CEO and Chairman between 1975 and 2000. Especially in the '90s, when GUIs became popular, PC software rapidly expanded and always managed to use all available processing power, sometimes not feeling any faster than the previous version of a package on a previous generation of hardware. With some packages it felt as if you didn't really get that much more work done quickly even though your hardware became faster and faster. However, beyond this law is also a frustration of Andy Grove who felt that Bill Gates wasn't always making proper use of new performance-enhancing features of the new processors and hence not using the new hardware to the full potential. Intel introduced the 80286 in 1982 and it was first used by IBM in the IBM AT in 1984, but it took until 1987 before an OS appeared that was aimed at regular PC users and fully  exploited the 80286, with OS/2, which was in fact mostly built by IBM. Though Microsoft should not be entirely to blame for this as it was not possible to use the extended  mode (actually called protected mode) while keeping compatibility with older DOS software also, except via an  undocumented trick (which is what OS/2 used).  The 80386 launched in 1985 and was the first processor of Intel that offered a 32-bit instruction set, and it  was also designed to run old 16-bit DOS programs well together with 32-bit software.  Though there were some Unix variants that supported the CPU in 32-bit mode and  a 32-bit version of OS/2 in early 1992, it took Microsoft until Windows NT 3.1 in  July 1993 to come with an OS for typical PC use that fully exploited the 32-bit  features of the 80386 (by then succeeded by the 80486 and Pentium).</p> <p>It used to be common practice in much of the scientific computing community to  ridicule Microsoft and to claim that UNIX and UNIX-derived operating systems are superior. A large part of the scientific computing community isn't doing any better though and we can think of several laws equivalent to Andy and Bill's law that  apply to scientific computing.</p> <p>What Andy giveth, Cleve taketh away</p> <p>where Cleve is Cleve Moler who really started the development of efficient linear algebra libraries such as LINPACK and EISPACK, predecessors to LAPACK, but then also developed MATLAB as a user-friendly environment to experiment with those libraries and in 1984 was one of the founders of MathWorks, the company that  went on to develop Matlab into what it is today. Matlab evolved into an  excellent system to prototype numerical algorithms. However, its language is not nearly as efficient as traditional programming languages when it comes to execution efficiency and hence is a good way to slow down modern hardware.</p> <p>What Andy giveth, James taketh away</p> <p>where James is James Gosling, the main developer of the Java programming language. Java, and many other programming languages from that area, may have good ideas to  improve programmer productivity or make it easier to run code on multiple machines, but this also came at a cost of performance. The first versions of the Java virtual  machine were just slow, and even later versions based on a just-in-time compiler are not that spectacular. Designers of just-in-time compilers have long promised us better performance than regular compilers as they can use runtime information to further improve the generated code, but the reality is that gathering that information and using it properly to improve the generated code is too expensive and cumbersome.  Abstracting away too much of the memory system is also not a good idea as making proper use of the memory hierarchy is essential for performance and one cannot expect compilers to be able to do the necessary code transformations on their own. Integrating with code written in other programming languages that make it easier to write high-performance library routines is also very cumbersome. And getting garbage collection to work well in a distributed memory context also requires build-in support in the virtual machines for this type of parallelism and cannot be done via a simple library add-on (there has been an effort do do MPI for Java but that didn't work well because of this). Granted not everything about Java is bad though. The language did support concurrency in the base language and was hence ready for shared memory execution. And in theory the just-in-time compiler concept also allows to quickly adapt to new processor architectures, if it were not that the language lacked the features to  enable to compiler to easily vectorise code, one of the features that influences  performance of modern processors most on scientific code.</p> <p>At some point Java gained some popularity in scientific computing basically because it became the first programming language taught at many universities and hence the one that beginning researchers were most familiar with, but it is mostly given here as an example of languages that try to abstract away to much of the underlying system  architecture and hence tend to run with less than optimal efficiency.</p> <p>What Andy giveth, Guido taketh away</p> <p>where Guido is Guido van Rossum, the original developer of the Python scripting language (back in 1989 already). Python for a long time was a scripting language only known by system administrators and the like, but from 2005 on, with the advent of NumPy, became more and more popular in scientific computing. The Python ecosystem exploded  with lots of small and often badly tested packages, and the language designers got in  the habit to break code with every minor release every 18 months. Moreover, the  language is usually interpreted and the interpreter is extremely inefficient on much code. In fact, the Python designers aren't really to blame for this as the  language was developed with a completely different purpose in mind and did a good job at that for a very long time. There have been several efforts to develop just-in-time compilers (and an ahead-of-time compiler to C)  for Python, but as of today there is still no JIT that does well on most Python code, and several companies that invested in the development of one have given up, though  all compilers will probably come with examples where they offer a 100x or 1000x speed increase over naively written pure Python code for small specific fragments. Cython is an example of an ahead-of-time compiler that needs some help as regular  Python code doesn't really offer enough type information to generate efficient code.  Numba and PyPy are two examples of just-in-time compilers where Numba seems to do best with code that heavily uses NumPy data structures while PyPy works better on non-NumPy code.</p> <p>Never mind that we also tend install Python and its packages from repositories that often only contain generic binaries compiled to run on as large a range of hardware as possible rather than binaries that exploit specific features of each processor to optimise performance.</p>"},{"location":"C05_Summary1/C05_S05_Andy_Bill/#but-heres-the-problem","title":"But here's the problem...","text":"<p>Of course it is easy to write bad performing code in any programming language, but the point is that there are languages where it is near impossible or even just impossible to write truly efficient code. One used to get away with that in the days that the performance-for-money ratio improved a lot with every new generation of hardware. But as we have discussed, these days are over for now and it may be a long time before they return. For now, progress will have to come from better, more efficient software that better exploits the features of current hardware, and we need computer languages that support that. We need computer languages that give the programmer sufficient control over data storage and data flows and hence help to exploit the hierarchy in memory in the node, and languages where parallelism is not an add-on but intrinsic to the language to make parallel programming less difficult.</p> <p>Before the '90s, computers were so expensive and so limited in what they could do  compared to even a basic PC today that researchers had to pay a lot of attention to  both good algorithms and a good implementation of those algorithms. After that, the attention to algorithms hasn't been that strong in all science domains, and the attention to a proper implementation has been poor, especially in the newer science domains. In some fields of scientific computing performance improvements have come as much from better numerical methods as from faster computers.</p> <p>Given the slow-down of performance growth of hardware at a constant budget, it is clear that performance growth will have to come from better algorithms and better software.</p>"},{"location":"C05_Summary1/C05_S05_Andy_Bill/#and-a-non-solution","title":"And a non-solution...","text":"<p>There may be a lot of discussion today about how quantum computers or computers with optical components will solve all problems. This is just a discussion by the hopeful. The reality today is that the quantum computer is still looking for an application in which it will excel, and currently needs to be attached to a rather powerful traditional computer also  simply to turn the information that it produces in something useful. The cooling needed for a quantum computer is also extremely expensive as it needs to be cooled to  nearly the absolute zero. Every fraction of a Kelvin above the absolute zero makes the results worse because of noise (which manifests itself as errors in the computation). Don't be fooled by the pictures you see of quantum computers: Most of these pictures don't even show the quantum computer nor all the hardware that is needed to measure the quantum state. The most popular pictures tend to show the nice-looking multistage cooling system. The same holds for optical computers. The reality there is that we are still very far from  integrated optical circuits, let alone ones that are powerful enough to do computations quicker than our current computers. The amount of money needed to develop that technology into something useful may turn out to be prohibitive unless we really hit barriers very hard.</p> <p>In fact, we have seen this happening before. As we have discussed, flash memory has serious problems. Longevity is a big problem. And furthermore it is still a block-oriented medium  limiting the ways in which it can be used. There were two promising technologies to replace it over time: the memristor and phase change memory. A form of the latter was brought to market as 3D XPoint by Micron and Intel in 2017. However, it was expensive compared to flash memory, partly also because there was a huge development cost that needed to be paid back with the relatively low initial volumes, and it would still have required a lot of funding to become truly price-volume competitive with flash memory. The technology was abandoned in 2022 because of that. </p> <p>Moreover, those new computer architectures (and certainly quantum computers) will also require to rethink algorithms and implementations. So why should we put our hope on those new architectures if today we already refuse to think about algorithms and implementations?</p>"},{"location":"C05_Summary1/C05_S06_Software_not_hardware/","title":"Software first, not hardware","text":"<p>It is the software more than the hardware that defines a supercomputer.</p> <p>In the early 60s computers were still mostly build from individual transistors. There were already smaller slower and bigger faster computers, but architectures differed vastly. Powers of two for number of bits, or binary number representations were not yet standard. The first programming languages, Fortran for technical computing and Cobol for business computing, already existed.</p> <p>In the late 60s and early 70s computer were build from 100s or 1000s of small integrated circuits (though some already appeared earlier). In this era we saw smaller minicomputers and large mainframes. The minicomputers were  more scaled-down versions of the mainframes.</p> <p>By the second half of the 70s a very specific supercomputer architecture that was not very well suited for general computing appeared on the market: The vector computers built by Cray and CDC. Specialised hardware meant that software also had to be adapted: The compilers did need some assistance to generate proper vector code, and the first numeric libraries that helped exploit the vector architecture also appeared. </p> <p>By the second half of the 80s the next big paradigm shift showed up. It had become  prohibitively expensive to design supercomputer hardware. Vector machines were still fairly popular, but keeping developing them had become very expensive, while a market for smaller workstations that offered decent performance for the time appeared.  Hence research labs and Intel started to experiment with building supercomputers that would reuse some of that workstation hardware. The first computer of that kind was probably the Cosmic Cube developed at Caltech which was based not even on a workstation processor but the two-chip 8086/8087 combo that was also used in personal computers. 64 of those were linked together using a special-purpose network. This research later led to the Intel iPSC product line of supercomputer built out of regular 80386 and later the Intel i860 RISC processors. Other manufacturers also picked up the idea, e.g., IBM with the SP line using processors developed for their POWER workstations in 1993 (IBM also build a vector computer in the late 80s). By 1995 distributed memory  computers based on processors designed for workstations or PCs already took a lot of the top spots in the Top 500 list, a list of fastest supercomputers on the  Linpack benchmark. Supercomputer manufacturers differentiated more in their designs for the interconnect and in the software than in the processor hardware as the latter was shared with less capable workstations or even came from another manufacturer. </p> <p>This trend only became even more pronounced from the late 90s on. These are the days when Linux started to become more popular among researchers and when Intel processors for PCs had fully caught up with typical workstation processors in terms of performance, gradually pushing the latter out of the market as the growing design costs had to be amortised on a too low volumes.  Modern supercomputers try to minimise the hardware cost by reusing technologies  that have other larger volume applications also and in some cases even reusing  more volume hardware. Even custom network technologies, for a long time the feature that vendors used to distinguish their supercomputers from more regular computers,  have largely disappeared from the market in favour of InfiniBand, a network technology originally designed for other applications in the data centre. Cray and Fujitsu are notable exceptions, both still investing in their home-grown interconnect technologies and still doing so today, though Cray has been bought by HPE. One can argue though that the most recent Cray network technology, called SlingShot, is largely derived from Ethernet with some customisations that  are in fact also partly software. More than ever before does the system and application  software make the supercomputer: Programming models implemented in libraries and compilers to deal with distributed computing, parallel file systems to turn huge arrays of disks and servers into a high bandwidth file system capable of efficiently serving multi-terabyte data files to applications, applications that exploit all levels of parallelism and the hierarchical structure of modern supercomputers, ...  In fact, without all that software a high-end gaming PC could very well be faster for  your application than a supercomputer!</p> <p>This evolution is only normal. Designing hardware is expensive and hardware also needs  a lot of testing before it can be brought to market as many errors are hard to correct afterwards. Moreover there is also a high production cost associated with hardware, and the cost is higher for hardware that can only be used in small volumes.  Software may not be cheap to design either, but it is easier to bring to market as it is easy to correct errors afterwards, and the cost of distributing it is low compared to the costs for hardware. As it is easy to continue improving software it is possible to upgrade a system and make it better during its lifetime.</p> <p>It is also largely the software that ensures that a supercomputer is still a different infrastructure from a server farm or a cloud infrastructure (though there is one element, the interconnect, that  still remains very important and tends to differ from those infrastructures also). Supercomputers focus on latency and staying close to \"bare metal\" to be able to get the maximum performance out of the hardware and to enable thousands of processors to work  together on solving big problems that cannot be split up in hundreds of almost independent tasks that can be executed in parallel. Supercomputers focus on scalability for capability applications and everything that compromises that scalability is not implemented. Cloud infrastructures on the other hand focus more on isolating even small users from one another, security and the creation of a personal  environment. They are typically only suited for fairly coarse grained parallelism and  applications where different tasks that can be executed in parallel are fairly independent of each other (think of the thousands of requests coming in on web servers). Supercomputers and cloud infrastructures have also very different exploitation models. That partly results from the different type of environment they intend to offer, but is also  partly the result of the fact that many supercomputers are installed in supercomputer centres that serve mostly academia or in academic institutions themselves, while the largest cloud infrastructures are commercial. Users think completely differently about resource use if they get an allocation upfront and don't see the bill then when they actually have to pay full cost for resources at a commercial cloud infrastructure.  Supercomputers tend to focus on a rapid succession of jobs of different users, relying mostly on shared storage as managing bare-metal node based storage in a multi-user environment can be hard. Modern Linux may offer some low-overhead solutions to ease some of those problems, but for some reason they have not become popular yet on supercomputers. The focus on rapid succession of jobs is only normal on a cluster where compute time is not billed in terms of money as there is not enough incentive for a user to carefully consider if it makes sense to keep the same resources unused for a short while to save on some other startup cost instead. This is different on a cloud infrastructure where there is a financial incentive to think economically and where it is not  uncommon to keep some servers a bit longer to, e.g., save on the data transport cost to bring data from a remote data store service to a local (virtualised) disk. The full virtualisation  that is often used also makes it a lot easier for the system to clean up afterwards, making it easier to offer local storage in a manageable way (and cloud servers tend to be larger also which also eases offering local storage, while supercomputers are built as compact as possible for better internode communication performance and to be able to use cheaper cabling).</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/","title":"Building up the supercomputer","text":""},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#the-uantwerpen-tier-2-clusters","title":"The UAntwerpen Tier-2 clusters","text":""},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#the-admin-nodes","title":"The admin nodes","text":"<p>The admin nodes that run all cluster management software and the login nodes of a cluster are nothing special as the next picture shows.</p> <p></p> <p>These are standard servers of a format known as \"2U\", just under 50cm wide and 9cm high. There is a lot of space for disk drives in the front, but this is not where the storage for the cluster is build. These disk slots are mostly empty and only used for the disk space that each of the servers need themselves.</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#the-storage-nodes","title":"The storage nodes","text":"<p>The storage nodes of the UAntwerpen cluster are currently also build from fairly regular hardware components.</p> <p></p> <p>The top part of the rack houses two regular servers similar to the ones for the  admin and login nodes. These servers serve the user file system (home directories), the applications file system and a 50TB hard disk based file system (called the data file system) that is also exported to other clusters in the VSC consortium and is also meant for some  tasks that a parallel file system is not good at. The drives however are not in those servers, but in the two boxes just underneath. These are a high-quality and fairly expensive storage system. The larger main box and the smaller box underneath house the SSDs for the user and application file systems and the hard disks for the 50 TB data file system. It actually also contains the SSDs that are used for the metadata of the parallel file system.</p> <p>The lower part contains again two servers. These servers run the parallel file system in virtual machines. Both the metadata and object servers run on those servers. The drives for the metadata are in the storage system in the upper half. The hard drives for the object servers are in the storage enclosers just above and just below the servers. Each box can contain 4 groups of 15 hard disks.</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#the-compute-nodes","title":"The compute nodes","text":"<p>The regular compute nodes of the UAntwerpen clusters are spread over several racks. The picture below shows 3 racks of compute nodes from the cluster called \"Leibniz\" which was installed in 2017 and will be decommisioned in 2024, and some leftover nodes from the even older compute cluster Hopper.</p> <p></p> <p>The nodes of Leibniz are grouped in groups of 24 compute nodes. Each group consists of 6 enclosures that each have the same width and height as the login and admin nodes, but now house 4 2-socket servers. These are basically trays that half half the height and half the width of the enclosure. At the fron there is again room for disks, 12 per enclosure so 3 per node, but these are largely empty. In fact, filling them up completely would even be bad for the cooling of the node as cold air is sucked in from the front and leaves again at the rear side.</p> <p>Above each group of 24 compute nodes there is a swich with 40 ports. 24 of those ports connect to the compute nodes, while the other 16 go to a series of switches mounted at the top that  connect the switches with each other and with the storage and admin nodes of the cluster.  This network architecture is called a tree.</p> <p>The picture below shows a compute node from an even older cluster, hopper. It is a compute node from the section in the lower left of the previous picture.</p> <p></p> <p>We see 2 CPUs with a passive cooler on them (so just a heatsink), some memory (in this case  2 modules on each side of each CPU), and a small add-in card with the interconnect at the front. The photo shows two hard drives at the rear end, though Hopper only had one.</p> <p>In this case, an enclosure contains 8 nodes and is twice the height of an enclosure of Leibniz. Powerful fans suck in air from the front and spit it out at the rear. The air flow through a node is so well controlled that the CPUs themselves don't need fans, contrary to what one  usually sees in PCs. The problem with PCs is that they are made to be very extensible and  are also often in cases that are not tuned to the precise parts in there, making it impossible to control the air flow enough to cool the CPU without giving the CPU its own fan.  Having the fans in the back is good for reliability though. On a system that large with that many  fans, you can be sure that some will fail during the life span of the cluster, and having them at the back makes it very easy to replace them. A bad element of this design though is that the hard disk is at the hot side of the node. With temperatures as high as in modern compute nodes  and storage that is very sensitive to temperatures and often cannot handle temperatures above 30 or 35  degrees well, this would no longer be possible.</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#the-first-vsc-tier-1-cluster","title":"The first VSC Tier-1 cluster","text":"<p>The picture below shows the outside of the first Tier-1 cluster of the VSC consortium, which was installed in a then branch new data centre at the University of Ghent in 2012. It uses the same architecture as the Hopper cluster mentioned above and is build out of groups of 24 compute nodes in 3 enclosures of 8 nodes as shown above.</p> <p></p> <p>The picture also shows some coolers. They connect to a cold water circuit in the compute room. Hot air is sucked in from the rear and blown out in the front, while the compute nodes suck in cold air at the front and push it out again at the rear.</p> <p>To make this efficient two rows of racks with opposite orientation are put next to one another, and the space in between is closed off with a ceiling and doors, forming a hot aisle between the two rows. This is shown in the picture below:</p> <p></p> <p>We can again distinguish the coolers, and the racks in between have perforated doors at the inside. Depending on the cluster, the temperature inside that hot aisle can be as high as 34 or 40 degrees, while the temperature in the data centre itself is kept at 20 to 25 degrees with that generation of hardware.</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#a-real-supercomputer-the-hpe-cray-ex","title":"A real supercomputer: The HPE Cray EX","text":"<p>Pushing air around consumes a lot of energy, especially if air has to be put at very high speed through the narrow spaces of high density servers, while the density has to be high to reduce the length of  cables for the interconnect. Some servers house one or two CPUs and 4 modern GPUs in a box less than  50cm wide and 9cm high, while producing roughly 3kW of heat. In fact, the energy consumption of just the fans that move air around can be 10% of the energy consumption of a cluster. Air is also a very inefficient medium to transport heat. Liquids tend to be much better than that. Hence big modern supercomputer often cool most if not all components with water. The added advantage of this is in fact that the water can also be warmer, both when entering the system and when leaving it again, then with the air cooling solution described above. An example of a fully water cooled system that is also very dense and very well designed to reduce the amount of fragile cables, is the Cray EX architecture.</p> <p>The HPE Cray EX architecture is currently (2023-2024) used for some of the biggest supercomputers in the world, including the 3 first US exascale systems Frontier, Aurora and El Capitan and the pre-exascale system Perlmutter, the European pre-exascale system LUMI and the Swiss system Alps that will be extended with GPU nodes in 2023.</p> <p> Structure of the Cray EX supercomputer</p> <p>Let's now have a look at how everything connects together to the European pre-exascale  supercomputer LUMI. LUMI has two types of main compute nodes:</p> <ul> <li>CPU-only nodes have 2 64-core AMD EPYC processors with 256 GB, 512 GB or 1 TB of RAM,     and one Slingshot 11 interconnect port. Slingshot 11 is an interconnect designed by     HPE Cray and very well suited for both small and very large machines. This section     of the computer is also called LUMI-C.</li> <li>Nodes with 1 64-core AMD CPU and 4 AMD MI250X GPUs. This node type will also be     discussed later in these lecture notes when discussing accelerators.     Each GPU has 128 GB of very high bandwidth memory, and the CPU connects to 512 GB     of RAM, bringing the total to 1 TB of RAM memory per node. Each node also has      4 Slingshot 11 ports. This section of the computer is also called LUMI-G.</li> </ul> <p>The Cray EX architecture does use a custom rack design for the compute nodes that is also fully water cooled. It is designed for maximum density to keep connections in the machine as short as possible. It is build out of units that can contain up to 4 custom cabinets, and a cooling distribution unit (CDU). The size of the complex as depicted in the picture is approximately 12 m<sup>2</sup>. Each cabinet contains 8 compute chassis in 2 columns of 4 rows. In between the two columns is all the power circuitry. Each compute chassis can contain 8 compute blades that are mounted vertically. Each compute blade can contain multiple nodes, depending on the type of compute blades. HPE Cray have multiple types of compute nodes, also with  different types of GPUs. In fact, the Aurora supercomputer which uses Intel CPUs and GPUs and El Capitan, which uses the MI300 series of APU (integrated CPU and GPU) will use the same design with a different compute blade. </p> <p>Each LUMI-C compute blade contains 4 compute nodes and two network interface cards, with each network interface card implementing two Slingshot interfaces and connecting to two nodes. The picture below shows a very similar node, but with 4 network cards each providing 2 network connections rather than 2 such cards. On LUMI, only the middle ones are present.</p> <p> Blade with 4 CPU nodes and 8 Slingshot ports</p> <p>At the front of this blade there are two connections to for the water cooling. The  network connections are on the back but are not visible in this picture.</p> <p>A LUMI-G compute blade contains two nodes and 4 network interface cards, where each interface card now connects to two GPUs in the same  node. All connections for power, management network and high performance interconnect of the compute node are at the back of the compute blade. At the front of the compute blades one can find the connections to the cooling manifolds that distribute cooling water to the blades. One compute blade of LUMI-G can consume up to 5kW, so the power density of this setup is incredible, with 40 kW for a single compute chassis.</p> <p> Blade with 2 MI250x GPU nodes and 8 Slingshot ports</p> <p>The back of each cabinet is equally genius. At the back each cabinet has 8 switch chassis, each matching the position of a compute chassis. The switch chassis contains the connection to the power delivery system and a switch for the management network and has 8 positions for  switch blades. These are mounted horizontally and connect directly to the compute blades. Each slingshot switch blade has 8x2 ports on the inner side for that purpose, two for each compute blade. </p> <p>The picture below shows a Slingshot switch blade, with the side facing the compute blades at the front. It has 8 square connectors, one each to each compute blade, but each connector carries two Slingshot connections. The connectors on both sides are for power and monitoring.</p> <p> Switch blade, side facing the compute blades, with 8 double ports</p> <p>The next picture shows the outward facing side of the switch blade. There are 24 electrical connectors, but each again carries two Slingshot links. At the far left and far right you can again note the connectors to the water cooling. Each switch itself can consume up to 250W and is also fully water-cooled.</p> <p> Switch blade, outward facing side</p> <p>As each switch blade carries two ports per compute blade, for LUMI-C two switch blades are needed in each switch chassis as each blade has 4 network interfaces, and for LUMI-G 4 switch blades are needed for each compute chassis as those nodes have 8 network interfaces. Note that this also implies that the nodes on the same  compute blade of LUMI-C will be on two different switches even though in the node numbering they are numbered consecutively. For LUMI-G both nodes on a blade will be on a different pair of switches  and each node is connected to two switches.</p> <p>The picture below gives an impression of how this works for blades with 4 CPU nodes with 1 (left) or 2 (right) connections to the Slingshot interconnect.</p> <p></p> <p>The left picture is not completely right as the interface coming from NMC0 should not connect to the HSS port but to the Switch 3 port. In these pictures, each node has two CPUs and those CPUs are next to each  other, not on top of each other, so the nodes are arranged in a 2x2 grid. Between each node pair there are 4 PCIe connectors, 2 of wich connect to the upper node and 2 of which to the lower node. NMC0 and NMC1 are the Slingshot network cards, and each has 2 PCIe connections that go to connectors that connect to  different nodes. In the left picture, with one Slingshot connection per compute node, node 0 and 1 connect to the switch in slot 3 while node 2 and 3 connect to the switch in node 7. In the right picture, with one Slingshot connection per CPU or two per node, Node 0 and 1 both connect to two switches, the ones in slot 3 and slot 1, while node 2 and 3 also both connect to two switches, the ones in slots 5 and 7. This setup gives each CPU direct access to the interconnect without interfering with the other CPU in the node.</p> <p>There are still free positions in the switch chassis as there is currently no node type that had more than 8 network ports per blade. These could still be useful in the future though. If not for an interconnect, one could, e.g., export PCIe ports to the back and attach, e.g., PCIe-based storage via blades as the  switch blade environment is certainly less hostile to such storage than the very dense and very hot compute blades.</p> <p>The storage of LUMI uses hardware that looks more conventional. Disks are also in JBODs just as in the setup of Vaughan. The file servers themselves though may look conventional on the outside but are also a special design at the inside, housing 2 single socket nodes and 24 dual-ported NVMe drives that connect to both nodes in a regular sized server with plenty of room for connections to the interconnect.</p> <p>The picture below shows the whole LUMI system as installed in the data centre:</p> <p></p> <p>At the front there are 5 rows of cabinets similar to the ones in the exploded Cray EX picture above. Each row has 2 CDUs and 6 cabinets with compute nodes, roughly 9.6\u00a0m by 1.75\u00a0m. At the back of the room there are more  regular server racks that house the storage, management nodes, some special compute nodes , etc. The total floor space (including storage etc.) is roughly 300\u00a0m<sup>2</sup>, the size of a tennis court. </p> <p>Remark</p> <p>The water temperature that a system like the Cray EX can handle is so high that in fact the water can be cooled again with so-called \"free cooling\", by just radiating the heat to the environment rather  than using systems with compressors similar to air conditioning systems, especially in regions with a colder climate. The LUMI supercomputer is housed in Kajaani in Finland, with moderate temperature almost  year round, and the heat produced by the supercomputer is fed into the central heating system of the city, making it one of the greenest supercomputers in the world as it is also fed with renewable energy.</p>"},{"location":"C05_Summary1/C05_S07_Building_up_the_supercomputer/#an-older-example-the-cray-2","title":"An older example: The Cray-2","text":"<p>The Cray-2 is a vector computer launched in 1985, and then one of the fastest machines one could buy. The processing part of the machine was build in a 300 degree arch and can be seen on the picture below.</p> <p></p> <p>It consisted of roughly 240,000 chips of which 75,000 were memory chips.  It was also one of the first shared memory supercomputers, having up to four independent vector processors running at a for that time incredible 240 MHz. The diameter was about 1.35\u00a0m, and the  height about 1.14\u00a0m. The floor surface for this unit was about 1.5\u00a0m<sup>2</sup>, and its power consupmption was close to 200\u00a0kW for the most powerful models in the series. It is clear that if you produce that much heat in such a small volume very special cooling was needed. The processing unit was cooled with immersion cooling. Next to the processing unit was a heat  exchanger to link to the cooling circuit of the data center. The system also needed some more external boxes to house disk drives, a front end, etc.</p> <p>Compare this to LUMI and other large supercomputers of today. The racks with processors of LUMI require over 60\u00a0m<sup>2</sup> not counting the empty space needed between the racks that is needed to access the parts and not counting the heat exchangers to keep the comparison fair with the Cray-2 that needed only 1.5\u00a0m<sup>2</sup>. For Frontier, the fastest supercomputer in the world according to the November 2023 Top-500 list, the floor space of the processing units alone would be close to 200 m<sup>2</sup>. The power consumption of LUMI is around 6\u00a0MW, and that of Frontier, is closer to 20\u00a0MW.  So one of the fastest supercomputers of 1985 consumed only 1% of the power of the fastest one in early 2023. The cost of the Cray-2 is approximately 40-50\u00a0M$ in dollars of today, while the current cost of the  exascale machines is estimated at 500\u00a0M$ or more, so the cost was about 10% of today's fastest computers.</p> Funny fact (click to expand) <p>In 1986, the University of Stuttgart got funding to buy a 4-processor Cray-2 system. It is still on display today, and it is a common practice to check if you still fit inside the system. But the picture below also gives a good impression of the small size of the system:</p> <p></p> <p>(with thanks to the willing model).</p>"},{"location":"C06_Middleware/","title":"Middleware: Turning the hardware into a usable supercomputer","text":"<ul> <li>Introduction</li> <li>Shared memory computing</li> <li>Vector computing</li> <li>Distributed memory supercomputing</li> <li>Some examples</li> <li>Further reading</li> </ul>"},{"location":"C06_Middleware/C06_S01_Introduction/","title":"Introduction: Why do I need to know this?","text":"<p>A supercomputer is not only about hardware. It is the software environment that turns hardware that is often just more or less regular server hardware repackaged for density in a performant supercomputer, and a lot of that software is so-called middleware that sits in between the operating system and the user application.</p> <p>Now a typical reaction is \"I'm not a programmer, should I know this.\" The answer is yes, for multiple reasons.</p> <p>It is important to realise that while your workstation is some standard hardware with a standard Linux distribution on top of it, this is not the case for a supercomputer. We've seen that some of the hardware is relatively standard, and almost all academic  supercomputers run some variant of Linux. But with that all you have is a loose set of servers, and not a supercomputer. There is a lot of software on top of that which  we call here the middleware, that turns this into a supercomputer. Some of it is management software and special file systems, but there are also libraries that provide you with  different means to exploit parallelism, e.g., communication libraries for distributed memory supercomputing, or libraries that are used internally by programming languages, e.g., the OpenMP runtime library that a compiler that supports OpenMP uses for many different operations. Many of those libraries are standardised at the API level (Application Programming Interface, the set of function names, predefined constants, etc., that you can use in your programs) but they are not standardised at the ABI level (Application Binary Interface, the interface that your program compiled with that library links to). MPI is a notorious example of a library with a well standardised API but non-standardised ABI.  OpenMP may be very standardised at the level of the programming language, but  implementers of the runtime libraries have total freedom except for some functions defined by the OpenMP standard. All this implies that a program that is compiled  with one compiler or one MPI library, cannot use the runtime libraries of another compiler or a different MPI library when running. One one hand, this implies that mixing compilers in your environment can cause problems which is why so many clusters are very strict about which programs can be used with which other programs. On the other hand, it also implies that getting programs that come as precompiled binaries to run on a cluster, is not always possible. E.g., it may be compiled with an MPI library that does not support the  interconnect on the cluster while it is also impossible to swap that library with one that supports the system because the ABI is different. </p> <p>Because of this, many scientific applications come as source code. Knowing something about middleware that is currently in use on supercomputers helps you judge whether a code is ready for modern hardware. It also helps you to figure out which components you'll need on the cluster and hence to check if these components are available or can run on that cluster. The middleware that has been used for the application also affects the way you have to start your program. E.g., distributed memory programs are often started through another program that is part of the middleware, the so-called program starter.  In other cases, some environment variables need to be set to tune the performance. This is often the case with shared memory programs but also more and more often for distributed memory programs, and certainly for big programs on big clusters.</p> <p>Moreover, often the support teams can no longer do all software installations for their users as the variety in software becomes too large and the quality of the installation procedures too low, so more and more personpower is needed for software installations, personpower that is often not available.</p> <p>We'll now run through middleware covering three levels of parallelism: vectorization, shared and distributed memory, but will do so in a different order that is more suited to explain the concepts.</p> <p>LUMI users</p> <p>Getting precompiled binaries to run is a particular problem on LUMI.  It needs an MPI library with explicit support for the Slingshot 11 interconnect for good performance, which implies that it must use the libfabric library on LUMI. Cray MPICH does so and supports the  \"MPICH 3.4 ABI\", so it can replace other MPICH libraries that support that ABI at least if there are no other runtime or general library conflicts. But Open MPI had a different ABI and Open MPI software  rarely runs well on LUMI, and certainly cannot use GPU-aware MPI on LUMI at the moment (late 2023).</p> <p>The same can happen with software that is compiled with a newer version of ROCm than we have on the system. Sometimes the functionality needed by that package will be supported by the current device driver on the system, but sometimes it won't and there will be no way to get that package to work, not even in a container.</p>"},{"location":"C06_Middleware/C06_S02_Shared_memory/","title":"Middleware for shared memory computing","text":"<p>When discussing shared memory parallelism in the  section on processors, we've already said that automatic shared memory parallelisation has not been very successful so far.</p>","tags":["shared memory","OpenMP","Julia"]},{"location":"C06_Middleware/C06_S02_Shared_memory/#openmp","title":"OpenMP","text":"<p>One of the more popular ways today to develop scientific software that makes use of shared memory parallelism, is OpenMP.</p> <p>OpenMP works through compiler directives, which are hints placed in the code in such a way that they would be neglected by compilers that don't know the directives. In C and C++ this is done via lines starting with <code>#pragma</code> while in Fortran a notation that would be interpreted as just another comment in the code is used (<code>!$OMP</code>).</p> <p>OpenMP supports two styles of parallelism: data-parallel and task-parallel computing. In data-parallel computing each thread works on a part of the data and it is a type of parallelism that is very popular in scientific applications as it has the potential to scale to more and more processors the larger the data set is.  In task-parallel computing each thread works on a specific task instead.  One could consider data parallelism as a special case of task parallelism where each task is \"operate on a particular part of the data\". But OpenMP does offer some mechanisms for work distribution when used for data-parallel computing.</p> <p>OpenMP is a fairly open standard. OpenMP directives are not vendor-specific.  OpenMP has been around since 1997, when shared memory computing was still a technology  mostly used in bigger servers and supercomputers.  The latest version at the last revision of this chapter is OpenMP 5.2, which was launched at the Supercomputing Conference in November 2021, and version 6.0 of the standard is currently (early 2024) in the preview stage, aiming for a release at SC'24 in November 2024. The standard nature implies that code developed with one compiler with OpenMP support will also compile with another compiler as long as the code and the compilers adhere to the language standards and OpenMP standard. </p> <p>OpenMP originally only supported shared memory parallelism. Version 4 however was a major revision introducing support for vectorisation and for offload to coprocessors (really designed with GPU computing in mind).</p> <p>In OpenMP, the runtime behaviour such as the number of threads used and the mapping of those  threads on the cores of the node, are often set through environment variables that are in fact also partly standardised (though many implementations add some others).  Some codes will use OpenMP library functions instead and will expect information from the user in, e.g., the input files.</p> <p>OpenMP for shared memory parallelism is currently widely supported in C/C++ and Fortran compilers for scientific use. </p> <p>VSC users</p> <p>OpenMP is supported in both the GNU C/C++ and Fortran compilers and the Intel compilers.</p> <p>LUMI Users</p> <p>On LUMI, OpenMP is supported in all available C/C++ and Fortran compilers (GCC, AMD aocc and ROCm, and the Cray Compiling Environment compilers).</p>","tags":["shared memory","OpenMP","Julia"]},{"location":"C06_Middleware/C06_S02_Shared_memory/#other-approaches","title":"Other approaches","text":"<p>C++ frameworks such as the Intel Thread Building Blocks (TBB) are also often used. The Intel TBB library is open-sourced and can be used with other compilers also. It has also been adopted in Intel's oneAPI specifications and is called oneTBB in that context. OneAPI is Intel's effort to create a set of specifications for compilers and libraries suitable for both CPU-only computing and computing with accelerators. Everybody is free to implement the specification, but not all oneAPI components that Intel provides are fully open-sourced, and Intel unfortunately uses the name also  for libraries that implement oneAPI with additional extensions on top of the specification.</p> <p>Some languages also have thread concepts or other concurrent processing concepts built into the language or its standard runtime library. Java is one such example, but Java is not really suited for distributed memory applications as the garbage collection strategy causes performance problems in the typical scientific distributed memory applications as many of those applications tend to use data parallelism which typically requires all processes to run quasi-synchronised as they frequently exchange data. Whenever a garbage collector kicks in at random it also slows down the other processes as they cannot communicate properly anymore with the process where the garbage collection is going on. Microsoft C# is another such language, but not really used in scientific computing at scale.  Another such language is Google Go but that one also is not very suitable for supercomputing  partly due to high memory consumption due to poor memory management which is also why Google isn't using the language as much anymore as they used to. A language really designed for scientific computing with parallelism in mind is Julia which can offer a nice and more performant alternative to  Python and Matlab. Strangely enough though the language designers focused on distributed memory computing first and have only started adding shared memory computing concepts in more recent years.</p> <p>Lastly, one can use explicit OS threading, especially for task-based parallelism. Linux supports the POSIX standard for threads via the Pthreads library. This is very low-level though and rather cumbersome as now the programmer has to code all thread management themselves. Certainly for data parallelism this is rarely a good idea.</p>","tags":["shared memory","OpenMP","Julia"]},{"location":"C06_Middleware/C06_S03_Vector_computing/","title":"Vector computing","text":"<p>Automatic vectorization in compilers is moderately successful. The main problem is that most programming languages are not written with vectorization in mind and focus on scalar computing concepts, while the language does not always include enough information for the compiler to detect if it is safe to vectorize. E.g., languages that allow overlapping memory structures such as C and C++ make the life of an automatic vectoriser very difficult. On the other hand, Fortran does not allow such behaviour but the compiler has no means to check if the programmer adheres to these rules, which can lead to subtle bugs if the compiler assumes  code is safe for vectorisation.</p>","tags":["vector instructions","OpenMP"]},{"location":"C06_Middleware/C06_S03_Vector_computing/#directives","title":"Directives","text":"<p>Part of the solution comes again from compiler directives to help the compiler with deciding whether code can be vectorised. In the past each compiler vendor had its own set of directives that only work with that vendor's compiler, though quickly at least some more or less common directives appeared. After the age of vector computers these directives were somewhat forgotten but they did make a return with the short  vector instruction additions to various instruction sets (including SSE and AVX in  the Intel/AMD architecture).</p> <p>With version 4.0 (in 2013) OpenMP got extended with support for vectorisation though many considered the OpenMP 4.0 directives worse than the vendor-specific ones  Later versions tried to offer some improvements though.  As these directives are standardised, they should work with all compilers that support the standard. One major criticism to the OpenMP vectorisation directives is that they are too much prescriptive rather than descriptive. Prescriptive means that they force the compiler to do certain things, while prescriptive means that they just give the compiler additional information to let the compiler decide if it is worth vectorising the code.</p>","tags":["vector instructions","OpenMP"]},{"location":"C06_Middleware/C06_S03_Vector_computing/#libraries","title":"Libraries","text":"<p>Often the critical part of the code that can benefit most from vectorisation is  contained in a few routines, and chances are that these are actually pretty standard operations for which good libraries exist, e.g., some basic linear algebra libraries, FFT or image processing. In those cases it is better to rework the code to make proper use of such libraries. E.g., optimised BLAS libraries for linear algebra (which are also discussed later in these course notes), the FFTW library which is a popular highly optimised library for FFT, ...</p>","tags":["vector instructions","OpenMP"]},{"location":"C06_Middleware/C06_S03_Vector_computing/#intrinsics","title":"Intrinsics","text":"<p>The method of last resort is manual vectorisation using intrinsics and additional data types that correspond to vector registers. These intrinsics and data types translate directly into vector instructions. As these are basically machine instructions and data types, they  are CPU-specific and they may also be compiler-specific (though often other compilers follow the intrinsics chosen by the CPU vendor's compiler). These intrinsics should be used with care as you loose portability to other CPU architectures or even other variants of the architecture. They may be an option though to get the last bit of performance out of an intensely used kernel in the code, and there are applications that use them in that way.</p>","tags":["vector instructions","OpenMP"]},{"location":"C06_Middleware/C06_S04_Distributed_memory/","title":"Distributed memory supercomputing","text":"<p>Automatic strategies through the compiler have never really made it past the research phase, but there are other options.</p>","tags":["distributed memory","MPI","PGAS","Julia"]},{"location":"C06_Middleware/C06_S04_Distributed_memory/#messaging-libraries","title":"Messaging libraries","text":"<p>In the early days of distributed parallel computing, each vendor had its own communication library which meant that it was very difficult to write portable programs.  PVM which stands for \"Parallel Virtual Machine\" was a research project that developed a very popular library that could even be used to combine a couple of workstations into a distributed memory cluster.  Being more a research project, and basically being developed by a small number of groups, it never really reached a level of support by supercomputer vendors themselves.</p> <p>However, just two years after the launch of PVM, a standardisation effort was started which led to the hugely successful MPI standard  which is still in use today.  MPI stands for Message Passing Interface and is fully standardised. This implies that software that compiles with one MPI library should also compile with any other MPI library adhering to that version of the standard. Compatibility is only at compile time though. The binary interface of the MPI libraries is not standardised and differs between implementations.  There are two main public domain implementations, Open MPI and MPICH,  which has an offspring that one could consider a third implementation, MVAPICH. Most vendor based MPI libraries are derived from one of these implementations.</p> <p>Many MPI programs skip the shared memory level, using one single-threaded process per core or even hardware thread. On modern CPUs it may be more efficient though to combine MPI with one of the shared memory programming techniques (most often OpenMP) and run with,  e.g., one process per node, per socket, per NUMA domain or on AMD architectures, per L3 cache  domain (CCD on zen3 or CCX on Zen2).  With the rapid increase of the number of cores on a supercomputer node this is more and more becoming a necessity as just the amount of messages risks to overwhelm the network adapter and the number of processes in an MPI job slows down program startup and collective communication (the type of communications where all processes compute a single result, e.g., a sum of numbers across all processes) become too expensive. Such applications are called hybrid MPI/OpenMP applications (assuming they use OpenMP). Examples of programs that are sometimes used in hybrid mode are QuantumESPRESSO, Gromacs and VASP.</p> <p>Examples of vendor-specific MPI libraries:</p> <ul> <li>Intel MPI is a derivative of MPICH,</li> <li>HPE Cray MPI is a derivative of MPICH,</li> <li>Mellanox/NVIDIA distributes Open MPI.</li> </ul>","tags":["distributed memory","MPI","PGAS","Julia"]},{"location":"C06_Middleware/C06_S04_Distributed_memory/#languages","title":"Languages","text":"<p>Some programming languages have distributed memory concurrent computing built into the language itself. </p> <p>One such language that we already mentioned is Julia. </p> <p>Another example is  Charm++, a language that is used in the popular molecular dynamics code NAMD but also in a few other lesser known or more specialised applications. </p>","tags":["distributed memory","MPI","PGAS","Julia"]},{"location":"C06_Middleware/C06_S04_Distributed_memory/#pgas-languages","title":"PGAS languages","text":"<p>Partitionad Global Address Space (PGAS) languages distinguish between local and remote memory but allow the latter to be used almost as it it is local memory, though with a severe speed penalty. It is then up to the compiler to translate that in message for the underlying hardware/OS/middleware combination. These languages were all the hype between roughly 2000 and 2010 when DARPA funded the development  of new high performance high productivity languages through the HPCS program.  It led to the development of three languages: Fortress (by Sun Microsystems), X10 (by IBM), and Chapel (by Cray), but Chapel seems to be the only surviving language of that program that is  still begin developed by HPE Cray.  Two other languages of this type not developed in the DARPA program are co-array Fortran and Unified Parallel C (UPC). Co-array Fortran became part of the Fortran 2008 standard and some Fortran compilers offer basic support for it. Unified Parallel C was derived from C99. It never became part of a standardisation process and as a result official support by compiler vendors is poor.  However, there are still compilers based on Clang and on GCC, and HPE Cray supports UPC in their Cray Compiling Environment compiler (which is based on Clang).</p> <p>The problem with PGAS languages is that performance is often poor. Therefore codes sometimes opt to still do some of the work with MPI.</p> <p>There are also library based approaches to PGAS. SHEM/OpenSHMEM and GASPI are just two examples. GASPI is also sometimes used as the runtime library underlying other PGAS languages.  Single-sided communications in MPI-2 and later MPI standards use a very similar approach as those libraries.</p> <p>PGAS languages and libraries have never become very popular, maybe with the exception of single-sided  MPI communications.</p>","tags":["distributed memory","MPI","PGAS","Julia"]},{"location":"C06_Middleware/C06_S05_Examples/","title":"Some examples","text":"<p>QuantumESPRESSO is a suite of codes for electronic structure calculations and materials monitoring. Some of those codes can scale to very large supercomputers when used to solve large problems.  QuantumESPRESSO is nowadays a hybrid code that combines MPI (the most mature  parallelisation technique in the code) with an increasing use of OpenMP.  To use the code properly, you need to understand the parallelism well. It is so important that it has  its own chapter in the user manual and a complete section in the FAQ. And both make it very clear that running the code properly is all but an automatic process...</p> <p>GROMACS is a molecular dynamics code.  The new 2023 manual has a  section explaining the various parallelisation schemes which includes vectorisation, OpenMP, an MPI library implemented on top of threading, MPI, GPU acceleration (which we will discuss in a later section) and a hybrid model combining MPI and OpenMP.</p> <p>A not-so-good example is SAMtools, a bio-informatics tool.  The default settings are downright poor and not even suitable anymore for a modern PC. E.g., the sort tool runs by default still on a single thread and will not even try to detect multiple cores in the computer, and the default amount of memory per thread is even in the newest version still a meager 768 MB. So if a naive user is using this code and reserving a supercomputer node for it, the code will actually use only a  fraction of the available memory and CPU capacity and use an out-of-core memory  instead for large problems, using space on disk as work space, slowing down the operation a lot. Even though distributed sorting algorithms were already developed in the '80s, SAMtools still does not support distributed memory computing (though for the typical problem sizes the code is used for this may not yet be a  big problems).</p>"},{"location":"C06_Middleware/C06_S06_Further_reading/","title":"Further reading on middleware","text":"<ul> <li>OpenMP web site</li> <li>Intel oneTBB home page<ul> <li>Repository on GitHub</li> </ul> </li> <li>Concurrency in languages for shared memory computing:<ul> <li>Concurrency in \"The Java<sup>TM</sup> Tutorials\"</li> <li>Concurrency in C#, in the O'Reilly \"Concurrency in C# Cookbook\"</li> <li>Concurrency in Go, in \"Effective Go\"</li> <li>Julia 1.x documentation<ul> <li>with a page on parallelism</li> <li>Julia YouTube channlel with plenty of good tutorial videos</li> <li>2020 article on Julia adoption</li> </ul> </li> </ul> </li> <li>Pthreads programming tutorial at LLNL</li> <li>Mathematics libraries:<ul> <li>BLAS - Basic Linear Algebra Subprograms</li> <li>FFTW, an optimised FFT implementation</li> </ul> </li> <li>MPI:<ul> <li>MPI forum, the standardisation committee</li> <li>Open MPI</li> <li>MPICH</li> <li>MVAPICH</li> </ul> </li> <li>Charm++.</li> <li>PGAS languages for distributed memory computing:<ul> <li>Chapel<ul> <li>Chapel web page with links to its social media</li> <li>Chapel on GitHub</li> </ul> </li> <li>X10<ul> <li>X10 web page</li> <li>X10 on GitHub</li> </ul> </li> <li>Fortress</li> <li>Coarray fortran<ul> <li>A very dated web page at RICE university</li> <li>An old page at NAG discussing coarrays in Fortran 2008</li> </ul> </li> <li>Unified Parallel C @ Berkeley Lab</li> </ul> </li> <li>PGAS libraries<ul> <li>OpenSHMEM</li> <li>GASPI<ul> <li>GASPI-Forum, the consortium setting the GASPI standard</li> <li>GPI-2, an open source implementation made by the Fraunhofer Institute</li> </ul> </li> </ul> </li> </ul>","tags":["OpenMP","Julia","MPI","PGAS"]},{"location":"C07_Expectations/","title":"What can we expect?","text":"<ul> <li>Speed-up</li> <li>Illustration: Matrix-matrix multiplication</li> <li>Speed-up and efficiency of DGEMM</li> <li>Conclusions</li> </ul>"},{"location":"C07_Expectations/C07_S01_Speedup/","title":"Speed-up","text":"<p>Or: Using 100 processors should mean your job runs 100 times faster, right?</p>"},{"location":"C07_Expectations/C07_S01_Speedup/#speed-up-and-efficiency","title":"Speed-up and efficiency","text":"<p>Success in running on a parallel computer is often measured by looking at the speed-up, i.e., how much faster does a code run on X processors compared to on the smaller number \\(Y\\) processors, with \\(Y\\) typically one?  So in a mathematical formula, </p> \\[S(X,Y) = {T(Y) \\over T(X)}\\] <p>with \\(T(X)\\) the time on \\(X\\) processors and \\(T(Y)\\) the time on \\(Y\\) processors. Ideally \\(Y = 1\\) but in  practice for big problems it is not feasible to measure \\(T(1)\\) because the problem doesn't fit in memory or simply takes too much time to run.</p> <p>An alternative measure is the efficiency. When \\(Y=1\\), the efficiency is simply the speed-up divided by \\(X\\), i.e., this would be \\(100%\\) in the ideal case that using \\(X\\) processors also speeds up the computations with a factor of \\(X\\). The more general definition is</p> \\[ \\epsilon(X,Y) =  \\frac{Y T(Y)}{X T(X)}, \\] <p>i.e., the total time spent by all \\(Y\\) processors (with \\(Y\\) ideally 1) divided by the total time spent by all \\(X\\) processors.</p>"},{"location":"C07_Expectations/C07_S01_Speedup/#amdahls-law-and-saturation","title":"Amdahl's law and saturation","text":"<p>In the ideal case, running on \\(X\\) processors instead of \\(1\\) should make the application \\(X\\) times faster, but this is never the case. The graph below shows three scenarios.</p> <p> Ideal speed-up, Amdahl's law and saturation</p> <p>The top line shows the ideal speed-up. However, Amdahl noted already in the late '60s that using parallelism would have its limitations as every program will have some part that cannot be executed in parallel. Even if the parallel section could be executed on an infinite number of processors, the total run time would still be limited by that of the sequential part of the code. Assuming that \\(99\\%\\) of the run time for a particular problem on one processor could be  replaced by parallel code, then you could still never get a speed-up larger than 100 as there is that \\(1\\%\\) of the runtime that cannot be eliminated. In fact, in later papers there was even a formula derived for this:</p> \\[S(X,1) = \\frac{1}{(1-p) + \\frac{p}{X}}.\\] <p>This curve is shown as the magenta line in the graph.</p> <p>In reality the situation is even worse. Amdahl's law does not take communication  overhead into account. In practice, the more processors are used the larger the relative communication overhead will become and this will lead to a saturation and even a lowering of the speed-up at some point as the number of cores is further increased. This is depicted in the lower line on the figure.</p> <p>Amdahl's law already implies that whenever using parallel computing, one has  to balance the cost of using more processors (as that cost will increase due to the efficiency loss) with the gain from getting an answer quicker. But this is even more pronounced in a more realistic situation.</p> <p> Ideal speed-up, Amdahl's law and saturation: Conclusions</p> <p>Looking at the blue curve (the lower one) in the above graph shows that it makes no sense to even use 64 or 128 cores for this particular problem as the speed-up is lower than  for roughly 32 cores (and hence the execution time higher). However, going from 8 to 16  cores we also effectively gain only \\(35\\%\\) performance so one can wonder if this is worth the cost. The sweet spot for this problem is probably somewhere between 4 and 8 cores.</p>"},{"location":"C07_Expectations/C07_S01_Speedup/#what-does-this-mean","title":"What does this mean?","text":"<p>Using \\(X\\) times more processors will almost never speed up your application with a  factor of \\(X\\) as there is always some overhead in using more cores.  There are some rare cases where this is not the case though and where you may see  what is called a superlinear speed-up. This is due to cache effects. As you are using more cores, the problem per core becomes smaller and as a consequence more of it may fit into the caches, improving the performance of a core.</p> <p>There is also no rule like program A runs best on \\(X\\) processors.  The optimal number of processors \\(X\\) does not only depend on the application that you're using. It also depends on the problem being solved. As we will see further in this chapter, the bigger the problem, the higher the optimal number of cores. Moreover, it also depends on the cluster: the characteristics of the CPU, the interconnect, ..., all have an influence on the number of cores that can be used with reasonable efficiency. In general, on supercomputers with a slower interconnect with less bandwidth or more latency, fewer cores can be used for any given problem, even if those cores are the same as on the computer with faster interconnect.</p> <p>The general rule is though that bigger problems give you a better speed-up and efficiency for a given number of cores, or let you use a larger number of processors for a given  target efficiency.</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/","title":"Illustration: Matrix-matrix multiplication","text":"<p>Let us illustrate the effect of the importance of optimising memory access patterns and how good libraries can help us with a problem that seems simple: the multiplication of two matrices (which we take square here to keep it as simple as possible).</p> <p>The element \\(c_{i,j}\\) on row \\(i\\) and column \\(j\\) of the product</p> \\[ C \\leftarrow C + A * B \\] <p>is computed as </p> \\[ c_{i,j} \\leftarrow c_{i,j} + \\sum_{k=1}^N a_{i,k} b_{k,j}. \\] <p>We can compute the new value of \\(C\\) with this formula either row by row or column by column. Computing row by row results in the following pseudo-code:</p> <pre><code>for i = 1 to N\n    for j = 1 to N\n        for k = 1 to N\n            c(i,j) = c(i,j)+a(i,k)*b(k,j)\n</code></pre> <p>We'll call this the ijk-variant because of the order of the loop indices.</p> <p>Computing the elements of \\(C\\) column by column can be done with the following pseudo-code:</p> <pre><code>for j = 1 to N\n    for i = 1 to N\n        for k = 1 to N\n            c(i,j) = c(i,j)+a(i,k)*b(k,j)\n</code></pre> <p>which we call the jik-variant for obvious reasons.</p> <p>The code for matrix-matrix multiplication has three nested loops. There are 6  different permutations of the loop indices i, j and k, and all 6 result in a correct program:</p> ijk jik <pre><code>for i = 1 to N\n  for j = 1 to N\n    for k = 1 to N\n      c(i,j) = c(i,j)+a(i,k)*b(k,j)\n</code></pre> <pre><code>for j = 1 to N\n  for i = 1 to N\n    for k = 1 to N\n      c(i,j) = c(i,j)+a(i,k)*b(k,j)\n</code></pre> ikj jki <pre><code>for i = 1 to N\n  for k = 1 to N\n    for j = 1 to N\n      c(i,j) = c(i,j)+a(i,k)*b(k,j)\n</code></pre> <pre><code>for j = 1 to N\n  for k = 1 to N\n    for i = 1 to N\n      c(i,j) = c(i,j)+ a(i,k)*b(k,j)\n</code></pre> kij kji <pre><code>for k = 1 to N\n  for i = 1 to N\n    for j = 1 to N\n      c(i,j) = c(i,j)+ a(i,k)*b(k,j)\n</code></pre> <pre><code>for k = 1 to N\n  for j = 1 to N\n    for i = 1 to N\n      c(i,j) = c(i,j)+ a(i,k)*b(k,j)\n</code></pre> <p>However, even though all 6 above variants are correct, it does not mean that they will also perform the same..</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/#timings-of-a-fortran-implementation","title":"Timings of a Fortran implementation","text":"<p>The following table shows run times and the performance in Gflops derived from the runtime and total number of operations for matrices of size \\(N = 2500\\), which require 47.4 MB of storage per matrix. The runs were all done on a node of an older cluster with Intel Xeon E5-2680v2 processors that can deliver a theoretical peak performance of 22.4 Gflops at a clock speed of 2.8 GHz. </p> <p>The reason for taking this older cluster is that the caches are still relatively small so that a smaller problem size can be used to demonstrate the effect of  cache misses without having to go to such a large problem size that the run time for the worst variants becomes prohibitively extensive to test easily.</p> <p>The results are summarised in the following table:</p> Variant Time (s) Gflops ijk 17.16 1.821 jik 24.35 1.283 ikj 63.68 0.491 jki 9.87 3.165 kij 40.77 0.7666 kji 13.29 2.352 F95 MATMULT 9.51 3.285 OpenBLAS dgemm (1 thread) 1.27 14.60 OpenBLAS dgemm (20 threads) 0.08 396.42 <p>The first 6 rows are the 6 variants of the matrix-matrix multiplication.  We notice that none of the implementations is fast compared to the theoretical speed of a core of this type. But the speed difference between the slowest and fastest of those 6 variants is large: a factor of 6.5x. </p> <p>Fortran 95 has a built-in operation for matrix-matrix multiplication but in this case the code doesn't really perform any better. </p> <p>OpenBLAS is a library of optimised routines for vector-vector, matrix-vector and matrix-matrix operations, and dgemm is the name of the BLAS routine that does the matrix-matrix multiplication that we need. We see immediately that we get a big boost and a result that is even slightly better than the theoretical speed of the core at the nominal frequency of 2.8 GHz. This is because the clock frequency on that node could not be fixed and as we were using just a single thread, we got a slight boost of the clock frequency. The result with OpenBLAS using one thread is 78.7x better than our best simple variant. However, changing just an environment variable lets us run the code using all 20 cores available on the node so without extra work we can even run at close to 400 Gflops!</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/#analysis-of-the-jki-variant","title":"Analysis of the jki-variant","text":"<p>Key to understanding the performance of the 6 variants is to realise that Fortran stores matrices column by column (C does this row by row and in a C implementation another variant would be the better one).</p> <p>Let us first analyse what happens in the innermost loop of the \\(jki\\)-variant, which was the best in the table:</p> <pre><code>for j = 1 to N\n    for k = 1 to N\n        for i = 1 to N\n            c(i,j) = c(i,j)+ a(i,k)*b(k,j)\n</code></pre> <p>for \\(j = 2\\) and \\(k = 3\\). The figure below shows what happens in the 4 steps of the inner loop:</p> <p> </p> <p>We use only one element of \\(B\\). The elements of both \\(A\\) and \\(C\\) are accessed column-wise, i.e., in the order in which they are stored in memory. This is optimal for cache use: When loading the first element, the cache will also load the next few elements in that column (assuming that it would happen to be the first element of a cache line). Typically, a cache will contain blocks of 64 bytes so 8 double precision floating point numbers (the data format used in the tests). So for the second iteration it is very likely that all the data needed is already in memory.</p> <p>The effect of the innermost loop is best summarised by the following picture:</p> <p></p> <p>The inner loop for the given values \\(j = 2\\) and \\(k = 3\\) takes the third column of \\(A\\), which we could consider a vector, multiplies it with the scalar element \\(b(2,3)\\) on the third row and second column of \\(B\\) and adds that column vector to the second row of \\(C\\). This vector interpretation is the key to understand what happens in the middle loop.</p> <p>Let us keep \\(j = 2\\) but now iterate over the middle loop. Each iteration of the middle loop  takes a column vector of \\(A\\), multiplies it with an element of \\(B\\) and adds it to a column vector of \\(C\\) as is shown in the following figure:</p> <p> </p> <p>The whole operation of the innermost two loops for \\(j = 2\\) is depicted in the following figure:</p> <p></p> <p>In all, executing the two innermost loops, we use only a single column of \\(B\\) and \\(C\\), but we  do run through the whole matrix \\(A\\), and would do so again in the next iteration of the outer loop. If \\(N\\) is large enough the leftmost part of the matrix \\(A\\) will have been pushed out of the cache before it is again needed in the next outer iteration, but we would be working on a different set of elements of \\(B\\) and \\(C\\).</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/#but-how-can-blas-then-even-be-faster","title":"But how can BLAS then even be faster?","text":"<p>BLAS stands for Basic Linear Algebra Subprograms and is a library that contains basic  linear algebra building blocks used in other libraries such as Lapack  (a library to solve dense linear systems and eigenvalue problems). It was defined in three phases</p> <ul> <li>BLAS 1 from 1979 defined a set of vector-vector operations to make it easier to     exploit vector computers.</li> <li>BLAS 2 extended that set in 1986 with matrix-vector operations. The extensions were     triggered by the evolving needs to get a good performance on large computers, where     exploiting only vector operations was not enough anymore and more care needed to      be taken of memory accesses too.</li> <li>BLAS 3 extended BLAS in 1988 with matrix-matrix operations, that help to better exploit     a cache hierarchy.</li> </ul> <p>There is a reference implementation of the BLAS API in Fortran, but that implementation doesn't really give you a high performance. However, vendors of microprocessors invest a lot of time in making an optimised implementation for their processors, and there are also a couple of open source projects that did so. A popular closed source vendor implementation is the Intel MKL library (which contains much more than just BLAS). OpenBLAS and BLISS are two recent open source  implementations.</p> <p>The DGEMM code in an optimised BLAS library is a lot more complex than any of our six variants. Optimised implementations work by splitting the matrices in small blocks that fit in cache and compute the matrix-matrix product out of the matrix-matrix products of those smaller blocks. Some libraries may even involve some assembler programming.</p> <p>Blocking for cache reuse is a strategy used by many optimised libraries, also in other domains.</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/#a-small-surprise","title":"A small surprise...","text":"<p>In the following table we repeated our experiment but now comparing the GNU Fortran compiler to the Intel (classic) Fortran compiler:</p> Variant GNU Gflops Intel Gflops ijk 1.821 1.60 jik 1.283 3.40 ikj 0.491 1.60 jki 3.165 3.40 kij 0.7666 10.74 kji 2.352 10.68 F95 MATMULT 3.285 10.97 BLAS dgemm (1 thread) 14.60 24.75 BLAS dgemm (20 threads) 396.42 417,64 <p>Not only does the Intel compiler in general produce better performance than the GNU compiler for this code, with the fastest result being more than three times faster than the best result with the GNU compiler (and another variant becoming the preferred one), but we also notice that the performance is the same for pairs of variants.  The \\(ijk\\)- and \\(ikj\\)-variants have the same performance,  and so have the \\(jik\\)- and \\(jki\\)-variants, and the \\(kij\\)- and \\(kji\\)-variants. What these variants have in common is that the first letter is the same. The Intel compiler is sometimes capable of recognising a bad memory access pattern and exchanging the order of the two innermost loops. We have seen this happening in less trivial user code also. The fact that a different variant is now the best one may be the result of the more powerful vectoriser in the Intel compiler that may have found a better way to vectorise those particular variants.</p>"},{"location":"C07_Expectations/C07_S02_Matrix_multiplication/#and-on-a-machine-with-less-cache-memory","title":"And on a machine with less cache memory...","text":"<p>Repeating the same experiment with the same matrix size on a  laptop with far less cache than a cluster node shows an even more interesting result:</p> Variant Time (s) Gflops ijk 297.40 0.105 jik 295.53 0.106 ikj 1002.28 0.031 jki 11.67 2.678 kij 1002.48 0.031 kji 15.85 1.971 F95 MATMULT 17.06 1.832 OpenBLAS dgemm (1 thread) 3.42 29.979 OpenBLAS dgemm (2 threads) 1.80 57.349 <p>The speed difference between the slowest and fastest of our 6 naive variants is now already a factor of 86. And again we see a more than 10-fold increase  from the best variant to a single-core run with a good BLAS library, and a further near doubling of the performance when we also employed the second core.</p>"},{"location":"C07_Expectations/C07_S03_Scaling/","title":"Speed-up and efficiency of DGEMM","text":"<p>Let us use the OpenBLAS DGEMM routine from the previous section and see how it behaves for different matrix sizes going from 1,000 to 25,000 and a different number of cores. First we look at the speed-up which in this case where we can run on a single core is defined as</p> \\[ S(p) = \\frac{T(1)}{T(p)} \\] <p>with \\(p\\) the number of cores.</p> <p> Speed-up of DGEMM for various matrix sizes</p> <p>As one would expect, the speed-up increases with a growing number of cores, except for an anomaly on this specific machine when using all 20 cores which was due to background work going on in the operating system. For smaller core numbers the four lines are almost indistinguishable but  as the number of cores increases we see that the speed-up is much better for the larger matrix sizes, when there is more work per processor.</p> <p>For this particular example we don't really see the flattening of the speed-up or even lowering predicted for large core numbers in the model in the first section of this chapter.</p> <p>The ideal speed-up is of course equal to the number of processors used, but this is more difficult to interpret in a graph. An easier number to work with is the efficiency, which is the speed-up divided by the number of processors:</p> \\[ \\epsilon(p) = \\frac{S(p)}{p} = \\frac{T(1)}{p T(p)}.\\] <p>Here the ideal would be 100%, i.e., a flat line independent of the number of cores.</p> <p>For the same configuration as in the previous graph, we get</p> <p> Efficiency of DGEMM for various matrix sizes.</p> <p>We see that the efficiency decreases with increasing core numbers, and for a given number of cores increases with the problem size.  If we would put the limit for efficiency at 80%, we could only use 6 cores for matrix size 1,000, but could use any number of cores in this 20-core node for the larger matrix sizes (neglecting for now the strange behaviour when all cores are used).</p> <p>However, to show how difficult benchmarking can be on a cluster where you cannot fix the clock speed of a processor and can have turbo mode kick in, consider the following graph where we  computed the efficiency based on the measured number of Gflops:</p> \\[\\epsilon(p) = \\frac{G(p)}{pG_{theoretical}}\\] <p>where \\(G(p)\\) is the speed on \\(p\\) processors and \\(G_{theoretical}\\) is the theoretical peak performance based on the nominal clock frequency of the node (22.4 Gflops at 2.8 GHz for this node).</p> <p></p> <p>We now see that for all four matrix sizes the efficiency defined this way is higher than 100% on a single core as the effective speed is higher because of the turbo boost. Bigger matrix sizes also benefit a little  more which is not abnormal as the whole blocking strategy also comes with overhead that is partly a fixed cost and shows more for the smaller problem. </p> <p>This just shows how difficult interpreting results from benchmarks is if there can be multiple factors in play, like turbo boost, a load on part of the node when other users are also on the node, load on the communication network from other users when benchmarking distributed memory code, ...</p>"},{"location":"C07_Expectations/C07_S04_Conclusions/","title":"Conclusions of this chapter","text":"<p>It is important to realise that not all codes are created equal, even if they implement the same operation or algorithm. E.g., a code using  matrix-matrix multiplication internally may itself implement one of our six variants, or it could be doing the right thing and use an optimised BLAS library. However, even then we still may be unlucky if that code only comes as a binary and would be using an older version of the BLAS library that does not yet contain a proper code path for the CPU you're using.</p> <p>As we have seen, it is also important to not reinvent the wheel. It is very likely that there are already good libraries that you can use for time-consuming parts of your code, or maybe even a complete code for your problem that is much better than anything you can come up with in a short time. Writing a proper BLAS3 library is a Ph.D. thesis by itself...</p> <p>The matrix multiplication illustration also clearly shows how important it is to pay attention to the memory access order and to exploit the memory hierarchy. The first was already illustrated in our six variants, but the optimised BLAS implementation also shows gain from really taking into account the cache hierarchy and optimise for the sizes of the caches using clever blocking inside the code. </p> <p>With respect to speed-up and efficiency, the general rule is that for a fixed problem size, using more cores will lead to lower efficiency (though there are some rare exceptions due to cache effects), while for a fixed number of cores, bigger problems will lead to higher efficiency. Supercomputers are made to  solve big problems.</p> <p>On a modern computer system, benchmarking has become very difficult as there are so many unexpected elements that can influence performance in sometimes  unpredictable or unexpected ways. As a user you may have control over some of these factors (e.g., modern clusters sometimes allow users to set an upper limit for the clock speed), but there are also factors one has no control over as  one cannot simply ask for exclusive access to a big supercomputer.</p>"},{"location":"C08_Accelerators/","title":"Accelerators","text":"<ul> <li> <p>What are accelerators?</p> </li> <li> <p>Offloading</p> </li> <li> <p>CPUs with accelerator features</p> </li> <li> <p>Accelerator programming</p> </li> <li> <p>Status of GPU computing</p> </li> <li> <p>Further reading</p> </li> </ul>"},{"location":"C08_Accelerators/C08_S01_What_are_accelerators/","title":"What are accelerators?","text":"","tags":["CUDA","vector accelerator","matrix accelerator","FPGA"]},{"location":"C08_Accelerators/C08_S01_What_are_accelerators/#in-short","title":"In short","text":"<p>We restrict ourselves to the most common types of compute accelerators used in  supercomputing and do not cover, e.g., accelerators in the network to process certain MPI calls.</p> <p>An accelerator is usually a coprocessor that accelerates some computations that  a CPU might be capable of doing, but that can be done much faster on more specialised hardware, so that it becomes interesting to offload that work to specialised hardware.  An accelerator is not a full-featured general purpose processor that can run a regular  operating system, etc. Hence an accelerator always has to work together with the regular CPU of the system, which can lead to very complicated programming with a host program that then offloads certain routines to the accelerator, and also needs to manage the transport of data to and from the accelerator.</p>","tags":["CUDA","vector accelerator","matrix accelerator","FPGA"]},{"location":"C08_Accelerators/C08_S01_What_are_accelerators/#some-history","title":"Some history","text":"<p>Accelerators have been around for a long time in large computers, and in particular in mainframes, very specialised machines mostly used for administrative work.</p> <p>However, accelerators also appeared in the PC world starting in the '90s.  The first true programmable accelerators probably appeared in the form of high-end sound cards.  They contained one or more so-called Digital Signal Processor (DSP) chips for all the digital processing of the sound. </p> <p>Graphic cards were originally very specialised fixed-function hardware that was not really programmable but this changed in the early '00s with graphics cards as the NVIDIA GeForce 3 and ATI Radeon 9300 (ATI was later acquired by AMD which still uses the Radeon brand). It didn't take long before scientist looking for more and cheaper compute power took note and started experimenting with using that programmability to accelerate certain scientific computations. Manufacturers, and certainly NVIDIA, took note and started adding features specifically for broader use. This led to the birth of NVIDIA CUDA 1.0 in  2007, the first successful platform and programming model for programming graphics cards that were now called Graphics Processing Units (or GPU) as they became real programmable processors. And the term GPGPU for General-Purpose GPU is also used for hardware that is particularly suited to be used for non-graphics work also. GPGPU programming quickly became popular, and even a bit overhyped, as not all applications are suitable for GPGPU computing.</p>","tags":["CUDA","vector accelerator","matrix accelerator","FPGA"]},{"location":"C08_Accelerators/C08_S01_What_are_accelerators/#types-of-accelerators","title":"Types of accelerators","text":"<p>The most popular type of accelerators are accelerators for vector computing. All modern GPUs fall in this family. Examples are</p> <ul> <li> <p>NVIDIA Data Center series (previously also called the Tesla series). These started of as basically     a more reliable version of the NVIDIA GeForce and Quadro GPUs, but currently, with the Ada Lovelace     GPUs and Hopper GPGPUs, these lines start to diverge a bit (and even before, the cards really meant     for supercomputing had more hardware on them for double precision floating point computations).     Strictly speaking the NVIDIA architecture is a single instruction multiple data (SIMD) architecture     and not one that uses explicit vector instructions, but the resulting capabilities are not really      different from more regular vector computers (but then vector computers with an instruction set that     also support scatter/gather instructions and predication, features that are missing from, e.g.,      the AVX/AVX2 instruction set).</p> </li> <li> <p>AMD has the Instinct series for GPGPU computing. They employ a separate architecture for their      compute cards, called CDNA, while their current graphics cards use various generations of the RDNA     architecture. The CDNA architecture is a further evolution of their previous graphics architecture     GCN though (used in, e.g., the Vega cards).</p> <p>AMD Instinct GPUs are used in the first USA exaflop computer Frontier (fastest system in the Top500 ranking of June and November 2022 and June 2023) and in the European LUMI system (fastest European system in the Top500 ranking of June and November 2022 and June 2023). These computers use the CDNA2 architecture. A future USA exascale system, El Capitan,  planned for 2023 (or possibly 2024 with the supply chain disruptions largely due to Covid), will employ a future version of the architecture, CDNA3, which will bring CPU and GPU very close together (but more about this later in this section).</p> </li> <li> <p>Intel is also moving into the market of GPGPU computing with their Xe graphics products. They     have supported GPU computing using their integrated GPUs for computations for many years already, with even support     in their C/C++/Fortran compilers, but are now making a separate product for the supercomputing market     with the Intel Data Center GPU MAX series based on the Xe<sup>HPC</sup> architecture,     which support additional data formats that are very needed for     scientific computing applications. The first product in this line is      is also known as the GPU code named Ponte Vecchio that     will be used in the USA Aurora supercomputer, which should become the second USA exaflop computer.     A future European pre-exascale system was planned to have a compute section with the successor of that      chip, code named Rialto Bridge, but as that chip is cancelled it is not clear which GPU will be used instead.</p> </li> <li> <p>The NEC SX Aurora TSUBASA has a more traditional vector computing architecture, but is physically also     an expansion card that is put in a regular Intel-compatible server. It is special in the sense that the     original idea was that applications would fully run on the vector processor and hence not use a host      programming with offloading, while under the hood the OS libraries would offload OS operations to the     Intel-compatible server, but in practice it is more and more used as a regular accelerator with a      host program running on the Intel-compatible server offloading work to the vector processor.</p> </li> </ul> <p>A second type of accelerator that became very popular recently, are accelerators for matrix operations, and in particular matrix multiplication or rank-k update. They were originally designed to speed up operations in certain types of neural networks, but quickly gained support for additional data types that makes them useful for a range of AI and other HPC applications. Some are integrated on GPGPUs while others are specialised accelerators. The ones integrated in GPUs are the most popular for supercomputing though:</p> <ul> <li> <p>NVIDIA Tensor cores in the V100 (Volta generation) and later generations (Ampere, Turing and Hopper).</p> </li> <li> <p>AMD matrix cores in the MI100 and later chips. The MI200 generation may be a little behind     the similar-generation A100 NVIDIA cards when it comes to low-precision formats used in some     AI applications, but it shines in higher-precision data formats (single and double precision     floating point), as it was developed in the first place for the needs for the Frontier     exascale simulation whose procurement predated the days were AI became very popular.</p> </li> <li> <p>Intel includes their so-called Matrix Engines in the Ponte Vecchio GPGPUs.</p> </li> <li> <p>The NVIDIA tensor cores, AMD matrix cores and Intel matrix engines are all integrated very closely with     the vector processing hardware on their GPGPUs, However, there are also dedicated matrix computing accelerators,     in particular in accelerators specifically designed for AI, such as the Google TPU (Tensor Processing Unit).</p> <p>Most neural network accelerators on smartphone processors also fall in this category as they are usually in a physically distinct area from the GPU hardware in the SOC (though not a separate chip or die).</p> </li> </ul> <p>A third and so far less popular accelerator in supercomputing is an FPGA accelerator, which stands for Field Programmable Gate Array. This is hardware designed to be fully configured after manufacturing, allowing the user to create a specialised processor for their application. One could, e.g., imagine creating specialised 2-bit CPUs for working with generic data. </p>","tags":["CUDA","vector accelerator","matrix accelerator","FPGA"]},{"location":"C08_Accelerators/C08_S01_What_are_accelerators/#a-note-on-the-name-gpu","title":"A note on the name \"GPU\"","text":"<p>Though usually called GPU computing or GPGPU computing, the accelerators still have an architecture  that is an evolution of that of the programmable GPUs from 2010 and later,  but often lack the full hardware rendering pipeline that one would expect on a true GPU. The increasing demand for performance while the cost per transistor tends to stay flat or even increase a bit and while new semiconductor manufacturing processes don't deliver the gains they used to 15 years ago, resulted in an evolution towards distinct \"GPUs\" for compute (traditional HPC and AI) and graphics rendering. We've outlined this already a bit when discussing the types of accelerators earlier on this page. It also implies that compute GPUs doe not always support typical graphics software stacks  such as OpenGL or Vulcan, or that part of these stacks have to rely on software-based rendering accelerated by the vector functions of the GPU rather than full hardware rendering.</p> <p>The NVIDIA line-up has had different cards for compute and rendering for quite a while already.  The Volta architecture launched in 2017, which was the first one to offer the tensor cores for AI,  was used in a few high-end video cards, but the Turing architecture launched a year later was the main architecture for rendering GPUs (though that one in turn was also used in some compute cards). The Ampere generation succeeded both in 2020, but with distinct chips for compute (the A100) and for rendering (with the GA102 as the most powerful one) and distinct differences between both chips. E.g., the A100 had much more FP64 hardware and more tensor hardware,  while the GA102 had ray tracing cores and offered much more regular precision CUDA cores, even though it had only half as many transistors and a 30% smaller die in a slightly larger  process node.  The NVIDIA Hopper compute GPUs and Ada Lovelace rendering GPUS launched in late 2022 were clearly designed together and share some characteristics, but are still very different beasts, this time emphasised by different code names. The ray tracing units present in the Ada architecture are not in the Hopper architecture, the raster engines also seem to be gone, and there is also no trace of video encoding blocks in the architectural documentation, though there is hardware decoding for some video formats and JPEG images as these can be useful in AI applications.</p> <p>AMD only really started in GPU compute architectures towards late 2018 (with basically a first product just to try the software stack) and its architectures for compute and rendering GPUs have been diverging from the start. AMD's rendering GPUs use the RDNA architecture of which the first iteration launched in 2019 and the  third iteration was launched by the end of 2022, while the compute GPUs use the CDNA architecture which is a descendant of the VEGA architecture with a relatively different structure of the compute units. The CDNA GPUs also lack the ray tracing units of RDNA2/3,  and the texture units and raster engine that are needed in rendering GPUs.</p> <p>It is to be expected that compute and render GPUs will only diverge more over time as it is increasingly impossible to build hardware that does both well and is still cost-effective for a large enough market. </p>","tags":["CUDA","vector accelerator","matrix accelerator","FPGA"]},{"location":"C08_Accelerators/C08_S02_Offloading/","title":"Offloading","text":"<p>In early accelerator generations (and this is still mostly the case in 2023), a CPU cannot directly operate on data in the memory of the GPU and vice-versa. Instead data needs to be copied between the memory spaces. Multiple accelerators in a system may or may not share a memory space. NVIDIA NVLink for instance is a technology that can be used to link graphics cards together and create some level of shared memory space, but coherency is also limited and in automatic page migration (migration of blocks of  memory of a fixed size, called a page) over the fast link is done. Many modern GPUs for  scientific computing include support for unified memory where CPU and GPUs in the system can share a single logical address space, but under the hood the data still needs to be copied. This feature has been present in, e.g., the NVIDIA Pascal and later  generations, where a page fault mechanism would be used to trigger a migration mechanism.</p> <p>Control-wise the program running on the CPU orchestrates the work but passes control to the accelerator to execute those code blocks that can be accelerated by the accelerator.</p> <p>The main problem with this model is that all the data copying that is needed, whether explicitly triggered by the application or implicitly through the unified memory model of some modern GPUs, can really nullify the gain from the accelerator. This is no different from what we saw for distributed computing.  We have seen that for distributed computing, the fraction of the algorithm that cannot be parallelized puts a limit to the theoretical speed-up that can be reached, and the communication overhead  reduces the attainable speed-up even more. Something similar holds for computing with accelerators: there is always a part of the code that is not suitable for the accelerators, and that will limit the theoretical speed-up, but all the data copying to and from the accelerator reduces the attainable speed-up  even more. Hence it is clear that for future generations of accelerators, the main attention will be in making them more versatile to reduce the fraction that cannot be accelerated, and in integrating them closer with the CPU to reduce or eliminate the overhead in passing data between CPU and GPU.</p> <p>The first signs of this evolution were in the USA pre-exascale systems Summit and Sierra that used a IBM POWER9 CPUs and NVIDIA V100 GPUs.  Those GPUs were connected to the CPU through NVLink, NVIDIA's interconnect for GPUs, rather than only PCIe links, so that the memory spaces of CPU and GPU become more merged, with some level of cache coherency, though in  practice migration of pages remains necessary in most cases.</p> <p>This technology of the Summit and Sierra supercomputers is carried over to the first three planned exascale systems of the USA. Frontier, and the European supercomputer LUMI, use the MI250X variant of the AMD CDNA2 architecture. The nodes in these systems consist of  4 MI250X GPUs, with each GPU consisting of two dies, and a custom Zen3-based CPU code named Trento. AMD's InfinityFabric is not only used internally in each package to connect the dies (and the zen3 chiplets with the I/O die in the Trento CPU), but also to connect the GPU packages to each other and the CPUs to the GPUs, hence creating a unified coherent memory space with some level of cache coherency. It enables each CPU chiplet to access the memory of the closest attached GPU die with full cache coherency, but the coherency is not usable over the full system.  The Aurora supercomputer, which uses Intel Sapphire Rapids CPUs and Ponte Vecchio GPUs, will also support a unified memory  and some level of cache-coherent memory space.  NVIDIA was lagging a bit with the Ampere generation that has no longer a corresponding CPU that supports its NVLink connection, but returned with its own ARM-based CPU code named Grace and the Hopper GPU generation. The Grace Hopper superchip combines a Grace CPU die and a Hopper GPU die with a coherent interconnect between them, creating a coherent memory space between the CPU and GPU in the same package, though the available whitepapers suggest no  coherency between different CPU-GPU packages in the system. Placing a CPU and GPU in the same package also not only ensures much higher bandwidth between both, but also a much lower energy consumption for the data transfers.</p> <p>Future generation products will go even further. The AMD MI300 is technically speaking no longer a GPU but rather an Accelerated Processing Unit or APU as it will integrate CDNA3 GPU dies, Zen4 CPU dies and memory in a single package. The CPU and GPU dies will no longer have physically distinct memory and AMD claims that copy operations between CPU and GPU in a package will be completely unnecessary. MI300-based products should start appearing towards the end of 2023 or early 2024.  Intel has also hinted at the Falcon Shores successor to Ponte Vecchio and Rialto Bridge, likely a 2024 or 2025 product,  that will also combine CPU and GPU chiplets in a single package with a fully unified memory space, but has now postponed this to a later generation. </p> <p>Note that current Intel or AMD-based PC's with integrated GPUs are not better in this respect. The CPU and integrated GPU share the physical RAM in the system, but each has its own reserved memory space and data still needs to be migrated between those spaces.  The Apple M1 and later chips on the other hand so seem to have a truly unified memory space, though given the limited information that Apple provides, it is hard to see if it is indeed a fully hardware based solution. The fact that the M1 can be so fast in photo- and video processing apps though indicates that the M1 does indeed have an architecture that allows to make use of all accelerators in a very efficient way.</p>"},{"location":"C08_Accelerators/C08_S03_CPUs_accelerator_features/","title":"CPUs with accelerator features","text":"<p>In the (relatively young) history of personal computers and their microprocessors, successful accelerators were often integrated in CPUs by means of extensions of the CPU instruction set. Though never as performant as a dedicated accelerator, it was often a \"good enough\" solution to the extent that some accelerators even disappeared from the market, and as these are now part of the instruction set of the CPU, programming is also greatly simplified as there is no need to pass data and control to a coprocessor.</p> <p>Vector accelerators have long had an influence on CPUs.  The original Intel MMX instructions (which is rumoured to stand for MultiMedia eXtensions) were  designed to compete with the DSPs used in sound cards in the second half of the '90s. They were introduced in an update of the Pentium architecture in 1997. This instruction set reused 64-bit registers from the floating point unit, so both could not be used together. Two years later, in 1999, intel introduced the first version of SSE, which used new 128-bit registers.  The MMX and SSE instruction sets made it feasible to process audio on the CPU and  soon eliminated the market of higher-end sound cards with DSPs.  The SSE instruction set continued to evolve for several generations and also adopted support for floating point computing. It became essential to get the full performance out of Intel  CPUs in scientific codes. The various editions of the SSE instruction set where superseded by the AVX and later AVX2 instruction sets, both of which use 256-bit registers, defining vector operations working on 4 double precision or 8 single precision floating point number simultaneously. Maybe less known is that Intel's current vector instruction set for scientific computing,  AVX512, which as the name implies uses 512-bit registers that can hold 8 double precision or 16 single precision floating point numbers, has its origin in a failed project to build a GPU with a simplified x86 architecture, code-named Larrabee. The Larrabee design was recycled as the Xeon Phi, a chip for supercomputing meant to compete with the NVIDIA GPUs, and in a  second generation Xeon Phi product the instruction set was thoroughly revised to become the AVX512 instruction set which is the first x86 vector extension with good support for scatter and gather operations and predication.</p> <p>Another interesting processor design is the ARM-based Fujitsu A64fx which is used in the  Japanese Fugaku supercomputer which held the crown of fastest computer in the world from June 2020 until June 2022 when it was finally surpassed by the Frontier supercomputer. The A64fx processor was built specifically for supercomputers. Together with ARM, a new vector extension to the ARM instruction set was developed, Scalable Vector Extensions or SVE, which later became an official part of an update of the ARM architecture. The A64fx combines 48 or 52 cores on a chip with a very high bandwidth but relatively small memory system that uses the same technology as the supercomputer GPUs of NVIDIA. It could be used to build supercomputers that were not only as fast as GPU-based systems, but also almost as power-efficient, and performance was good even on some applications that are less suited for vectorisation but also don't run very good on traditional CPUs due to the lack of memory bandwidth in the latter.</p> <p>Matrix accelerators, although fairly new in the market, are also already starting to influence CPU instruction sets.  IBM has added matrix instructions for both AI and linear algebra (the latter requiring  single and double precision floating point) to the POWER10 processor. Intel has already some instructions in some CPUs but only for low-precision inference, but will add a new instruction set extension called AMX in Sapphire Rapids, a server CPU that should come out in late 2022 or early 2023. It is still only meant for AI applications, supporting 4 and 8-bit integers and Googles bfloat16 data format. Similarly the ARM V9-A instruction set adds Scalable Matrix Extensions to the architecture. As is the case for Intel, these extensions are also aimed at AI applications, supporting  8-bit integer and bfloat16 data formats.</p> <p>Though these CPU instructions certainly don't make CPUs so fast that they beat GPUs, they also have two advantages over accelerators: there is no need to pass control to a remote processor, which can save some time, and there are no issues with data needing to be moved. Also, one should not forget that a single GPU card for a supercomputer easily costs three times or more as much as a single CPU socket with memory (and even more if the lower end SKUs in the CPU line are used), so a CPU doesn't need to be as fast as a GPU to be the most economical solution.</p>","tags":["vector instructions","matrix instructions"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/","title":"Accelerator programming","text":"<p>We will restrict ourselves to GPGPU programming as these are currently the most widely used accelerators in supercomputers.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#the-current-state","title":"The current state","text":"<p>Accelerator programming is, also due to the early stage of the technology, still a mess, and standardisation still has to set in. This is not uncommon in the world of supercomputing: The same happened with message passing where there were several proprietary technologies until the needs were sufficiently well understood to come to a standardisation.</p> <p>Currently there are three competing ecosystems:</p> <ol> <li> <p>NVIDIA was the first to market with a fairly complete ecosystem for GPGPU     programming. They chose to make large parts of their technology proprietary     to create a vendor lock-in and keep hardware prices high.</p> <p>CUDA is the basis of the NVIDIA ecosystem. NVIDIA together with partners also  created OpenACC,a set of compiler pragma's for C/C++ and Fortran for GPGPU  programming, on paper managed by an open consortium, but many other compiler vendors are hesitant to pick those up.</p> <p>The NVIDIA toolset also offers some support for open, broadly standardised technologies, but the support is usually inferior to that for their proprietary technologies or standards that are in practice largely controlled by them.</p> </li> <li> <p>AMD is a latecomer to the market. Their software stack is called ROCm, and they     open sourced all components to gain more traction in the market. They basically     focus on two technologies that we will discuss: HIP and OpenMP.</p> </li> <li> <p>Intel calls its ecosystem oneAPI, as it tries to unify CPU and GPGPU programming,     with libraries that make it easy to switch between a CPU-only version and a      GPGPU-accelerated version of an application.</p> <p>The oneAPI ecosystem is largely based on Data Parallel C++, an extension of SYCL, and OpenMP compiler pragmas. </p> <p>OneAPI is partly open-sourced. In some cases, only the API is available and  vendors should make their own implementation, but, e.g., the sources for their clang/LLVM based Data Parallel C++ compiler are available to all and this has been used already to make ports to NVIDIA and AMD GPU hardware.</p> </li> </ol>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#lower-level-models","title":"Lower-level models","text":"<p>These models use separate code for the host and the accelerator devices that are then linked together to create the application binary.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#cuda","title":"CUDA","text":"<p>CUDA is the best known environment in the HPC world. It is however a proprietary NVIDIA technology. It is possible to write GPGPU kernels using subsets of C, C++ and Fortran. CUDA also comes with many libraries with optimised routines for  linear algebra, FFT, neural networks, etc. This makes it by far the most extensive of the lower-level models. </p> <p>CUDA, which launched in 2007, has gone through a rapid evolution though it is now reaching a level of maturity with less frequent major updates.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#hip","title":"HIP","text":"<p>HIP or Heterogeneous Computing Interface for Portability is AMD's  alternative to CUDA. It tries to be as close to CUDA as legally possible and technically achievable on the AMD GPU architecture. The HIP API maps one-to-one on the CUDA API, making it possible to recompile HIP code with the CUDA tools using just a header file. In theory this process is without loss of performance, but there is one caveat: A kernel that is efficient on an AMD GPU may not be the most efficient option on a NVIDIA GPU as, e.g., the vector size is different and memory sizes for the on-chip memory are also different.  So careful programming is needed to ensure efficiency on both platforms.</p> <p>HIP is purely based on C++ with currently no support for Fortran GPU kernels.  Feature-wise it is roughly comparable to CUDA 7 or 8, but features that cannot yet be supported by AMD hardware are missing. The ROCm platform also comes with a lot of libraries that provide APIs that are also similar to the APIs provided by their CUDA  counterpart to further ease porting code from the CUDA ecosystem to the AMD ROCm ecosystem.</p> <p>HIP also comes with two tools that help in reworking CUDA code into HIP code, though  both typically need some manual intervention.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#opencl","title":"OpenCL","text":"<p>OpenCL or Open Compute Language is a framework for heterogeneous computing developed by the non-profit, member-driven technology consortium Khronos Group which manages many standards for GPU software (including also OpenGL and its offsprings and Vulkan). It develops vendor-neutral standards.</p> <p>OpenCL is a C and C++-based technology to develop code that not only runs on GPU accelerators, but the code can also be recompiled for CPU only, and some DSPs and FPGAs are also supported. This makes it a very versatile standard and it was for a long time a very attractive standard for commercial software development. It is by far not as advanced as CUDA or not even HIP.  The evolution of the standard has been very slow lately, with vendors hardly picking up  many of the features of the 2.x versions. This led to version 3.0 of the standard that  defined a clear baseline and made the other features explicitly optional so that programmers at least know what they can expect. </p> <p>OpenCL is in principle supported on NVIDIA, AMD and Intel hardware, but the quality of the implementation is not always spectacular. There are also some open source implementations with varying hardware support. It has been used in supercomputing software though it was not  always the only model offered as several packages also contained specialised CUDA code for better  performance on NVIDIA hardware. The molecular dynamics package GROMACS is one example, and they will be switching away from OpenCL to newer technologies to support  non-NVIDIA hardware.</p> <p>OpenCL is largely superseded by newer technologies developed by the Khronos Group. Vulkan is their effort to create an API that unifies 3D graphics and computing and is mostly oriented towards game programming etc., but less towards supercomputing (as several GPUs used for supercomputing even start omitting graphics-specific circuits) while SYCL, which we will  discuss later in these notes, is a new initiative for GPGPU programming.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#compiler-directives","title":"Compiler directives","text":"<p>In these models, there are no separate sources for the GPU that are compiled with a separate compiler. Instead, there is a single base of code with compiler directives instructing the compiler how certain parts of the code can be offloaded to an accelerator. These directives appear as comments in a Fortran code to compilers that do not support the technology or use the <code>#pragma</code> directive in C/C++ to make clear that they are directives to compilers that support those.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#openacc","title":"OpenACC","text":"<p>OpenACC was the first successful compiler directive model for GPGPU computing. It supports C, C++ and Fortran programming.</p> <p>The first version of standard made its debut at the supercomputing computing conference SC'11 in November 2011. The technology was largely developed by 4 companies: the GPU company NVIDIA, the compiler company Portland Group which was later renamed PGI and later bought by NVIDIA, the supercomputer manufacturer Cray which got bought by HPE and largely lost interest in  OpenACC, now more promoting an alternative, and CAPS Entreprises, a French company and university spin-off creating tools for accelerator programming that failed in 2013. </p> <p>The OpenACC standard is currently controlled by the OpenACC Organization, and though it does count other companies that develop GPUs among its members, it still seems rather dominated by NVIDIA which may explain why other companies are somewhat hesitant to pick up the standard. The standard is at the time of writing of this section at version 3.3, released in November 2022, but is updated almost annually in November (there was no update in 2023).</p> <p>OpenACC is well supported on NVIDIA hardware through the NVIDIA HPC compilers (which is the  new name of the PGI compilers adopted after the integration of PGI in NVIDIA). GCC offers some  support on NVIDIA and some AMD hardware for version 2.6 of the standard (the November 2017 version) since version 10 of the GCC compilers, but the evolution is slow and performance is often not that great. More promising for the future is the work going on in the clang/LLVM community to support OpenACC, as this effort is largely driven by the USA national labs who want to avoid having to port all OpenACC-based code developed for the NVIDIA based pre-exascale supercomputers to other programming models to run on the new AMD and Intel based exascale supercomputers. In fact, the clang/LLVM ecosystem is the future for scientific computing and not the GNU Compiler Collection ecosystem as most compiler vendors already base their compilers on that technology. The NVIDIA, AMD and new Intel compilers are all based on LLVM and the clang frontend for C and C++-support, with NVIDIA and AMD still using a Fortran front-end developed by PGI and donated to the LLVM project while Intel is using its own front-end.  There is also a new community-developed modern front-end for Fortran but it is not yet ready for prime time as the optimisation is not yet that great. OpenACC support in the LLVM ecosystem will build upon the OpenMP support, the technology that we will discuss next, using extensions for those OpenACC features that still have no equivalent in OpenMP. However, as of version 17 (early fall 2023) this support is still very incomplete and experimental.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#openmp","title":"OpenMP","text":"<p>We've already discussed OpenMP as a technology for shared memory parallelism and for  vectorisation (the latter since version 4.0 of the standard). But OpenMP is nowadays even  more versatile. Since version 4.0 of the standard, released in July 2013, there is also support for offloading to accelerators. That support was greatly improved in  version 5.0 of the standard which was released at the SC'18 supercomputer conference. It became a more descriptive and less prescriptive model (offering the compiler enough information to decide what it should do rather then enforcing the compiler to do something in a particular way), with the prescriptive nature being criticised a lot by the OpenACC community who claimed superiority because of this. It also contained much better support for debuggers, performance monitoring tools, etc.</p> <p>OpenMP has since had minor extensions i the form of version 5.1 at SC'20 and 5.2 at SC'21. The standard is controlled by a much larger consortium than the OpenACC standard.</p> <p>OpenMP is an important technology in the AMD ROCm and Intel oneAPI ecosystems. Intel has in fact supported OpenMP offload to some of its own GPUs for many years, long before establishing the oneAPI ecosystem. GCC has had some support for OpenMP offload to NVIDIA hardware since version 7 and to  some AMD hardware since version 10. However, as is the case for OpenACC, it may not be the most performant option on the market. The clang/LLVM ecosystem is working hard for full support of the newest OpenMP standards. The AMD and new oneAPI Intel compilers are in fact fully based on clang and LLVM using some of their own plug-ins to offer additional features, and the NVIDIA  HPC compiler also largely seems to be based on this technology. NVIDIA and AMD also use the LLVM backend to compile CUDA and HIP kernels respectively, showing once more that this is the compiler ecosystem for the future of scientific computing.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#c-extensions","title":"C++ extensions","text":"","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#sycl","title":"SYCL","text":"<p>SYCL is a programming model based on recent versions of the C++ standard.  The earlier drafts of the standard go back to the 2014-2015 time frame, but SYCL really took off with the 2020 version of the standard. SYCL is also a logical successor to OpenCL, but now making it possible to target CPU, GPU, FPGA and possibly other types of accelerators from a single code base. Though the programmer is of course free to provide alternative implementations for different hardware for better performance, it is not needed by design. SYCL is heavily based on C++ template libraries, but to generate code for accelerators it still needs a specific compiler.</p> <p>There are several compilers in development with varying levels of support for the standard, targeting not only GPUs, but also, e.g., the NEC SX Aurora Tsubasa vector boards.  Most if not all of these implementations are again based on Clang and LLVM. One implementation worth mentioning is AdaptiveCpp. This project started under the name hipSYCL, which as its name suggests targets HIP for the backend to support AMD CPUs, but it has been extended to support all three major GPU families now using ptx for NVIDIA, amdgcn code for AMD and  SPIR-V for Intel GPUs, and the project is also working on multi-CPU support.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#dpc-or-data-parallel-c","title":"DPC++ or Data Parallel C++","text":"<p>Data Parallel C++ is the oneAPI implementation of SYCL. It is basically a project by Intel to bring SYCL into LLVM, and all code is open sourced. The implementation does not only cover CPUs and Intel GPUs, but with the help of others (including the  company Codeplay that has since been acquired by Intel) it also adds support for  NVIDIA and AMD GPUs and even Intel FPGAs, the latter through a backend based on OpenCL and SPIR. That support is not included in the binaries that Intel distributes though.</p> <p>When DPC++ initially launched, it was really an extension of the then-current SYCL standard, which is why it gets a separate subsection in these notes.  However, it is currently promoted as a SYCL 2020 implementation.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#camp","title":"C++AMP","text":"<p>C++AMP or C++ Accelerated Massive Parallelism is a programming model developed by Microsoft. It consists of C++ libraries and a minor extension to the language. There are experimental implementations for non-Microsoft environments, but these are not really popular and the technology is not really taking off in scientific computing.  The technology is now deprecated by Microsoft. Yet we want to mention it in these notes as it was a source of inspiration for SYCL.</p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#frameworks","title":"Frameworks","text":"<p>There exist also several C++ frameworks or abstraction libraries that support creating code that is portable to regular CPUs and GPU systems of various vendors. They let you exploit all levels of parallelism in a supercomputer except distributed  computing.</p> <p>Kokkos is a framework developed by Sandia National Labs and probably the most popular one of the frameworks mentioned here. It was first released in 2011 already but grew to a complete ecosystem with tools to support debugging, profiling and tuning also, and now even some support for distributed computing also. Kokkos already supports backends for CUDA and ROCm, and there are experimental backends (based on SYCL and OpenMP offload)  that can also support the Intel GPUs that will be used in the Aurora supercomputer.</p> <p>RAJA  is a framework developed at Lawrence Livermore National Laboratory, based on standard C++11.  Just as Kokkos, RAJA has several backends supporting SIMD, threading through the TBB library or OpenMP, but also GPU computing through NVIDIA CUDA, AMD HIP, SYCL and OpenMP offload, though not all  back-ends are as mature or support all features. In particular the SIMD, TBB, SYCL and OpenMP target offload  (the latter needed for Intel GPUs) are still experimental at the time this section was last revised (March 2023).</p> <p>Alpaka  is a framework developed by CASUS - Center for Advanced Systems Understanding of the  Helmholtz Zentrum Dresden Rossendorf. Alpaka also supports various backends, including a CUDA back-end for NVIDIA GPUs. There is also work going on on a HIP backend for AMD GPUs, with support for Intel GPUs coming through a SYCL and an OpenMP offloac backend. </p>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S04_Programming_accelerators/#libraries","title":"Libraries","text":"<p>One can often rely on already existing libraries when developing software for GPUs.</p> <p>NVIDIA CUDA comes with a wide range of libraries.</p> <p>AMD ROCm provides several libraries that mimic (subsets of) libraries in the CUDA ecosystem, also easing porting from NVIDIA to AMD hardware.</p> <p>Intel has adapted several of its CPU libraries to use GPU acceleration also in its oneAPI platform.</p> <p>However, there are also several vendor-neutral libraries, e.g.,</p> <ul> <li> <p>MAGMA which stands for Matrix Algebra on GPU and Multicore Architectures, which is an early      example of such a library.</p> </li> <li> <p>heFFTe is a library for FFT </p> </li> </ul>","tags":["CUDA","ROCm","HIP","OpenMP","OpenACC","SYCL","oneAPI","OpenCL","Kokkos","RAJA","Alpaka"]},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/","title":"Status of GPU computing","text":""},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#too-much-hype","title":"Too much hype","text":"<p>Even though GPU computing, or accelerator computing in general, is definitely here to stay and important for the future of not only supercomputing but computing in general as  what can be done with a given amount of power is important in many markets (also on mobiles that have to get their power from a battery), it does not mean that it works for all applications. Benchmarking of supercomputers with accelerators is often more benchmarketing. GPU computing is often overhyped using incomplete benchmarks (basically only benchmark that part of the code that can be properly accelerated), marketing by numbers (redefine common terms to get bigger numbers, something that in particular NVIDIA is very good at) and comparing apples and oranges by comparing systems with a very different price or total cost of ownership, e.g., comparing a server with multiple accelerators costing 60k EURO or more with a standard dual socket server costing only 10k EURO and using  only one fifth of the power (these prices being what one actually paid for such nodes  in 2022).</p> <p>The NVIDIA vendor lock-in and its success in the market have made accelerators very  expensive. At the current price point, GPU computing only makes sense from a price point of view if the speed-up at the application level is a factor of 2.5 or more per accelerator card compared to a standard medium-sized dual socket node. Due to the overdemand due to the explosion of AI, the situation is currently getting even worse as the prices of GPU cards rise faster than those of CPUs, so this may soon become a factor of 4 or 5 that you need to gain.</p> <p>As we have seen, some features of accelerators have been carried over to traditional CPUs  in the form of new instructions supporting vector and nowadays even matrix computing, and  in some cases they may just be the better choice as CPUs have more memory readily available and as programming is easier.</p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#problems-and-solutions","title":"Problems and solutions","text":"<p>There are several problems with current GPU designs:</p> <ol> <li> <p>The amount of memory that a GPU can address directly and has fast enough access to,     is limited. 2020 GPUs were limited to 32-48 GB of memory, in early 2021 a 80 GB GPU     appeared on the market, and in 2022 it is technically possible to have 128 GB on     a single package. But this is still small to typical memory sizes on a regular dual     socket server that is much slower than the GPU.</p> </li> <li> <p>Programming bottleneck: Having to organise all data transport manually and working with     separate memory spaces is a pain.</p> </li> <li> <p>The link between the CPU and the GPU is a bottleneck. The PCIe buss that typically links the CPU to the GPU has      a rather limited bandwidth compared to either the bandwidth of the CPU memory of the bandwidth     of GPU memory. </p> </li> <li> <p>The GPU is rather slow for serial code, so that code often has to run on the host.     Which is then an issue since it may require additional copying between the CPU and GPU.</p> </li> </ol> <p>However, there is hope.</p> <p>The amount of memory that can be used by a GPU will increase a lot the coming years. Both the memory packages are getting bigger by stacking more dies in 3D, and the number of memory packages that can be integrated in the overall GPU package is increasing. As of 2022, 8 memory stacks in a GPU package is feasible as is shown by the AMD MI250(X) GPUs, while in 2023-2024 12 memory stacks in a single GPU package should become a possibility. The size of the stacks is also increasing from 16GB to 24GB (available very soon) or 32GB. The two combined may make memory sizes of 192  and likely even 384 GB possible by 2024 or 2025.</p> <p>The programming bottleneck can already be partially solved by unified memory, using memory pointers that work on both CPU and GPU, and further hardware support for virtual memory that can then trigger software that migrates memory pages under the hood. NVIDIA GPUs have had some of those features since the Pascal generation in 2017. However, one can do even better by physically sharing memory spaces between GPU and CPU, and supporting some level of coherency so that the CPU can access the GPU memory  without risk of inconsistent data, or even the GPU can access the memory of the CPU, though that is less interesting as the CPU can never provide the memory bandwidth that the GPU needs to perform well.  NUMA style shared memory spaces were first explored in the Sierra and Summit USA pre-exascale systems (as we discussed before) but is now also seen in the MI250X GPU from AMD which is a special  version of the MI200 family connecting to the CPU through InfinityFabric, the same interconnect that AMD  uses internally in its sockets to link the CPU dies to the I/O die, and also uses to connect CPU sockets or to connect GPUs to each other in the MI100 and MI200 generations.  The Intel Ponte Vecchio GPU combined with the Sapphire Rapids CPU that will be used in the Aurora supercomputer supports a similar feature, as does the NVIDIA Grace CPU and Hopper GPU integrated in a single package.</p> <p>The physical sharing of memory spaces with a level of cache coherency is also the first step in solving the problem of copying data back and forth all the time. E.g., if a CPU can access memory attached to the  GPU without risks of coherency problems, then there is less need to copy full memory pages and also not to  copy those back, as the link that is used in those GPU systems to connect the CPU to GPU is as fast as the links that are typically used to connect two CPU sockets. The NVIDIA Grace Hopper \"superchip\" shows the next  step. By integrating the CPU and GPU in the same package, it is possible to have a much higher bandwidth between both, reducing the copying bottleneck in cases where copying is still needed.  However, with the AMD MI300A, and without doubt future Intel and NVIDA chips,  we will see even closer integration where CPU and GPU chiplets share the memory controllers. The Apple M series chips give an indication of what can be obtained with such a system, as  these systems perform way better in some applications that use acceleration than one would expect from looking at systems with discrete GPUs with similar theoretical performance. This can solve both the  third and fourth issue mentioned above.</p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#evolution-of-gpu-nodes","title":"Evolution of GPU nodes","text":"<p>We will now discuss this evolution in some more detail.</p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#gpu-subsystem-connected-via-pcie","title":"GPU subsystem connected via PCIe","text":"<p>Around 2016, a typical GPU compute node consisted of a dual socket server with 1-4 GPUs attached to the CPUs. A typical design would have been:</p> <p></p> <p>The red line between the two CPUs denotes a fully cache coherent link between the CPUs.  In 2016 this would very likely have been either Intel CPUs or IBM POWER CPUs, and both had proprietary fully cache coherent links to link CPUs in a shared memory system. The red link between the GPUs denotes a similarly proprietary connection between the GPUs for easy data transfer between the GPUs at typically a much higher bandwidth than that offered by the connections between the CPU and GPU. However, not all systems used such a link between graphics cards. A CPU was connected to the GPUs using PCIe, and similarly a network interface would also be connected to a CPU using PCIe.</p> <p>A typical 4-GPU node based on the NVIDIA Ampere A100 GPU launched in 2020 would look similar to:</p> <p></p> <p>There are many variants of quad GPU designs with the A100 GPU, with single and dual socket CPU servers. However, as some 2020-era CPUs didn't have enough PCIe lanes to connect 4 GPUs, 2 network cards and in some cases also some local fast SSDs on two sockets (let alone a single CPU socket), a different solution was needed.  The above design solves this by using two PCIe switches (the magenta circles), and each PCIe switch connects the CPU to two of the GPUs.  In the above design two network cards are used per node, one connected to each of the CPU sockets. Some variants of the design will connect the network  cards and/or NVMe SSDs (a type of SSD that uses the PCIe interface for fast data transfer) also to the switches for better direct data transfer between  the interconnect and the GPUs and/or the SSDs and the GPUs.</p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#cache-coherent-interconnect-between-cpu-and-gpu","title":"Cache-coherent interconnect between CPU and GPU","text":"<p>The next evolution of this design is used in the USA pre-exascale systems Sierra and Summit, both using IBM POWER9 CPUs and NVIDIA Volta V100 GPUs. More recently, the idea is revived in the  GPU compute nodes of the Frontier and LUMI supercomputers based on the MI250X GPU. A simplified diagram of the LUMI and Frontier GPU nodes is:</p> <p></p> <p>The GPU compute nodes of LUMI and Frontier use a special variant of the Zen3-based AMD Epyc processor. In this variant, the PCIe lanes are replaced by Infinity Fabric connections. In an MI250X, the 4 GPU packages are connected with each other and with  the CPU through Infinity Fabric links. Each GPU package connects to its own quarter of the AMD Epyc processor (remember from earlier that the I/O die is subdivided in 4 quarters, in this case each connected to two CPU chiplets with 8 cores each). This creates a unified memory space with some level of coherency.  Also noteworthy is that the interconnect is no longer connected to the CPU, but the 4 network cards are each connected to a GPU package (through a PCIe interface). This makes this compute node really a GPU-first system, almost a system where the CPU is only used for those parts of the code that cannot be accelerated at all by a GPU and to run the  Linux operating system.</p> <p>The true picture of the MI250X GPU node is a bit more complicated though. Each GPU package contains two GPU dies, and these are connected to each other through some Infinity Fabric links. Each GPU die connects to 4 memory packages, with 64 GB of memory per GPU die. However, the connection between two GPU dies is sufficiently bandwidth-starved that programming-wise a single GPU package should be considered as two separate GPUs.  Each individual GPU die has its own InfinityFabric link to the CPU and seems to have a preferential CPU chiplet. Even though the connection between the GPU packages appears to  be an all-to-all connection, this is not true when one looks at the connections between the GPU dies. </p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#integration-of-cpu-and-gpu-in-a-single-package-nvidia-gh100","title":"Integration of CPU and GPU in a single package: NVIDIA GH100","text":"<p>Even though the MI250X has a level of cache coherency, using the memory in a unified matter is still a problem, partly because of the extra latency introduced by the fairly long distance between the CPU and GPU, partly also because of the limited memory bandwidth on the CPU side. NVIDIA's Grace Hopper Superchip, expected in 2023, works on those two problems by integrating the CPU and GPU on a single package and not only putting the GPU memory, but also the CPU memory on that package.  The CPU part is called Grace and is a CPU based on the ARMv9 architecture, the newest version of the ARM architecture at the time of design of the Grace processor. The GPU part of the package is the Hopper architecture and similar to the one in the SXM and PCIe cards released in the fall of 2022, but with all 6 memory controllers enabled.</p> <p>The CPU and GPU still both have their own memory controllers and basically behave as separate NUMA domains, but as the connection between the two has been brought on-chip the bandwidth between CPU and GPU is a lot higher than in the MI250X architecture OR the Summit and Sierra systems with IBM POWER9 and NVIDIA V100 chips. The CPU memory is not provided through external DIMMs, but through a number of internal  LPDDR5X modules integrated in the CPU-GPU package in a similar way as the GPU memory has been integrated in the package for a number of generations already.  Integration of this memory type is popular in smartphones where it saves both space and power, and is also used in the Apple Silicon M-series chips, where in addition to space and power savings it also provides higher bandwidth. In the Grace chip it  enables a very high memory bandwidth for the CPU, even 20% better than what the  AMD EPYC 4 generation offers per socket (but not as much as the Intel Sapphire Rapids MAX chips that incorporate a small amount of GPU-style memory in the package) while  still offering a high memory capacity. Pre-release material claims up to 512 GB per CPU,  but this number has to be taken with a grain of salt as it does not correspond to the available memory modules that can be found in catalogues of memory providers in early 2023.</p> <p>The Grace Hopper superchip provides two types of external connections. There are a number of regular PCIe connections that come from the CPU die. They can be used to attach, e.g., network cards (including a HPC interconnect) and NVMe drives. There are also a number of NVLINK connections coming from the GPU die. These connections are similar to the ones already used in previous generation NVIDIA GPUs to link GPU packages and offer a much higher bandwidth interconnect. They can be used to interconnect a number of Grace Hopper superchips in a NUMA shared memory way.</p> <p>This makes the Grace Hopper superchip a very flexible building block. Depending on the needs those chips can be combined in different ways.  One can connect individual single package nodes through any HPC interconnect in a distributed memory way.  It is also possible to use NVLINK technology to connect multiple superchips into a single system where each CPU and each GPU appears as a NUMA node to the OS.  The bandwidth of this connection is much higher than the typical inter-socket interconnect in CPU-based servers, but still a lot lower than the memory bandwidth that the GPU memory system can offer. So it is very important that applications exploit the NUMA structure of the memory. It is also possible to combine both approaches: Build supercomputer nodes with up to 8 superchips with a single NVLINK switch level, and link those nodes together using a traditional HPC interconnect. A possible layout of such a node  is shown in the following figure:</p> <p></p> <p>In this figure we combine 4 Grace Hopper packages in a single node and have chose to connect each package directly to the interconnect for additional bandwidth.</p>"},{"location":"C08_Accelerators/C08_S05_Status_GPU_computing/#fully-unified-cpu-and-gpu-amd-mi300a","title":"Fully unified CPU and GPU: AMD MI300A","text":"<p>The AMD MI250X is really just a transition to the MI300 \"Antares\" series,  that goes one step further beyond the integration that the NVIDIA Grace Hopper architecture offers.  In that generation, announced in December 2023,  one of the variants (the MI300A) merges the CPU and GPU completely, as CPU cores and GPU Compute Units are integrated in a single package and share  the memory controllers and on-package memory. In fact, the reality is that memory outside the package is also starting to limit CPU performance as an increasing number of CPU codes becomes memory bandwidth bound, so even for the CPU it makes sense to switch to smaller but much higher bandwidth memory in the package. </p> <p>The AMD MI300A fully integrates the CPU and GPU chiplets and memory controllers with memory in a single package.  Whereas the MI250x has some level of cache coherency but still needs to rely on page transfers in most cases, in the MI300A the CPU and GPU memory is fully unified (physical and virtual), with both sharing the same memory controllers and memory, which will enable to fully eliminate redundant memory copies at least when going between CPU and GPU in the same package. The MI300A was first mentioned at the AMD Financial Analyst Day in June 2022 and at  CES'2023 (early January 2023), where a full package was shown, but still with very little detail. It was launched in AMD's AI event on December 6, 2023. The MI300A  combines 24 zen4 CPU cores with 228 CDNA3 GPU Compute Units in a single package. The package contains 13 chiplets stacked into two layers.  The bottom layer contains 4 chiplets with the memory controllers and  last level cache. These 4 chiplets connect to 8 HBM3 memory modules  offering a total of 128 GB of memory with a bandwidth of 5.3 TB/s. On top of this are 6 GPU chiplets with 38 Compute Units each, and 3 CPU chiplets with 8 zen4 cores each. The bandwidth between these 13 chiplets is much higher than between the MI250X chiplets, enabling the whole package to function as a single GPU.</p> <p>The MI300A will be used as the basis of the El Capitan exascale supercomputer installed at Lawrence Livermore National Laboratory in the USA.  The basic design of the node is shown in the following picture:</p> <p></p> <p>Here we see four MI300A packages. These four packages have an all-to-all connection using a new generation of InfinityFabric, and each GPU packages also connects to a network card using PCIe.</p> <p>The total memory capacity of such a node is rather limited.  It is not clear why AMD chose to use the 16 GB memory packages rather than the  24 GB ones (which would have resulted in 192 GB of memory per package),  as the latter are used in the MI300X which is the discrete, PCIe-attached version of the MI300 GPUs. It may be for cost reasons, or simply because the verification and production of MI300A started a few months before MI300X and the 24 GB packages may not have been available yet. But for applications that don't need those large memory capacities and scale nicely over NUMA domains and then further over distributed memory nodes, the more uniform architecture will certainly make life easier and offer great performance benefits.</p> <p>In Europe, HLRS has ordered a small system based on MI300A GPUs to prepare for a bigger exascale system using a future AMD GPU later in the decade.</p> <p>Intel was also working on a similar design, code-named Falcon Shores that was intended to hit the market in 2024, but in an announcement in March 2023 it was said that the chip was postponed till 2025, and the announcement also mentioned it as a GPU, explicitly scrapping the XPU term they used before.</p>"},{"location":"C08_Accelerators/C08_S06_Further_reading/","title":"Further reading on accelerators","text":"<ul> <li> <p>AMD GPUs</p> <ul> <li> <p>Whitepaper: Introducing AMD CDNA<sup>TM</sup> 2 Architecture     which includes information on how the MI250X connects to the CPU, and on the vector and matrix cores of the GPU.</p> </li> <li> <p>HPCwire article on the AMD MI300 APU     that will be used in the El Capitan supercomputer.</p> </li> <li> <p>AnandTech article on the MI300,     based on information from AMD's 2022 Investors Day. The article shows some not very detailed slides about the memory architecture.</p> </li> <li> <p>GPU comuting at the AMD Financial Analyst Day 2022 on YouTube (not an official AMD recording). The CDNA3 part starts at 1:07:50.</p> </li> <li> <p>MI300 @ AMD CES'2023 keynote (YouTube). The MI300 part starts at 1:29:42.</p> </li> </ul> </li> <li> <p>NVIDIA GPUs</p> <ul> <li> <p>Whitepaper: Summit and Sierra Supercomputers: An Inside Look at the U.S. Department of Energy\u2019s New Pre-Exascale Systems      for information on the physically unified memory space made possible by the NVLink connections     between the CPUs and GPUs. </p> </li> <li> <p>Whitepaper: NVIDIA Grace Hopper Superchip Architecture      for all information on the combination of the Grace ARM-based CPU and the Hopper GPUs.</p> </li> </ul> </li> <li> <p>NVIDIA CUDA</p> <ul> <li>NVIDIA CUDA toolkit</li> </ul> </li> <li> <p>AMD ROCm</p> <ul> <li> <p>AMD ROCm information portal</p> </li> <li> <p>AMD HIP fundamentals</p> </li> <li> <p>HIP API Guides</p> </li> </ul> </li> <li> <p>Intel oneAPI</p> <ul> <li>Intel oneAPI overview</li> </ul> </li> <li> <p>OpenCL</p> <ul> <li>Khronos Group OpenCL documentation</li> </ul> </li> <li> <p>OpenACC</p> <ul> <li> <p>The OpenACC Organization</p> </li> <li> <p>Clacc, OpenACC in LLVM project page</p> </li> </ul> </li> <li> <p>OpenMP</p> <ul> <li>OpenMP Architecture Review Board web site</li> </ul> </li> <li> <p>SYCL</p> <ul> <li> <p>Khronos Group SYCL documentation</p> </li> <li> <p>Open SYCL (formerly hipSYCL)</p> </li> <li> <p>Intel oneAPI DPC++/C++ compiler</p> </li> <li> <p>Intel oneAPI DPC++ compiler GitHub</p> </li> </ul> </li> <li> <p>Kokkos</p> <ul> <li>Kokkos ecosystem home page</li> </ul> </li> <li> <p>RAJA framework</p> <ul> <li> <p>RAJA home page</p> </li> <li> <p>RAJA documentation</p> </li> </ul> </li> <li> <p>Alpaka abstraction library</p> <ul> <li> <p>Alpaka information on the CASUS web site</p> </li> <li> <p>Alpaka information on GitHub</p> </li> <li> <p>Alpaka training from 2020 on YouTube</p> </li> </ul> </li> <li> <p>Libraries</p> <ul> <li> <p>MAGMA</p> </li> <li> <p>heFFTe</p> </li> </ul> </li> </ul>","tags":["CUDA","ROCm","HIP","oneAPI","OpenCL","OpenACC","OpenMP","SYCL","Kokkos","RAJA"]},{"location":"C09_Conclusions/","title":"Conclusions","text":"","tags":["Julia"]},{"location":"C09_Conclusions/#cost-concious-computing","title":"Cost-concious computing","text":"<p>Supercomputing does not come cheap. Some really big runs run on bigger supercomputers may easily cost 10k EURO. Academic users typically don't see the full bill. They either pay a minor contribution, or have to submit a proposal to get the compute time, but someone has to pay the bill. Given the high cost, it is clear that a supercomputer should be used efficiently.</p> <p>Unfortunately, using a supercomputer efficiently is not that trivial. But as we have seen, even your work that you do on a regular PC or workstation may benefit from it as after all, a modern PC has all the  levels of parallelism in a supercomputer with the exception of the distributed memory level and probably the NUMA characteristic of the memory.</p> <p>It is important to realise that a supercomputer is not a machine that wil automatically and in a magical way run every program faster than a PC, though at first it may look like there is a high degree of compatibility with Linux PCs. (And that even works in the other direction: it is not hard to install some of the middleware used on a supercomputer also on your PC to, e.g., test distributed memory programs.) A supercomputer is no excuse for lousy programming. One needs to first optimise a program and then, if the cheaper hardware is still not capable of running it properly, move to a (larger) supercomputer, and not the  other way around, first move to a supercomputer and then if it still doesn't work think about developing better code.</p> <p>This becomes even more important as we can no longer rely on a rapid  improvement of performance at a constant budget. To be able to advance research, it will only become more important to use all available computing resources properly.</p>","tags":["Julia"]},{"location":"C09_Conclusions/#cost-concious-computing-as-a-software-user","title":"Cost-concious computing as a software user","text":"<p>It is important to select the software that you use with care and to follow the evolutions in your field. Hardware evolves, and some software packages evolve with the hardware while others stay behind. EuroHPC, the European initiative to make large supercomputers available at an international level, and the USA funding agencies and in particular the national labs, invest a lot of resources into improving or rewriting popular scientific software packages, and these are often available as open source or free to license for academic research. The most hyped package or technology may not always be the best one for your needs. But the package that your Ph.D. advisor used 25 years ago for their research  may not be adequate anymore either.</p> <p>It is also important to learn to use the software packages efficiently, with a balance between execution efficiency and time-to-solution. Most packages, and this is even more true for the free ones, have no auto-tune facility. Users need to do that work instead, which implies that you also need some understanding of the computer that you are using. For a numerical simulation it is also important to understand the limits of the models (accuracy-wise) and of the numerics. Asking for more precision than makes sense given the errors in the model and the stability of the numerical algorithms may come at a very high cost or even failed runs.</p>","tags":["Julia"]},{"location":"C09_Conclusions/#cost-concious-computing-as-a-developer","title":"Cost-concious computing as a developer","text":"<p>Prototype languages such as Matlab are really meant for prototyping algorithms but not for production runs at scale, though the situations is somewhat improving with some languages having decent just-in-time compilation features or being more designed for efficient execution on modern hardware than only easy of programming. Julia is a modern language developed at MIT that  offers much of the ease-of-use of Matlab but where being able to compile the  program to efficient computer code was taken into account when designing the language. In fact, some users who have translated existing code to Julia mention  that they hardly lost speed compared to Fortran while their Matlab and Python codes run orders of magnitude faster when rewritten in Julia.</p> <p>The same holds for languages meant for scripting, with Python the most used one in scientific computing. Such languages were never designed for efficiency and for tasks that execute continuously and require a lot of CPU time.  Python can be acceptable when it is used to bind modules together that themselves each are highly optimised C/C++ code, but it is very hard to get pure Python code to work efficiently. There have been several efforts to try to write just-in-time compilers that can increase the efficiency of pure Python code, but none of the efforts has succeeded in making one that is fairly universal. An example that is somewhat popular in scientific computing is Numba, that seems to work well especially with code that uses data structures from NumPy. (Numba is in fact based on the LLVM compiler back-end that was mentioned so often in these nodes.) Here again, Julia can be a nice alternative.</p>","tags":["Julia"]},{"location":"C09_Conclusions/#why-should-you-care","title":"Why should you care?","text":"<p>A typical reaction from a Ph.D. student could be: \"Why should I care? I need to write my Ph.D. as fast as possible, produce papers, and by the way, it's (almost) free, isn't it?\"</p> <p>Resources are not infinite and most clusters run at a fairly high load. On bigger machines, a user typically gets a certain allocation (in terms of core-hours etc.) and can only use that much. On smaller supercomputers, at the level of a university, a fair share policy is often implemented to come to a somewhat reasonable distribution of resources. A fair share policy typically implies that users or groups who have used a lot of computer time recently, will get a lower priority.  Few if no supercomputers will use a first come, first served policy which would allow you to simply dump a lot of work onto the machine and not care at all.</p> <p>Moreover, funding agencies in the end have to pay the bill and they will also look at the outcome of the research done on a supercomputer before deciding to fund the next one. So the user community can better optimise the scientific return and often economic return of a supercomputer to ensure funding for  the next machine. The way in which research can be made usable for companies is becoming more and more important. And for companies, compute time comes at a high cost so they are not often interested in using inefficient procedures to solve their problems.</p> <p>We also live in a time where energy comes with an increasing cost. One should  realise that supercomputers do consume a lot of energy. A supercomputer consuming only 100 kW is still small, and the biggest machines on earth at the moment are closer to 20 or 30 MW. The supercomputer world is looking at many ways to lower the energy consumption. Software that uses hardware more efficiently or uses more efficient hardware (such as accelerators) is often the easiest route to go...  Hardware designers have already developed ways to cool  supercomputers with water that is so warm even when it enters the  computer, that \"free cooling\" can be used to cool the hot water that leaves the computer, i.e., just radiators and fans to pass the air over the radiator. Some centres are experimenting with heat recovery for the purpose of heating their buildings.  The waste heat of LUMI, a European pre-exascale machine installed in Kajaani, Finland, is used to produce heat for the city of Kajaani.</p> <p>Another reaction one sometimes hears is \"But by the time I've reworked my code or rolled out a better application in my research, faster computers will be around so I don't need to care anymore.\"</p> <p>This was to some extent true from roughly 1980 to 2005 if you look at  personal computers. It was as if Intel engineers were optimising  your program while you were having lunch or going to a party, by designing better, faster processors. In those days, code would run considerably faster on a new processor even without recompiling, though recompiling would improve that even more. However, these days are long over. Around 2005, Dennard scaling started to  break down and further increasing the clock speed became very hard. We've moved from 4 MHz in 1980 to  almost 4 GHz in 2005, but from then on evolution slowed down (and took even a step back for a while) and now we are somewhere close to 6 GHz for processors that are really optimised for sequential code and consume a lot of power. It also became harder and harder to get gains from increased instruction level parallelism. Instead of that, we saw growing popularity of vector instructions and the use of multiple cores. 8 to 16 cores is not uncommon anymore in regular PC's, and the first 24-core PC processor was announced in late 2022. So the further speed increases almost exclusively came from other levels of parallelism that require more effort from the programmer: vector instructions and more cores in shared memory computing, and in supercomputing, more nodes in distributed memory.</p> <p>Nowadays even that is slowing down. The number of transistors that can be put on a chip is no longer growing as fast, the power consumption per transistor is no longer decreasing much with every new generation, and the cost of a transistor isn't really decreasing anymore, but in  fact is starting to increase again when moving to a smaller manufacturing process. It looks like the days of further significant performance gains without  growing budgets to match are mostly over.</p> <p>The focus in computing is also more and more on performance per Watt as energy bills become prohibitively high for supercomputer centres and  centres operating large server farms or as portability matters as in  smartphones, where you want more and more performance while the battery capacity isn't increasing that quickly. This implies more but slower cores, so more need to exploit parallelism and less single-thread performance. There are only a few markets that don't care much about this, but those markets are becoming too small to carry the cost of designing faster and  faster processors with limited parallelism.</p>","tags":["Julia"]},{"location":"C09_Conclusions/#software-quality-matters","title":"Software quality matters","text":"<p>We've seen that even though further shrinking dimensions in semiconductor manufacturing remains possible, we are bumping into economical constraints. Basically the cost per gate or transistor for the manufacturing processes used for CPUs and GPU accelerators isn't really decreasing anymore. As a consequence,</p> <ul> <li>We can only expect moderate performance increases for a constant budget, unless     a switch would be made to very different architectures. The switch from CPU to     GPU computing has offered such increase for some applications.</li> <li>We cannot expect that performance per Watt will improve as quickly as it did in     the past, again unless big architectural changes would bring a solution.</li> </ul> <p>Even though microprocessor and GPU manufacturers claim that computers 1000x as fast as current computers will be possible in ten years, it remains to be seen for what applications this will be the case. Statements of that kind are usually misleading. E.g., when going from generation 1 to 2 one saw a twofold speed increase for application A, while going from generation 2 to 3 one saw a twofold speed increase for a very different application B, hence generation 3 is 4 times faster than generation 1. While it is more  likely that both application A and B run only twice as fast.</p> <p>Further speed increases will largely have to come from better software.  This implies that we need an increased focus on both more efficient algorithms and a more efficient implementation of those algorithms. It also implies that we must be willing to adapt to architectural changes in computers and adapt codes, and not program them as if we are still in the '90s. We again need computer languages with a strong focus on performance, preferably combined with attention to programmer productivity also, but can no longer rely on languages that only focus on  programmer productivity.</p> <p>The term \"computational\" in \"computational science\" means more than ever that one pays a lot of attention to the computational aspects also and that there is a willingness to understand what you're doing at the computational level also.  You cannot drive a big truck or a Formula 1 car with just a regular driver's license and no further training as they are very different things that react very differently. Similarly you cannot use a supercomputer in the same way as a regular PC or a smartphone.</p> <p>It is more and more the software that makes the supercomputer!</p>","tags":["Julia"]},{"location":"C09_Conclusions/#summary-of-some-lessons-learnt","title":"Summary of some lessons learnt","text":"<p>Lesson 0: After going trough these lecture notes, you should realise that HPC truly stands for High-Performance Computing and is not synonym to High-end Personal Computer. A supercomputer is not a machine that is just like a PC but 100x or 1000x faster for everything.</p> <p>Lesson 1: These lecture notes also showed that software makes the supercomputer, as it is built from fairly standard components that find their use in other types of computers also (even the interconnect is moving in that direction), but it is the software that turns all of that hardware into a unique machine that can solve some problems no other system  can.</p> <p>Lesson 2: The three keywords of supercomputing are streaming, parallelism and hierarchy.</p> <p>Lesson 3: Whether a run on a supercomputer will be efficient or not, depends on three elements:</p> <ol> <li>The hardware of the supercomputer,</li> <li>the application used,</li> <li>and the problem being solved. Larger problems usually mean that more resources can be      used efficiently.</li> </ol> <p>So unfortunately no support person can give you a simple recipe for your work that will always work...</p> <p>Lesson 4: It's crisis! (But many users don't believe this yet.) Transistors don't become cheaper anymore so we can't rely on that to get more and more compute power over time.</p> <p>And this brings us back to lesson 1: We'll have to focus more on high-quality software in the future if we want to keep supercomputing affordable yet want to be able to do more on supercomputers than we can do today!</p>","tags":["Julia"]},{"location":"tags/","title":"Tags index","text":"<p>Unfortunately the sorting of the tags is still case-sensitive.</p>"},{"location":"tags/#alpaka","title":"Alpaka","text":"<ul> <li>Accelerator programming</li> </ul>"},{"location":"tags/#beowulf","title":"Beowulf","text":"<ul> <li>Distributed memory</li> </ul>"},{"location":"tags/#cuda","title":"CUDA","text":"<ul> <li>What are accelerators?</li> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#fpga","title":"FPGA","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#hip","title":"HIP","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#ilp-instruction-level-parallelism","title":"ILP (Instruction Level Parallelism)","text":"<ul> <li>ILP I - Pipelining</li> <li>ILP II - Superscalar</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> </ul>"},{"location":"tags/#julia","title":"Julia","text":"<ul> <li>Shared memory programming</li> <li>Distributed memory programming</li> <li>Further reading on middleware</li> <li>Conclusions</li> </ul>"},{"location":"tags/#kokkos","title":"Kokkos","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#mpi","title":"MPI","text":"<ul> <li>Distributed memory</li> <li>Distributed memory programming</li> <li>Further reading on middleware</li> </ul>"},{"location":"tags/#numa","title":"NUMA","text":"<ul> <li>Shared memory</li> <li>A modern (super)computer</li> </ul>"},{"location":"tags/#openacc","title":"OpenACC","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#opencl","title":"OpenCL","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#openmp","title":"OpenMP","text":"<ul> <li>Shared memory programming</li> <li>Vector programming</li> <li>Further reading on middleware</li> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#pgas","title":"PGAS","text":"<ul> <li>Distributed memory programming</li> <li>Further reading on middleware</li> </ul>"},{"location":"tags/#raja","title":"RAJA","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#rocm","title":"ROCm","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#smp","title":"SMP","text":"<ul> <li>Shared memory</li> </ul>"},{"location":"tags/#sycl","title":"SYCL","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#core","title":"core","text":"<ul> <li>Shared memory</li> </ul>"},{"location":"tags/#distributed-memory","title":"distributed memory","text":"<ul> <li>Distributed memory</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> <li>Distributed memory programming</li> </ul>"},{"location":"tags/#matrix-accelerator","title":"matrix accelerator","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#matrix-instructions","title":"matrix instructions","text":"<ul> <li>CPUs with accelerator features</li> </ul>"},{"location":"tags/#oneapi","title":"oneAPI","text":"<ul> <li>Accelerator programming</li> <li>Further reading on accelerators</li> </ul>"},{"location":"tags/#pipelined-execution","title":"pipelined execution","text":"<ul> <li>ILP I - Pipelining</li> </ul>"},{"location":"tags/#shared-memory","title":"shared memory","text":"<ul> <li>Shared memory</li> <li>A modern (super)computer</li> <li>Processors: Lessons learnt</li> <li>Shared memory programming</li> </ul>"},{"location":"tags/#superscalar-execution","title":"superscalar execution","text":"<ul> <li>ILP II - Superscalar</li> </ul>"},{"location":"tags/#vector-accelerator","title":"vector accelerator","text":"<ul> <li>What are accelerators?</li> </ul>"},{"location":"tags/#vector-instructions","title":"vector instructions","text":"<ul> <li>Vector computing</li> <li>Processors: Lessons learnt</li> <li>Vector programming</li> <li>CPUs with accelerator features</li> </ul>"}]}