{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Supercomputers for Starters \u00b6 Some preliminary words Introduction Goals Why supercomputing What it is not A compartmentalised supercomputer Overview of the notes Processors for supercomputers The memory hierarchy Storing data on supercomputers Introduction Problems with a parallel disk setup Parallel file systems A storage revolution? To remember Putting it all together Scaling Dennard scaling Transistor cost Three keywords: Streaming, Parallelism, and Hierarchy Andy and Bill's law Software first, not hardware Middleware What can we expect? Accelerators What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading Conclusions","title":"Home"},{"location":"#supercomputers-for-starters","text":"Some preliminary words Introduction Goals Why supercomputing What it is not A compartmentalised supercomputer Overview of the notes Processors for supercomputers The memory hierarchy Storing data on supercomputers Introduction Problems with a parallel disk setup Parallel file systems A storage revolution? To remember Putting it all together Scaling Dennard scaling Transistor cost Three keywords: Streaming, Parallelism, and Hierarchy Andy and Bill's law Software first, not hardware Middleware What can we expect? Accelerators What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading Conclusions","title":"Supercomputers for Starters"},{"location":"0_00_preliminary/","text":"Some preliminary words \u00b6 These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum . The latest version of the corresponding slides is available on the CalcUA training web page . There are also video recordings from the November 2020 edition with some updates from the spring 2021 edition but these may not be world-visible.","title":"Preliminary words"},{"location":"0_00_preliminary/#some-preliminary-words","text":"These course notes are a work-in-progress to better document the material in the CalcUA course \"Supercomputers for Starters\" but may in the future also evolve to cover other similar courses. It is often written in a rather informal style and isn't meant to be a book or so. Disclaimer: This is unofficial documentation and work-in-progress. It is the result of my work at the CalcUA service of the University of Antwerp for the Vlaams Supercomputer Centrum . The latest version of the corresponding slides is available on the CalcUA training web page . There are also video recordings from the November 2020 edition with some updates from the spring 2021 edition but these may not be world-visible.","title":"Some preliminary words"},{"location":"1_Introduction/","text":"Introduction \u00b6 Goals Why supercomputing? What it is not A compartmentalised supercomputer Overview of the notes","title":"Introduction"},{"location":"1_Introduction/#introduction","text":"Goals Why supercomputing? What it is not A compartmentalised supercomputer Overview of the notes","title":"Introduction"},{"location":"1_Introduction/1_01_Goals/","text":"Goals \u00b6 The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a cheap PC or smartphone. Whether a supercomputer will be used well depends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastructure use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the parameters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparent as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_01_Goals/#goals","text":"The goals of these lecture notes are that at the end, the reader should be cable to answer questions such as: Why would I consider using a supercomputer? How does supercomputer hardware influence my choice of software or programming techniques? And is this only so for supercomputers or does it actually also affect other compute devices such as a regular PC, a tablet or a smartphone? What can and can we not expect from a supercomputer? The reader may be very much tempted to say \"I'm not a programmer, do I need to know all this?\" but one should realise a supercomputer is a very expensive machine and a large research infrastructure that should be used efficiently unless a cheap PC or smartphone. Whether a supercomputer will be used well depends on your choice of software. It also depends on the resources that you request when starting a program. Unlike your PC, a supercomputer is a large shared infrastructure and you have to specify which part of the computer you need for your computations. And this requires an understanding of both the supercomputer that you are using and the needs of the software you're using. In fact, if the software cannot sufficiently well exploit the hardware of the supercomputer, you should be using a different type of computing such as simply a more powerful PC, or in some cases a cloud workstation. Another goal is also to make it very clear that a supercomputer is not a superscaled PC that does everything in a magical way much faster than a PC. Some inspiration for these lecture notes comes from looking at the manuals of some software packages that users of the CalcUA infrastructure use or have used, including CP2K, OpenMX, QuantumESPRESSO, Gromacs and SAMtools, and checking which terminology those packages use. In fact, the GROMACS manual even contains a section that tries to explain in short many of the terms (and even some more) that we discuss in these notes. The above figure shows an extract from the GROMACS manual with a lot of terms that one needs to know to tune the parameters of the GROMACS mdrun command for good performance. Most of these will be covered in these lecture nodes. Next, let's have a look at the manual page of the SAMtools sort command. Note that the SAMtools sort command has parameters to specify the maximum amount of memory that it should use and the number of threads. Hence the user should have an idea of how much memory can be realistically used, what a thread is and how one should chose that number. That does require knowledge of the working of the SAMtools sort command which is domain-specific help desk that only a help desk specialised in bio-informatics tools could help you with, but it also requires knowing what a thread is and what it is used for. You can't rely on the defaults as these are 768 MB for memory and a single thread, which are very nice defaults for a decent PC in 2005, but not for a supercomputer in 2022 (or not even a PC in 2022). Finally, the next picture shows a number of terms copied from a VASP manual The reader is confronted with a lot of technical terms that they need to understand to use VASP in a sufficiently optimal way. In fact, a vASP run may even fail if the parameters used in the simulations do not correspond to the properties of the hardware used for the run. In general, running software on a supercomputer is not at all as transparent as it is on a PC, and there are many reasons for that. Some of the complexity comes from the fact that a supercomputer is shared among users and hence needs a system to allocate capacity to users. To keep the usage efficient, most of the work on a supercomputer is done through a so-called batch service where a user must write a (Linux/bash) script (a small program by itself) to tell to the scheduler of the supercomputer what hardware is needed and how to run the job. After all, one cannot afford to let a full multi-million-EURO machine wait for user input. Part of the complexity comes also from the fact that on one hand the hardware is much more complex than on a regular PC, so that it is harder for software to adapt automatically, while on the other hand developers of packages typically don't have the resources to develop tools that would let programs auto-optimize their resource use to the available hardware. No user is willing to pay the amounts of money that is needed to develop software that way. In fact, much of the applications used on supercomputers is \"research-quality\" code that really requires some understanding of the code and algorithms to use properly.","title":"Goals"},{"location":"1_Introduction/1_02_Why/","text":"Why supercomputing? \u00b6 If supercomputing is that complex, why would one want to use it? When a PC or server fails... \u00b6 Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a workstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fields of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue. Supercomputing jobs \u00b6 From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accommodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demand becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the design of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug design one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualisation of very large data sets or simulation results, were a visualisation workstation may not be enough. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes with someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be grammatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#why-supercomputing","text":"If supercomputing is that complex, why would one want to use it?","title":"Why supercomputing?"},{"location":"1_Introduction/1_02_Why/#when-a-pc-or-server-fails","text":"Processing of large datasets may require more storage than a workstation or a departmental server can deliver, or it may require more bandwidth to memory or disk than a workstation or server can deliver, or simply more processing power than a workstation can deliver. In many cases, a supercomputer may help in those cases. However, it may require different or reworked software. Large simulations, e.g., partial differential equations in various fields of physics, may require far more processing power than a workstation can deliver, and often result in large datasets (which then brings us back to the previous point). Supercomputers are often extremely suited for those very large simulations, but again it may require different software or a significant reworking of the software. But there is also an easier case: applications such as parameter analysis or Monte Carlo sampling. In these cases we may want to run thousands of related simulations with different input. A workstation may have more than enough processing power to process a single or a few samples, but to work on thousands of those we need a lot more processing power if we want to finish the work in a reasonable amount of time. Here also supercomputer can come to the rescue.","title":"When a PC or server fails..."},{"location":"1_Introduction/1_02_Why/#supercomputing-jobs","text":"From the above discussion we already get the feeling that there are two big reasons to use a supercomputer. We may want to use a supercomputer simply because we want to improve the turnaround time for a large computation, or because the computation is not even feasible on a smaller computer because of the required compute time and memory capacity. This is also called capability computing . In capability computing one typically thinks in terms of hours per job. We may also want to use a supercomputer to improve throughput if we have to run a lot of smaller jobs. This is called capacity computing . In capacity computing one typically thinks in terms of jobs per hour. Furthermore, we can distinguish simulation and data processing as two big domains where supercomputing could be used. Some will add AI as a third pillar, but AI is typically used for data processing and has requirements similar to the other data processing jobs we will discuss. Supercomputers are really build for capability computing jobs. They can also accommodate capacity computing jobs, but many capacity computing jobs could be run equally well on just a network of servers, e.g., a cloud infrastructure, at possibly a much lower cost (should the user be charged a realistic amount for the compute resources consumed). There are many examples of capability computing in simulation. Computational fluid dynamics of turbulent and even many laminar flow requires a huge amount of compute capacity. The demand becomes only higher when considering fluid-structure interactions causing, e.g., vibration of the structure. Another example is virtual crash tests during the design of cars. That a car manufacturer can produce so much more different models than 30 years ago is partly the result from the fact that developing a different body for a car is now much cheaper than it was 30 years ago despite the more stringent safety rules, as far less prototypes need to be build before having a successful crash tests thanks to the virtual crash tests. Weather and climate modeling are also examples of capability computing, as is the simulation of large complex molecules and their chemical reactions. But there are also examples of capacity computing in simulation. E.g., when doing a parameter study that requires simulating a system for multiple sets of parameters. In drug design one often wants to test a range of molecules for certain properties. The risk analysis computations done by banks are also an example of capacity computing. Sometimes one really has a combination of capacity computing and capability computing, e.g., when doing a parameter analysis for a more complex system. Data processing can also lead to capability computing. One example is the visualisation of very large data sets or simulation results, were a visualisation workstation may not be enough. Another example are the electronic stamps offered by the US postal services, certainly in the initial years. Buyers of electronic stamps would get codes that they can each use once, but of course US postal needs to check not only if a code is valid but also if it has not been used yet. And a clever abuser may share codes with someone at the opposite site of the country. Yet US postal has to be able to detect abuse in real time to not slow down the sorting machines. For this they used a computer which around 2010 (when the author of these notes learned about this application) could truly be considered a supercomputer. Examples of capacity computing for data processing are many of the data mining applications that often consist of many rather independent tasks. The processing of the CERN Large Hadron Collider data is also mostly capacity computing. Another example is a researcher from a Flemish university who used a VSC supercomputer to pre-process a text corpus consisting of newspaper articles of many years. All texts had to be grammatically analysed. After the preprocessing, a regular server was enough for the research.","title":"Supercomputing jobs"},{"location":"1_Introduction/1_03_What_it_is_not/","text":"What supercomputing is not \u00b6 Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous century. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '70s, with the advent of so-called vector supercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What it is not"},{"location":"1_Introduction/1_03_What_it_is_not/#what-supercomputing-is-not","text":"Around 2010 the author of these notes got a phone call from a prospective user with the question: \"I have a Turbo Pascal program that runs too slow on my PC, so I want to run it on the supercomputer.\" Now Turbo Pascal is a programming language that was very popular in the '80s and the early '90s of the previous century. It is almost a miracle that the program even still ran on the PC of that user, and it is also very limited in the amount of memory a program can use. Assuming that prospective user had a modern PC on their desk, and forgetting for a while that Turbo Pascal generates software that cannot even run on Linux, the OS of almost all supercomputers nowadays, the program would still not run any faster on the supercomputer for reasons that we will explain later in these lecture notes. The reader should realise that supercomputers only become supercomputers when running software developed for supercomputers. This was different in the '60s and early '70s of the previous century, the early days of supercomputing. In those days one had smaller computers with slower processors, and bigger machines with much faster processors, but one could essentially recompile the software for the faster machine if needed. That started to change in the second half of the '70s, with the advent of so-called vector supercomputers. There one really needed to adapt software to extract the full performance of the machine. Since then things only got worse. Due to physical limitations it is simply not possible to build ever faster processors, so modern supercomputers are instead parallel computers combining thousands of regular processors that are not that different from those in a PC to get the job done. And software will not automatically use all these processors in an efficient way without effort from the programmer. Yet there is no need to be too pessimistic either. In some cases, in particular capacity computing, the efforts to get your job running efficiently can be relatively minor and really just require setting up a parallel workflow to run a sufficient number of cases simultaneously. But developing code for capability computing is much more difficult, but then a supercomputer can let you compute results that you could not obtain in any other way. And in many cases someone else has done the work already for you and good software is already available.","title":"What supercomputing is not"},{"location":"1_Introduction/1_04_Compartmentalised/","text":"A compartmentalised supercomputer \u00b6 Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly starting that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compartmentalised supercomputer"},{"location":"1_Introduction/1_04_Compartmentalised/#a-compartmentalised-supercomputer","text":"Even though a supercomputer usually runs some variant of Linux, it does work differently from a regular Linux workstation or server. When you log on on a PC, you immediately get access to the full hardware. This is not the case on a supercomputer. A supercomputer consists of many compartments: When users log on to a supercomputer, they land on the so-called login nodes. These are one or more servers that each look like a regular Linux machine but should not be used for big computations. They are used to prepare jobs for the supercomputer: small programs that tell the supercomputer what to do and how to do it. Each supercomputer also has a management section. This consists of a number of servers that are not accessible to regular users. The management section is responsible for controlling and managing the operation of the supercomputer, including deciding when a job may start and properly starting that job. Each supercomputer also has a storage section, a part of the hardware that provides permanent storage for data or a scratch space that can be used on the complete machine. But the most important part of each supercomputer is of course the compute section, or compute sections in many cases as most supercomputers provide different types of compute resources to cover different needs of applications. In these notes, we will mostly talk about the structure of the compute section of the cluster, but we will also cover some typical properties of the storage system as that has a large influence on what one can do and what one cannot do on a supercomputer.","title":"A compartmentalised supercomputer"},{"location":"1_Introduction/1_05_Overview/","text":"Overview of the notes \u00b6 A supercomputer is a parallel computer \u00b6 A supercomputer is not a superscaled PC that runs regular PC software much faster, but it is a parallel computer: In a supercomputer, many processors work together to create a fast system, and this is even multi-level parallelism. We will discuss this in the section on Processors . The memory of a supercomputer is also organised in a hierarchy: from fast buffer memory close to the processor to slower memory and then all the way to slow disks. Most of this will be explained in the section on the memory hierarchy . The permanent storage system of a supercomputer consists of many hard disks and solid state drives that are combined to a powerful storage system with the help of software. This is explained in the section on storage for supercomputers . In the current state of technology this is far from transparent. It may be mostly transparent for correctness of the program but it is not al all for performance, which is why one needs properly written software adapted to the specific properties of supercomputers. After all, supercomputing is not about trying to upscale a PC equally in all parameters as that is uneconomical and mostly physically impossible, but it is about building hardware as economical as possible (as the market is too small to develop many non-standard components) and using software to bind all hardware together to a very powerful machine. Yet much of what you learn about supercomputers is also useful for programming PC's. Modern PC's also increasingly rely on parallelism, and this has become very visible with the AMD Ryzen-based PC's and the 12th Gen and later Intel processors. In fact, smartphones arrived there even earlier as 8-core Android smartphones have been around since 2013. PC's also have most of the memory hierarchy that you'll find in supercomputers, and taking that hierarchy into account when writing software is as important for PC's as it is for supercomputers. Even the inner workings of a solid state drive has elements in common with how supercomputer storage work, and understanding the behaviour of supercomputer storage helps you also understand why an SSD will not always show the expected peak performance claimed in the specifications. A supercomputer has a layered architecture \u00b6 A supercomputer is much more than some hardware on which you run your application. As already suggested above, between the hardware and your applications sits a lot of other software that binds the hardware into a usable supercomputer. At the lowest level you have the operating system which may be just linux but sometimes is a modified version of linux with some features disabled that may harm the performance of a supercomputer and are not needed by real supercomputer applications. But on a supercomputer there is a lot of other software that sits between the hardware and basic OS on one hand and your application on the other hand. That software is often called middleware . Most readers of these notes may be only interested in the applications they want to run. However, some understanding of the hardware is needed even for simple things as starting a job, as a supercomputer is a multi-user machine and you need to ask exactly what you need to be able to run your application efficiently. And not all software can run efficiently on all hardware, or even run at all. But it also requires some understanding of the middleware. Your application, if it is really developed to use supercomputers properly, will also use some of that middleware, and not all middleware can be supported on all supercomputers. For developers even better hardware knowledge is required to understand how to write programs efficiently, and you also need a good understanding of the middleware that will be used to develop your program. The sections on processors , memory hierarchy and storage discuss the hardware of a supercomputer. The section on middleware then gives an overview of the most popular middleware used to develop software that exploits parallelism. We cannot discuss much about the application layer in these notes, as that quickly becomes too domain-specific, but we will discuss what you can expect from a supercomputer . In recent years, accelerators have also become a hot topic. As the cost, both the investment cost and energy cost, of supercomputers that are fully based on general-purpose hardware has become too high to sustain the growth of compute capacity, scientists have turned into other technologies, collectively known as accelerators, to further increase the performance at a lower investment and energy cost, but losing some of the easy-of-use and versatility of general purpose hardware. The most important types of compute accelerators and their programming models (and corresponding middleware) will be discussed in the section on accelerators .","title":"Overview of the notes"},{"location":"1_Introduction/1_05_Overview/#overview-of-the-notes","text":"","title":"Overview of the notes"},{"location":"1_Introduction/1_05_Overview/#a-supercomputer-is-a-parallel-computer","text":"A supercomputer is not a superscaled PC that runs regular PC software much faster, but it is a parallel computer: In a supercomputer, many processors work together to create a fast system, and this is even multi-level parallelism. We will discuss this in the section on Processors . The memory of a supercomputer is also organised in a hierarchy: from fast buffer memory close to the processor to slower memory and then all the way to slow disks. Most of this will be explained in the section on the memory hierarchy . The permanent storage system of a supercomputer consists of many hard disks and solid state drives that are combined to a powerful storage system with the help of software. This is explained in the section on storage for supercomputers . In the current state of technology this is far from transparent. It may be mostly transparent for correctness of the program but it is not al all for performance, which is why one needs properly written software adapted to the specific properties of supercomputers. After all, supercomputing is not about trying to upscale a PC equally in all parameters as that is uneconomical and mostly physically impossible, but it is about building hardware as economical as possible (as the market is too small to develop many non-standard components) and using software to bind all hardware together to a very powerful machine. Yet much of what you learn about supercomputers is also useful for programming PC's. Modern PC's also increasingly rely on parallelism, and this has become very visible with the AMD Ryzen-based PC's and the 12th Gen and later Intel processors. In fact, smartphones arrived there even earlier as 8-core Android smartphones have been around since 2013. PC's also have most of the memory hierarchy that you'll find in supercomputers, and taking that hierarchy into account when writing software is as important for PC's as it is for supercomputers. Even the inner workings of a solid state drive has elements in common with how supercomputer storage work, and understanding the behaviour of supercomputer storage helps you also understand why an SSD will not always show the expected peak performance claimed in the specifications.","title":"A supercomputer is a parallel computer"},{"location":"1_Introduction/1_05_Overview/#a-supercomputer-has-a-layered-architecture","text":"A supercomputer is much more than some hardware on which you run your application. As already suggested above, between the hardware and your applications sits a lot of other software that binds the hardware into a usable supercomputer. At the lowest level you have the operating system which may be just linux but sometimes is a modified version of linux with some features disabled that may harm the performance of a supercomputer and are not needed by real supercomputer applications. But on a supercomputer there is a lot of other software that sits between the hardware and basic OS on one hand and your application on the other hand. That software is often called middleware . Most readers of these notes may be only interested in the applications they want to run. However, some understanding of the hardware is needed even for simple things as starting a job, as a supercomputer is a multi-user machine and you need to ask exactly what you need to be able to run your application efficiently. And not all software can run efficiently on all hardware, or even run at all. But it also requires some understanding of the middleware. Your application, if it is really developed to use supercomputers properly, will also use some of that middleware, and not all middleware can be supported on all supercomputers. For developers even better hardware knowledge is required to understand how to write programs efficiently, and you also need a good understanding of the middleware that will be used to develop your program. The sections on processors , memory hierarchy and storage discuss the hardware of a supercomputer. The section on middleware then gives an overview of the most popular middleware used to develop software that exploits parallelism. We cannot discuss much about the application layer in these notes, as that quickly becomes too domain-specific, but we will discuss what you can expect from a supercomputer . In recent years, accelerators have also become a hot topic. As the cost, both the investment cost and energy cost, of supercomputers that are fully based on general-purpose hardware has become too high to sustain the growth of compute capacity, scientists have turned into other technologies, collectively known as accelerators, to further increase the performance at a lower investment and energy cost, but losing some of the easy-of-use and versatility of general purpose hardware. The most important types of compute accelerators and their programming models (and corresponding middleware) will be discussed in the section on accelerators .","title":"A supercomputer has a layered architecture"},{"location":"2_Processors/","text":"Processors for supercomputers \u00b6 Introduction A very basic CPU Instruction level parallelism 1: Pipelining Instruction level parallelism 2: Superscalar processing Data parallelism through vector computing","title":"Processors for supercomputers"},{"location":"2_Processors/#processors-for-supercomputers","text":"Introduction A very basic CPU Instruction level parallelism 1: Pipelining Instruction level parallelism 2: Superscalar processing Data parallelism through vector computing","title":"Processors for supercomputers"},{"location":"2_Processors/2_01_Intro/","text":"Introduction \u00b6 Long ago supercomputers were build from specialised processors designed specifically for supercomputers. This was in the days that chip technology used large geometries and the speed of even multichip processors was limited by the switching speed of transistors and not by physical distance. Cheaper minicomputers and the early personal computers used single chip processors while mainframes and supercomputers used processors that were build from multiple chips, sometimes several thousands of them. Halfway the '80s things started to change slowly. The processors for PC's that were build from 1 or 2 chip packages (in the case of 2, one for the floating point instructions and one for everything else) became more competitive, not yet in speed, but in price/performance, leading to new supercomputer architectures based on those much cheaper processors. In the course of the '90s processors consisting of multiple chip packages gradually disappeared as the communication between packages became the speed limiting factor. Gradually all supercomputers were no longer being build from very specialised processors, but from processors derived from those used in PCs, and nowadays even from processors derived from those in smartphones, but with enhancements for better reliability. These CPUs have gone through a login. Both advances in semiconductor technology and the advances in architecture made possible by the semiconductor technology advances have made it possible to build ever faster CPUs. The workings of all modern processors is governed by a clock, yet the clockspeed (which is measured in GHz nowadays rather than in MHz in the '80s) is not a good basis to compare the speed of CPUs. This is because there have been so many architectural advances that have increased the amount of work a CPU can do during each clock cycle. These improvements fall into two categories: Improvements that enable to execute more instructions per clock cycle: this is called instruction level parallelism , and improvements that enable the CPU to do more work per instruction, and the main strategy here is vectorization and nowadays even matrix computing . Yet those improvements were not enough to satisfy the ever growing need for compute speed of researchers, so two more strategies were used: Using more CPUs (now called cores ) that share memory which is called shared memory parallel computing , and Using multiple nodes that collaborate not by sharing memory but by sending messages over a network, which is called distributed memory parallel computing . All 4 of these strategies - instruction level parallelism, vector and matrix computing, shared memory parallel computing and distributed memory parallel computing - rely on finding and exploiting parallelism in the solution of computational problems, and the further down you go in the list, the more difficult this becomes, but also the more performance you can get. As we shall see, nowadays even your PC and smartphone rely on the first three forms of parallelism for performance, so learning to deal properly with parallelism is no longer an option but a necessity to remain competitive.","title":"Introduction"},{"location":"2_Processors/2_01_Intro/#introduction","text":"Long ago supercomputers were build from specialised processors designed specifically for supercomputers. This was in the days that chip technology used large geometries and the speed of even multichip processors was limited by the switching speed of transistors and not by physical distance. Cheaper minicomputers and the early personal computers used single chip processors while mainframes and supercomputers used processors that were build from multiple chips, sometimes several thousands of them. Halfway the '80s things started to change slowly. The processors for PC's that were build from 1 or 2 chip packages (in the case of 2, one for the floating point instructions and one for everything else) became more competitive, not yet in speed, but in price/performance, leading to new supercomputer architectures based on those much cheaper processors. In the course of the '90s processors consisting of multiple chip packages gradually disappeared as the communication between packages became the speed limiting factor. Gradually all supercomputers were no longer being build from very specialised processors, but from processors derived from those used in PCs, and nowadays even from processors derived from those in smartphones, but with enhancements for better reliability. These CPUs have gone through a login. Both advances in semiconductor technology and the advances in architecture made possible by the semiconductor technology advances have made it possible to build ever faster CPUs. The workings of all modern processors is governed by a clock, yet the clockspeed (which is measured in GHz nowadays rather than in MHz in the '80s) is not a good basis to compare the speed of CPUs. This is because there have been so many architectural advances that have increased the amount of work a CPU can do during each clock cycle. These improvements fall into two categories: Improvements that enable to execute more instructions per clock cycle: this is called instruction level parallelism , and improvements that enable the CPU to do more work per instruction, and the main strategy here is vectorization and nowadays even matrix computing . Yet those improvements were not enough to satisfy the ever growing need for compute speed of researchers, so two more strategies were used: Using more CPUs (now called cores ) that share memory which is called shared memory parallel computing , and Using multiple nodes that collaborate not by sharing memory but by sending messages over a network, which is called distributed memory parallel computing . All 4 of these strategies - instruction level parallelism, vector and matrix computing, shared memory parallel computing and distributed memory parallel computing - rely on finding and exploiting parallelism in the solution of computational problems, and the further down you go in the list, the more difficult this becomes, but also the more performance you can get. As we shall see, nowadays even your PC and smartphone rely on the first three forms of parallelism for performance, so learning to deal properly with parallelism is no longer an option but a necessity to remain competitive.","title":"Introduction"},{"location":"2_Processors/2_02_Basic_CPU/","text":"A very basic CPU \u00b6","title":"A basic CPU"},{"location":"2_Processors/2_02_Basic_CPU/#a-very-basic-cpu","text":"","title":"A very basic CPU"},{"location":"2_Processors/2_03_ILP_I_Pipelining/","text":"Instruction level parallelism 1: Pipelining \u00b6","title":"ILP I - Pipelining"},{"location":"2_Processors/2_03_ILP_I_Pipelining/#instruction-level-parallelism-1-pipelining","text":"","title":"Instruction level parallelism 1: Pipelining"},{"location":"2_Processors/2_04_ILP_II_Superscalar/","text":"Instruction level parallelism 2: Superscalar processing \u00b6","title":"ILP II - Superscalar"},{"location":"2_Processors/2_04_ILP_II_Superscalar/#instruction-level-parallelism-2-superscalar-processing","text":"","title":"Instruction level parallelism 2: Superscalar processing"},{"location":"2_Processors/2_05_Vectorisation/","text":"Data parallelism through vector computing \u00b6","title":"Vector computing"},{"location":"2_Processors/2_05_Vectorisation/#data-parallelism-through-vector-computing","text":"","title":"Data parallelism through vector computing"},{"location":"3_Memory/","text":"The memory hierarchy \u00b6","title":"The memory hierarchy"},{"location":"3_Memory/#the-memory-hierarchy","text":"","title":"The memory hierarchy"},{"location":"4_Storage/","text":"Storing data on supercomputers \u00b6 Introduction Problems with a parallel disk setup Parallel file systems A storage revoulution? To remember","title":"Storing data on supercomputers"},{"location":"4_Storage/#storing-data-on-supercomputers","text":"Introduction Problems with a parallel disk setup Parallel file systems A storage revoulution? To remember","title":"Storing data on supercomputers"},{"location":"4_Storage/4_01_Introduction/","text":"Introduction \u00b6 We've seen that physics makes it impossible to build a single processor core that is a thousand or a million times faster than a regular CPU core in a PC and that we need to use parallelism and lots of processors instead in a supercomputer. The same also holds for storage. It is not possible to make a single hard disk that would spin a thousand times faster and have a capacity a thousand times more than current hard disks for use in a supercomputer. Nor would it be possible to upscale the design of a solid state drive to the capacities needed for supercomputing and at the same time also improve access etc. A storage system for a supercomputer is build in the same way as the supercomputer itself is build from multiple processors: Hundreds or thousands of regular disks or SSDs are combined with the help of some hardware and mostly clever software to appear as one large and very fast disk. In fact, some of this technology is even used in PCs and smartphones as an SSD drive also uses multiple memory chips and exploits parallelism to get more performance out of the drive as would be possible with a single chip. This is why, e.g., the 256 GB hard drive in the M2 MacBook Pro is slower than the 512 GB one as it simply doesn't contain enough memory chips to get sufficient parallelism and saturate the drive controller. However, just as not all programs can benefit from using multiple processors, not all programs can benefit from a supercomputer disk setup. A parallel disk setup only works when programs access large amounts of data in large files. And the same is true for storage as for computing: The storage of your PC can be faster than the shared storage of a supercomputer if you don't use it in the proper way. But similarly accessing files in the right way may make your already fast PC storage even faster as there are applications that are so badly written that they use the SSD in your PC also at only 5% of its potential data transfer speed...","title":"Introduction"},{"location":"4_Storage/4_01_Introduction/#introduction","text":"We've seen that physics makes it impossible to build a single processor core that is a thousand or a million times faster than a regular CPU core in a PC and that we need to use parallelism and lots of processors instead in a supercomputer. The same also holds for storage. It is not possible to make a single hard disk that would spin a thousand times faster and have a capacity a thousand times more than current hard disks for use in a supercomputer. Nor would it be possible to upscale the design of a solid state drive to the capacities needed for supercomputing and at the same time also improve access etc. A storage system for a supercomputer is build in the same way as the supercomputer itself is build from multiple processors: Hundreds or thousands of regular disks or SSDs are combined with the help of some hardware and mostly clever software to appear as one large and very fast disk. In fact, some of this technology is even used in PCs and smartphones as an SSD drive also uses multiple memory chips and exploits parallelism to get more performance out of the drive as would be possible with a single chip. This is why, e.g., the 256 GB hard drive in the M2 MacBook Pro is slower than the 512 GB one as it simply doesn't contain enough memory chips to get sufficient parallelism and saturate the drive controller. However, just as not all programs can benefit from using multiple processors, not all programs can benefit from a supercomputer disk setup. A parallel disk setup only works when programs access large amounts of data in large files. And the same is true for storage as for computing: The storage of your PC can be faster than the shared storage of a supercomputer if you don't use it in the proper way. But similarly accessing files in the right way may make your already fast PC storage even faster as there are applications that are so badly written that they use the SSD in your PC also at only 5% of its potential data transfer speed...","title":"Introduction"},{"location":"4_Storage/4_02_Problems/","text":"Problems with a parallel disk setup \u00b6 Disks break \u00b6 Hard drives fail rather often. And though SSDs may not contain moving parts, when not used in the right way they are not that much more reliable. Given that a single drive on average fails after 1 or 2 million hours (which may seem a lot), in a stack of a thousand drives one can expect that a drive might fail every 50 to 100 days. Loosing some data every 50 days is already bad, but if all disks are combined with software to a single giant disks with even average sized files spread out over multiple disks for performance, much more data will be damaged than that single disk can contain. This is clearly unacceptable. Moreover, we don't want to stop a supercomputer every 50 or 100 days because the file system needs repairs. Let alone that big supercomputers have even bigger disk systems... The solution is to use extra disks to store enough information to recover lost data by using error correcting codes. Now hard disks and SSDs are block-oriented media: data is stored in blocks, often 4 kiB in size, so the error correction is done at the block level. A typical setup would be to use 8 drives with 2 drives for the error correction information, which can support up to two drive failures in the group of 10. But that has implications for reading and writing data. This is especially true for writing data, as whenever data is written to a block on one disk, the corresponding blocks on the disks with the error correction information must also be updated. And that requires also first reading data to be able to compute the changes in the error correcting information. Unless of course all corresponding blocks on the 8 disks would be written concurrently, as the we already have all the information to also compute the error correction information. But that makes the optimal block size for writing data effectively 8 times larger... For reading data in such a setup you can actually already benefit as you can read from all drives involved in parallel. This technique is also known as RAID, Redundant Array of Inexpensive Disks, and there exist several variants of this technique, some only to increase performance and others to improve reliability also. Error correction codes are in fact also used to protect RAM memory in servers and supercomputers, or internally in flash drives, and sometimes also in communication protocols (though they may use other techniques also). File system block size \u00b6 A file system organizes files in one or more blocks (which can be different from the blocks on disk). A block in a file system is the smallest element of data that a file system can allocate and manage. On a PC, the block size of the file system used to be 512 bytes though now it is often 4 kiB. However, on a supercomputer this would lead to an enormous amount of blocks which would become impossible to manage. There are two solutions for that. One solution is to simply use a larger block size, which is the case in, e.g., the IBM Spectrum Scale file system. Larger blocks are also a better fit with the RAID techniques used to increase the reliability of the storage system. However, as the block is the smallest amount of storage that can be allocated in a file system, it also implies that very small files will still take a lot of space on such a file system (the size of a block), though some file systems may have a solution for really small files. This can lead to a waste of space if many small files are used like that. The disk setup used at the UAntwerp HPC service until 2020 suffered from this problem. In fact, we once had a user who managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. Now the first IBM-compatible PCs had a memory limit of 640 kiB... The second solution is to use a 2-level hierarchy. Big files are split in a special way in smaller files called objects that are then stored on smaller separate servers called the object servers. As these servers are smaller, they can use a smaller block size. And small files would use only a single object, making the smallest amount of disk space consumed by a file the block size of a single object server. Examples of this approach are the Lustre and BeeGFS file system used in supercomputing. UAntwerp-specific The supercomputer storage of the CalcUA facility of the University of Antwerp used the IBM Spectrum Scale file system (then still known as GPFS) for its disk volumes. The scratch storage had a block size of 128 kiB. One user managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. And that user ran thousands of tests each storing 5 such files... Now remember the first IBM-compatible PCs had a memory limit of 640 kiB and students had to use those to write a complete Ph.D. thesis... The storage that was installed in 2020 at the CalcUA service uses BeeGFS for the file system. Which comes with its own problems as we shall see later. Physics and the network \u00b6 On your PC, the storage sits both physically and logically very close to the processor. The fastest SSDs, NVMe drives, are often even directly connected to the CPU, and on the M-series MAc this is even taken one step further, with part of the drive (the controller) integrated into the CPU (though this probably saves more on power consumption than it gives additional speed). Moreover, at the logical level, there is only one file system in between your program and the drive. The program talks directly to the OS which talks directly to the drive. With shared storage on a supercomputer the picture is very different. The storage is both physically and logically further away from your application. Physically because there are (at least) two processors and a network involved (and on the server side disks usually not be as close to the processor as on your PC, except in some of the most expensive storage systems). The physical delay caused by the network may not be that important with hard disk storage, but it is important when accessing SSDs or cached storage. The software adds even more delays. After all, your program talks to a network file system which then sends the request to the server where it also has to pass through multiple layers of software: through the network stack, to the file server software, to the file system (which may be similar to that on your PC but doesn't have to), and back through the network stack before the answer to the request is off again to your application, where it also first has to pass through the network stack and network file system again. Parallel file systems often have an optimised and shorter route for reading and writing data, but often at the cost of more costly open and close operations and hence a very high cost for access to small files. And the path is still much, much longer than that to a local drive. This comes with a number of consequences: Programs that open and close hundreds of small files in a short time may work slower than on a PC. This is particularly true if all data access comes from a single thread as your program will be waiting for data all the time. That software also puts a very high load on\\ the file systems of a supercomputer, to the extent that several supercomputer centres nowadays take measures to keep that software off the supercomputers. Unpredictable file access patterns may make things even worse as any logic to prefetch data\\ and hide latency will fail. One may wonder why supercomputers don't always provide local drives to cope with the slowness of shared storage. There are many reasons: From a management point of view, there are several problems. The management environment has to clean them at the end of a job, but it is not always clear which files can be removed if multiple jobs are allowed to run on a single node of the supercomputer. And the software that needs to local storage most, also tends to be the software that cannot use a full node of a supercomputer efficiently. Modern Linux does have a solution to the cleaning problem (namespaces), but that solution then comes with other restrictions that users may have trouble living with, e.g., starting processes on other nodes should always be done through the resource manager software to ensure that the processes start in the right namespace. Which implies that, e.g., ssh , a very popular mechanism to start a session on a different host, should not be used. Moreover, the data becomes inaccessible to the user when the job ends, so if the so-called job script , the script that instructs the supercomputer what to do, is ended because the resources expire or because of a crash, the data on the local drive will be lost to the user. For reading one also loses time as the data needs to be copied first to the local drive. It is physically also not easy to add local drives to a supercomputer node. Supercomputer nodes are built with very high density as it is important to keep all links in a supercomputer as short as possible to minimise communication delays between nodes. Modern CPUs and GPUs run very hot, while storage prefers a lower temperature. On an air cooled node, the storage has to be early in the air flow through the node as once the air has gone through the CPU or GPU coolers, it is way too hot to cool the storage sufficiently. But given the small size of a supercomputer node, those storage devices may hinder the air flow through the node and hence make the cooling less effective. On a water cooled node, things aren't that much easier though the situation is improving. As M-type SSDs (those that you insert in a slot on the motherboard close to the CPU) nowadays even need cooling in a regular PC, they have been made more friendly to the addition of cooling elements. However, reliability of SSD drives is also an issue. SSD drives based on flash memory (which is currently practically any drive still in production) have a limited life span under a heavy write load, while the use suggested here, as a temporary buffer for the duration of a job, is precisely a scenario with a high write load. Replacing broken hardware is an issue, made worst because of the dense construction of a supercomputer. One may wonder why local drives are so much more common in cloud infrastructure. The constraints in cloud infrastructure are different. Supercomputers, with the exception of some commercial clusters, are built to be as cost-effective as possible. So one tends to solve problems with better software rather than adding more hardware. Cloud infrastructures on the other hand are usually commercial offerings. They are built at a very large scale, with often an even more custom hardware design, and they are simply overprovisioned. E.g., a server may have two SSD drives where the software will simply switch over to the second drive when the first one breaks, but the broken one will never be replaced. The management model of a cloud infrastructure is also very different. Cloud is based on virtualisation technologies to isolate users, and let users built a virtual network of servers in which regular Linux access methods can be used. These layers of software add additional overhead (even with hardware features to support virtualisation) which is undesirable on a supercomputer where each additional 100 nanoseconds of communication delay may limit how far a job can scale on the computer. Note though that the virtualisation overhead, thanks to hardware support, has become low enough that small supercomputer jobs can run very well on some cloud infrastructures. Metadata \u00b6 Each file contains both the actual data in the file, and so-called metadata such as the name of the file, access rights to the file, the date of creation and of last use, ... This metadata is stored in a directory which on a traditional file system with folder and subfolder structure is a special type of file for each (sub)directory. This implies that if you do many small disk accesses to files in the same directory, or store a lot of files in a single directory and access them simultaneously, you create a bottleneck as many updates are needed to that directory. Most file systems are not very good at parallelising that directory access. Bad patterns of metadata access are probably the most common source for performance problems on supercomputer file systems. A typical scenario is when in a distributed memory application, each process creates its own set of files in the same shared directory, rather than use a feature called parallel file I/O to create a few giant files to which data is written in orchestrated multi-node operations. This can bring almost every supercomputer file system to its knees. Building a file system that can cope with this load might be technologically impossible or would at least make storage an order of magnitude more expensive. An equally stupid idea is to open a file before every write and then close it again to ensure that data is written to disk immediately. This is already an expensive operation on a PC with local storage that will slow down your program a lot if those writes are very frequent, but it will kill almost any networked file system and is even worse for the so-called parallel file systems on supercomputers as they tend to have more expensive open and close operations.","title":"Problems"},{"location":"4_Storage/4_02_Problems/#problems-with-a-parallel-disk-setup","text":"","title":"Problems with a parallel disk setup"},{"location":"4_Storage/4_02_Problems/#disks-break","text":"Hard drives fail rather often. And though SSDs may not contain moving parts, when not used in the right way they are not that much more reliable. Given that a single drive on average fails after 1 or 2 million hours (which may seem a lot), in a stack of a thousand drives one can expect that a drive might fail every 50 to 100 days. Loosing some data every 50 days is already bad, but if all disks are combined with software to a single giant disks with even average sized files spread out over multiple disks for performance, much more data will be damaged than that single disk can contain. This is clearly unacceptable. Moreover, we don't want to stop a supercomputer every 50 or 100 days because the file system needs repairs. Let alone that big supercomputers have even bigger disk systems... The solution is to use extra disks to store enough information to recover lost data by using error correcting codes. Now hard disks and SSDs are block-oriented media: data is stored in blocks, often 4 kiB in size, so the error correction is done at the block level. A typical setup would be to use 8 drives with 2 drives for the error correction information, which can support up to two drive failures in the group of 10. But that has implications for reading and writing data. This is especially true for writing data, as whenever data is written to a block on one disk, the corresponding blocks on the disks with the error correction information must also be updated. And that requires also first reading data to be able to compute the changes in the error correcting information. Unless of course all corresponding blocks on the 8 disks would be written concurrently, as the we already have all the information to also compute the error correction information. But that makes the optimal block size for writing data effectively 8 times larger... For reading data in such a setup you can actually already benefit as you can read from all drives involved in parallel. This technique is also known as RAID, Redundant Array of Inexpensive Disks, and there exist several variants of this technique, some only to increase performance and others to improve reliability also. Error correction codes are in fact also used to protect RAM memory in servers and supercomputers, or internally in flash drives, and sometimes also in communication protocols (though they may use other techniques also).","title":"Disks break"},{"location":"4_Storage/4_02_Problems/#file-system-block-size","text":"A file system organizes files in one or more blocks (which can be different from the blocks on disk). A block in a file system is the smallest element of data that a file system can allocate and manage. On a PC, the block size of the file system used to be 512 bytes though now it is often 4 kiB. However, on a supercomputer this would lead to an enormous amount of blocks which would become impossible to manage. There are two solutions for that. One solution is to simply use a larger block size, which is the case in, e.g., the IBM Spectrum Scale file system. Larger blocks are also a better fit with the RAID techniques used to increase the reliability of the storage system. However, as the block is the smallest amount of storage that can be allocated in a file system, it also implies that very small files will still take a lot of space on such a file system (the size of a block), though some file systems may have a solution for really small files. This can lead to a waste of space if many small files are used like that. The disk setup used at the UAntwerp HPC service until 2020 suffered from this problem. In fact, we once had a user who managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. Now the first IBM-compatible PCs had a memory limit of 640 kiB... The second solution is to use a 2-level hierarchy. Big files are split in a special way in smaller files called objects that are then stored on smaller separate servers called the object servers. As these servers are smaller, they can use a smaller block size. And small files would use only a single object, making the smallest amount of disk space consumed by a file the block size of a single object server. Examples of this approach are the Lustre and BeeGFS file system used in supercomputing. UAntwerp-specific The supercomputer storage of the CalcUA facility of the University of Antwerp used the IBM Spectrum Scale file system (then still known as GPFS) for its disk volumes. The scratch storage had a block size of 128 kiB. One user managed to store 36 bytes worth of data while consuming over 640 kiB on the file system as each of the 5 numbers were written to a separate file, effectively using 128 kiB per number (the block size of that file system) and 512 bytes to store the directory information for each file. And that user ran thousands of tests each storing 5 such files... Now remember the first IBM-compatible PCs had a memory limit of 640 kiB and students had to use those to write a complete Ph.D. thesis... The storage that was installed in 2020 at the CalcUA service uses BeeGFS for the file system. Which comes with its own problems as we shall see later.","title":"File system block size"},{"location":"4_Storage/4_02_Problems/#physics-and-the-network","text":"On your PC, the storage sits both physically and logically very close to the processor. The fastest SSDs, NVMe drives, are often even directly connected to the CPU, and on the M-series MAc this is even taken one step further, with part of the drive (the controller) integrated into the CPU (though this probably saves more on power consumption than it gives additional speed). Moreover, at the logical level, there is only one file system in between your program and the drive. The program talks directly to the OS which talks directly to the drive. With shared storage on a supercomputer the picture is very different. The storage is both physically and logically further away from your application. Physically because there are (at least) two processors and a network involved (and on the server side disks usually not be as close to the processor as on your PC, except in some of the most expensive storage systems). The physical delay caused by the network may not be that important with hard disk storage, but it is important when accessing SSDs or cached storage. The software adds even more delays. After all, your program talks to a network file system which then sends the request to the server where it also has to pass through multiple layers of software: through the network stack, to the file server software, to the file system (which may be similar to that on your PC but doesn't have to), and back through the network stack before the answer to the request is off again to your application, where it also first has to pass through the network stack and network file system again. Parallel file systems often have an optimised and shorter route for reading and writing data, but often at the cost of more costly open and close operations and hence a very high cost for access to small files. And the path is still much, much longer than that to a local drive. This comes with a number of consequences: Programs that open and close hundreds of small files in a short time may work slower than on a PC. This is particularly true if all data access comes from a single thread as your program will be waiting for data all the time. That software also puts a very high load on\\ the file systems of a supercomputer, to the extent that several supercomputer centres nowadays take measures to keep that software off the supercomputers. Unpredictable file access patterns may make things even worse as any logic to prefetch data\\ and hide latency will fail. One may wonder why supercomputers don't always provide local drives to cope with the slowness of shared storage. There are many reasons: From a management point of view, there are several problems. The management environment has to clean them at the end of a job, but it is not always clear which files can be removed if multiple jobs are allowed to run on a single node of the supercomputer. And the software that needs to local storage most, also tends to be the software that cannot use a full node of a supercomputer efficiently. Modern Linux does have a solution to the cleaning problem (namespaces), but that solution then comes with other restrictions that users may have trouble living with, e.g., starting processes on other nodes should always be done through the resource manager software to ensure that the processes start in the right namespace. Which implies that, e.g., ssh , a very popular mechanism to start a session on a different host, should not be used. Moreover, the data becomes inaccessible to the user when the job ends, so if the so-called job script , the script that instructs the supercomputer what to do, is ended because the resources expire or because of a crash, the data on the local drive will be lost to the user. For reading one also loses time as the data needs to be copied first to the local drive. It is physically also not easy to add local drives to a supercomputer node. Supercomputer nodes are built with very high density as it is important to keep all links in a supercomputer as short as possible to minimise communication delays between nodes. Modern CPUs and GPUs run very hot, while storage prefers a lower temperature. On an air cooled node, the storage has to be early in the air flow through the node as once the air has gone through the CPU or GPU coolers, it is way too hot to cool the storage sufficiently. But given the small size of a supercomputer node, those storage devices may hinder the air flow through the node and hence make the cooling less effective. On a water cooled node, things aren't that much easier though the situation is improving. As M-type SSDs (those that you insert in a slot on the motherboard close to the CPU) nowadays even need cooling in a regular PC, they have been made more friendly to the addition of cooling elements. However, reliability of SSD drives is also an issue. SSD drives based on flash memory (which is currently practically any drive still in production) have a limited life span under a heavy write load, while the use suggested here, as a temporary buffer for the duration of a job, is precisely a scenario with a high write load. Replacing broken hardware is an issue, made worst because of the dense construction of a supercomputer. One may wonder why local drives are so much more common in cloud infrastructure. The constraints in cloud infrastructure are different. Supercomputers, with the exception of some commercial clusters, are built to be as cost-effective as possible. So one tends to solve problems with better software rather than adding more hardware. Cloud infrastructures on the other hand are usually commercial offerings. They are built at a very large scale, with often an even more custom hardware design, and they are simply overprovisioned. E.g., a server may have two SSD drives where the software will simply switch over to the second drive when the first one breaks, but the broken one will never be replaced. The management model of a cloud infrastructure is also very different. Cloud is based on virtualisation technologies to isolate users, and let users built a virtual network of servers in which regular Linux access methods can be used. These layers of software add additional overhead (even with hardware features to support virtualisation) which is undesirable on a supercomputer where each additional 100 nanoseconds of communication delay may limit how far a job can scale on the computer. Note though that the virtualisation overhead, thanks to hardware support, has become low enough that small supercomputer jobs can run very well on some cloud infrastructures.","title":"Physics and the network"},{"location":"4_Storage/4_02_Problems/#metadata","text":"Each file contains both the actual data in the file, and so-called metadata such as the name of the file, access rights to the file, the date of creation and of last use, ... This metadata is stored in a directory which on a traditional file system with folder and subfolder structure is a special type of file for each (sub)directory. This implies that if you do many small disk accesses to files in the same directory, or store a lot of files in a single directory and access them simultaneously, you create a bottleneck as many updates are needed to that directory. Most file systems are not very good at parallelising that directory access. Bad patterns of metadata access are probably the most common source for performance problems on supercomputer file systems. A typical scenario is when in a distributed memory application, each process creates its own set of files in the same shared directory, rather than use a feature called parallel file I/O to create a few giant files to which data is written in orchestrated multi-node operations. This can bring almost every supercomputer file system to its knees. Building a file system that can cope with this load might be technologically impossible or would at least make storage an order of magnitude more expensive. An equally stupid idea is to open a file before every write and then close it again to ensure that data is written to disk immediately. This is already an expensive operation on a PC with local storage that will slow down your program a lot if those writes are very frequent, but it will kill almost any networked file system and is even worse for the so-called parallel file systems on supercomputers as they tend to have more expensive open and close operations.","title":"Metadata"},{"location":"4_Storage/4_03_Parallel_filesystems/","text":"Parallel file systems \u00b6 On PCs and many more regular file server file systems, each block on disk can be used for either metadata or data. (Well, in practice there will be a zone that is used exclusively for metadata but that zone is extended transparently when needed.) These file systems are very flexible and support small file sizes very well. But at the same time it is very difficult to ge very high bandwidth, as a file will be managed by a single storage server, so even with the fastest and most expensive storage attached to the storage servers, the single server would ultimately be a performance bottleneck. Larger supercomputers need a different kind of file system for higher performance, one where multiple servers can be used concurrently to access a single file. This is called a parallel file system. Popular examples and their use at the VSC Lustre is probably the most used file parallel file system on large supercomputers. It is used at KU Leuven, the VSC Tier-1 system Hortense at UGent and on the LUMI system in Finland, a pre-exascale supercomputer project to which Belgium and Flanders participate. Lustre uses two layers. It is implemented on top of another file system that is used for the actual disk accesses. Lustre is open-source but with companies offering support and doing most of the development. BeeGFS has a very similar architecture as Lustre and is used for the scratch file system on the clusters of the University of Antwerp. BeeGFS is open-source, but most of the development is done by a spin-off of a German research laboratory, and they also offer (paid) support. In the early days of the VSC, IBM Spectrum Scale , then known as GPFS , was used on all clusters. However, increasing licensing costs made this impossible. Spectrum Scale/GPFS doesn't have the same two-layer architecture as Lustre or BeeGFS (or at least, it is not visible), but does the full managment of the disks itself. Panasas PanFS is a storage system that has its roots in the same research project as Lustre. It is a commercial offering consisting of software and dedicated hardware. WEKA is also a fully commercial offering, but one running on more standard file server hardware. It requires a full SSD system though which makes it a rather expensive offering. In a parallel file system, the metadata is separated from the actual data. The metadata servers take care of all metadata operations, which include controlling the process of opening and closing files. However, after that the content of the file can be served by multiple servers, called the object servers in Lustre and BeeGFS (but not to be confuse with object storage such as Amazon S3 or Ceph). Each object server is responsible for certain parts of the file, but a big read or write access can engage multiple servers simultaneously. Parallel file systems will use their own client software to access the file storage. The metadata server(s) will pass all information that is needed to the clients on the nodes involved with the file access, so that the clients can then directly talk to the object servers that are involved with the data transfer. But since the metadata servers are not involved with the actual data transfer, it should be clear that opening and closing a file is a more expensive operation, as the metadata server has to pass the necessary information to the client(s) when opening a file and orchestrate the opening and closing of the file with the object servers. Such a setup can produce very high bandwidth for large read and write operations coming from optimised parallel software using the right libraries to optimise that data transport, and this at a very reasonable cost. However, it has trouble dealing with lots of small files, and metadata access, certainly to files in one directory, can be a bottleneck. Moreover, the amount of storage space for metadata is determined when the system is purchased or configured as it is on separate servers, so there is also a strict limit to the number of files such a system can store. It is not uncommon nowadays for supercomputer centres to impose strict limits on what users can do on the shared file systems. Just as software needs to adapt to the processing model of a supercomputer, software also needs to adapt to the way large storage systems on supercomputers work. You cannot simply run hundreds of instances of that naive PC program that accesses thousands of files in a short time on your supercomputer, but you have to organise your data in a better way to scale to supercomputer storage. The storage on the clusters at the University of Antwerp At the CalcUA supercomputer service, we have different storage systems. The Linux home directory is kept small deliberately so that it can be served from a few SSD drives in a very high reliability setup to prevent data loss. The system is small, hence a broken drive should not break the bank, while there are clear advantages to using SSD drives for home directories. The home directories are on a regular Linux file system served to the compute nodes via NFS as that better fits the typical data access pattern to the home directory than a parallel file system. The centrally installed applications are also installed on SSD drives. As program files are relatively small to very small and as packages like Python and R need thousands of very small files, it is served by a regular Linux file system exported to the compute nodes via NFS as this setup is better at dealing with small files than a parallel file system, certainly on a cluster the size of the one at the University of Antwerp. Next there is a 50TB file system on regular hard drives in a RAID setup with two redundant drives for every 8 drives. It uses a regular Linux file system served through NFS. We made this choice to also have a somewhat larger file system that can deal better with small files than a parallel file system can. It is, e.g., a good file system for users who want to build their own Python or R installation. The largest file system is a roughly 0.6 PB scratch file system. That uses 105 regular hard drives spread over 7 object servers for the storage of the data, and a number of SSDs for the metadata. The low latency of SSDs is very important to get good performance from the metadata servers, and again a broken SSD won't break the bank as there are only few. But for the object servers hard disks are used as they are an order of magnitude cheaper than SSDs.","title":"Parallel file systems"},{"location":"4_Storage/4_03_Parallel_filesystems/#parallel-file-systems","text":"On PCs and many more regular file server file systems, each block on disk can be used for either metadata or data. (Well, in practice there will be a zone that is used exclusively for metadata but that zone is extended transparently when needed.) These file systems are very flexible and support small file sizes very well. But at the same time it is very difficult to ge very high bandwidth, as a file will be managed by a single storage server, so even with the fastest and most expensive storage attached to the storage servers, the single server would ultimately be a performance bottleneck. Larger supercomputers need a different kind of file system for higher performance, one where multiple servers can be used concurrently to access a single file. This is called a parallel file system. Popular examples and their use at the VSC Lustre is probably the most used file parallel file system on large supercomputers. It is used at KU Leuven, the VSC Tier-1 system Hortense at UGent and on the LUMI system in Finland, a pre-exascale supercomputer project to which Belgium and Flanders participate. Lustre uses two layers. It is implemented on top of another file system that is used for the actual disk accesses. Lustre is open-source but with companies offering support and doing most of the development. BeeGFS has a very similar architecture as Lustre and is used for the scratch file system on the clusters of the University of Antwerp. BeeGFS is open-source, but most of the development is done by a spin-off of a German research laboratory, and they also offer (paid) support. In the early days of the VSC, IBM Spectrum Scale , then known as GPFS , was used on all clusters. However, increasing licensing costs made this impossible. Spectrum Scale/GPFS doesn't have the same two-layer architecture as Lustre or BeeGFS (or at least, it is not visible), but does the full managment of the disks itself. Panasas PanFS is a storage system that has its roots in the same research project as Lustre. It is a commercial offering consisting of software and dedicated hardware. WEKA is also a fully commercial offering, but one running on more standard file server hardware. It requires a full SSD system though which makes it a rather expensive offering. In a parallel file system, the metadata is separated from the actual data. The metadata servers take care of all metadata operations, which include controlling the process of opening and closing files. However, after that the content of the file can be served by multiple servers, called the object servers in Lustre and BeeGFS (but not to be confuse with object storage such as Amazon S3 or Ceph). Each object server is responsible for certain parts of the file, but a big read or write access can engage multiple servers simultaneously. Parallel file systems will use their own client software to access the file storage. The metadata server(s) will pass all information that is needed to the clients on the nodes involved with the file access, so that the clients can then directly talk to the object servers that are involved with the data transfer. But since the metadata servers are not involved with the actual data transfer, it should be clear that opening and closing a file is a more expensive operation, as the metadata server has to pass the necessary information to the client(s) when opening a file and orchestrate the opening and closing of the file with the object servers. Such a setup can produce very high bandwidth for large read and write operations coming from optimised parallel software using the right libraries to optimise that data transport, and this at a very reasonable cost. However, it has trouble dealing with lots of small files, and metadata access, certainly to files in one directory, can be a bottleneck. Moreover, the amount of storage space for metadata is determined when the system is purchased or configured as it is on separate servers, so there is also a strict limit to the number of files such a system can store. It is not uncommon nowadays for supercomputer centres to impose strict limits on what users can do on the shared file systems. Just as software needs to adapt to the processing model of a supercomputer, software also needs to adapt to the way large storage systems on supercomputers work. You cannot simply run hundreds of instances of that naive PC program that accesses thousands of files in a short time on your supercomputer, but you have to organise your data in a better way to scale to supercomputer storage. The storage on the clusters at the University of Antwerp At the CalcUA supercomputer service, we have different storage systems. The Linux home directory is kept small deliberately so that it can be served from a few SSD drives in a very high reliability setup to prevent data loss. The system is small, hence a broken drive should not break the bank, while there are clear advantages to using SSD drives for home directories. The home directories are on a regular Linux file system served to the compute nodes via NFS as that better fits the typical data access pattern to the home directory than a parallel file system. The centrally installed applications are also installed on SSD drives. As program files are relatively small to very small and as packages like Python and R need thousands of very small files, it is served by a regular Linux file system exported to the compute nodes via NFS as this setup is better at dealing with small files than a parallel file system, certainly on a cluster the size of the one at the University of Antwerp. Next there is a 50TB file system on regular hard drives in a RAID setup with two redundant drives for every 8 drives. It uses a regular Linux file system served through NFS. We made this choice to also have a somewhat larger file system that can deal better with small files than a parallel file system can. It is, e.g., a good file system for users who want to build their own Python or R installation. The largest file system is a roughly 0.6 PB scratch file system. That uses 105 regular hard drives spread over 7 object servers for the storage of the data, and a number of SSDs for the metadata. The low latency of SSDs is very important to get good performance from the metadata servers, and again a broken SSD won't break the bank as there are only few. But for the object servers hard disks are used as they are an order of magnitude cheaper than SSDs.","title":"Parallel file systems"},{"location":"4_Storage/4_04_Revolution/","text":"A storage revolution? \u00b6 Why are flash-based SSDs (not) the solution? \u00b6 University of Antwerp-specific The joint bandwidth on the BeeGFS scratch file system at the CalcUA compute service in use in 2022 is on the order of 7 GB/s. At the same time, some NVMe SSDs for PCIe 4 also claim to offer a read bandwidth of 7 GB/s and a write bandwidth that is not that much lower. So one could wonder if we shouldn't use 105 of those drives instead. There is of course the cost of the drives. But also a lot more server hardware would be needed simply to connect the drives and also to support the bandwidth over the interconnect. The following table shows prices and properties for some drives in early 2022: Seagate Exos X20 Seagate Nytro 3732 Seagate Nytro 3332 Samsung 980 Pro Samsung 970 EVO Plus Samsung 870 QVO Technology spinning magnetic disks 3D eTLC NAND flash 3D eTLC NAND flash TLC V-NAND flash TLC V-NAND flash QLC V-NAND Flash Market datacenter (SAS) datacenter (SAS) datacenter (2xSAS) prosumer (NVMe) consumer (NVMe) consumer (SATA) Capacity 20 TB 3.2 TB 15.36 TB 2 TB 2 TB 8 TB Read speed 0.28 GB/s 1.1 GB/s 1.05-2.1 GB/s 7 GB/s 3.5 GB/s 0.56 GB/s Write speed 0.28 GB/s 1 GB/s 0.95-1 GB/s 5.1 GB/s 3.3 GB/s 0.53 GB/s Latency 4,16 ms 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? Endurance ? 58.4 PB 28 PB 1.2 PB 1.2 PB 2.88 PB DWPD ? 10 1 0,33 0.33 0.2 (@5 year) Data written/day ? 32 TB/day 15.3 TB/day 0.66 TB/day 0.66 TB/day 1.5 TB/day Time needed 8h50m 4h15m 2m9s 3m20 s 50 m Price 0.025-0.05 \u20ac/GB 0,85 \u20ac/GB 0,31 \u20ac/GB 0.16 \u20ac/GB 0.12 \u20ac/GB 0.08 \u20ac/GB In this table we compare a popular high-quality hard drive for use in the datacenter with several NAND flash based drives, ordered from the highest to the lowest quality measured in durability first and speed second. The Nytro 3732 is a very high endurance drive but only exists in relatively small capacities. It uses a SAS interface which is very popular in servers as it is a good interface to build large disk systems, where the drives are also further away from the CPU or in this case the drive controller. The 3332 is a somewhat similar drive but with much higher capacity but lower endurance. It has two SAS interfaces that can be used to get double the bandwidth. The Samsung 980 Pro is a NVMe drive in M.2-format, meant to be put in a PCIe 4 slot on the motherboard. This is a much faster interface than the one used in the two Nytro drives, which also explains its very high speed. But it is also a less scalable interface as long distance connections would be expensive, as are the switches that would be needed if the CPU does ot provide enough PCIe connections itself. The Samsung 970 EVO Plus is a slightly lower-end drive with a slower interface. All these drives use so-called TCL NAND, which stands for Triple Level Cell NAND, meaning that it stores 3 bits per memory cell. The last drive, the Samsung 870 QVO differs in two aspects from the other SAMSUNG drives and the datacenter drives: It uses QLC NAND, which stores 4 bits per cell, making it a bit cheaper, but also uses a much slower interface, SATA, which is an older interface that was very popular for hard disks in PCs. It also has a hard disk form factor and is not one that you plug in on the motherboard as the other two Samsung drives. For each of the drive series, we compare the larger capacity SKUs that one can get as after all we are interested in building big storage systems and as they also tend to be relatively cheaper. For SSDs, the larger capacity drives are sometimes also a bit faster than the lower capacity ones in the same series. This is because a flash drive itself is already a highly parallel device using parallelism over multiple banks of flash memory chips to increase the speed. The smaller drives in a given series may not have enough banks of flash memory to fully exploit all the parallelism that the controller chip of the drive (itself a processor more powerful than those in the first smartphones) is capable of using. The main weakness of hard drives and strength of flash drives becomes immediately clean when we look at the rows for (peak) read and write speed and for latency: Hard disks have much lower peak read and write speeds and a latency that is orders of magnitude higher. Note that we did not find precise latency numbers for the flash drives in the table, but the numbers are a reasonable estimate based on other drives for which the data was available. One should be careful interpreting the write bandwidth. Hard disks become slower for both reading and writing as they fill up partly because of mechanical reasons as the outer zones of the drive are also used and the read and write heads have to move more, and partly because of a phenomenon known as drive fragmentation, where data that belongs together gets spread all over the disk instead of stored in a single zone of the disk. There is software to try to correct the latter. But SSDs also get slower the more data is already on them, and this is more pronounced the more bits are stored per memory cell. But SSDs have another problem: Just as regular hard drives, data is written in blocks. But data cannot be erased or overwritten in single blocks. Before overwriting data, the block has to be erased, but this can only be done in clusters of blocks. Therefore, if a drive is getting full and data is written and rewritten all the time, the drive has to reorganise its storage all the time which can lead to write speeds that are much lower than on a well-organised hard disk. Some modern hard disks with very high capacity also have a similar problem, but those are only used for archiving while slightly lower capacity drives that don't have this problem are used for more active storage. And another problem is with drives that store multiple bits per cell, which is currently basically all drives. When the drive is rather empty and only one bit needs to be stored per cell, write speeds are much higher than when the drive is filling up and 3 or 4 bits are stored per cell. The table does however clearly show another problem of SSDs: endurance. Unfortunately endurance for hard disks and SSDs is measure differently so that it is hard to compare in the table. But basically, the Seagate Exos is suitable for near continuous reading and writing and will last for the 5 years of its warranty period. For SSDs the story is very different. In the early years, an SSD memory cell could be erased and rewritten several thousands of times without failing. However, both the miniaturisation and the use of multiple bits per cell have severely lowered that life span. Some flash memory cell can be erased and rewritten only 150 to 500 times without failing. Drives try to compensate for that by very clever write and erase strategies and by keeping some spare storage on the drive that can be used to replace worn out parts, so that a fixed size can be reported to the operating system. This technique is called wear levelling . This complex management of data on the drive to appear as a regular drive to the operating system is also one of the reasons why flash drives have actually rather powerful processors. Endurance of SSDs is measured in Drive Writes Per Day, abbreviated DWPD. In the table above, these have all been normalised to a 5-year life span of the drive. Another measure is the amount of data that can be written per day to have a 5 year life span of the drive. It is obvious that the larger the drive, the more data can be written also. We compare this with how long it would take to write that daily capacity to the drive in the (invalid) assumption that we could write at the maximum write bandwidth. Now we see that the Nytro 3732 is a very good drive. It supports 10 DWPD and even at its relatively small capacity this is still so much data per day that one could probably write almost continuously to it. It is certainly a drive suitable for a scenario with high write loads, as is the case for supercomputer scratch storage. For the Nytro 3332 which is 1 DWPD it is really only the capacity that saves us. It is slow enough that one can still write a lot of data to it. But if we would use a smaller and cheaper 1 DWPD in, e.g., a compute node, we would probably run into problems as that local drive in the compute node would typically be used to contain a temporary copy of large data sets, and hence have a lot of data written to it in each job. The Samsung NVMe drives are meant for a totally different access scenario though. They can only sustain 0.33 DWPD, and if you could write that data at the full write speed, this really means that you could only write to them for a few minutes per day. That high speed is certainly not meant to put a higher write load on the drives. On the contrary, these drives are meant for use in PCs under a typical workload for a PC, where much of the data on the drive is very static and consists of files that may be read often, or are archived for long times. The QVO drive can sustain only 0.2 DWPD, but still ingest quite a lot of data per day due to its high capacity. And due to its slow write speed due largely to the slow SATA interface, it also looks as if one can write for quite some time to it, but also given how much quad level cells can slow down when filling up, it is really a drive for archiving, and for storing data where the speed of the write operations doesn't really matter. This brings us to the last line of the table, the cost of the drives and here we see the big problem of SSDs. The cheaper Samsung drives may only be 2 to 5 times more expensive then the enterprise quality hard drive that we are comparing with, but this is really an apples-and-oranges comparison. Those cheaper SSDs are not really suitable for datacenter use. One could imagine building a storage system with high capacity for high read load scenarios from the QVO drives, and some companies build such storage, but these drives are not suited for the typical load on supercomputer file systems, neither for a local drive or for, e.g., the scratch file system. Some SSDs are suitable (and the Nytro 3732 certainly is) but then the cost is easily 10 times or more higher per byte than for good quality hard drives. This is one of the reasons why SSDs are not used more in supercomputer storage. After all, in supercomputer storage we need storage with a high capacity that can deal with a high write load. Nice to know Around 2010 a file storage expert at KU Leuven looked at how storage was used on their cluster to determine which type of storage should be bought. It turned out that the load was 90% write and only 10% read, which means that most of the data written to disk was even never read... It is hoped that this text makes clear that writing data is all but free. New memory types for a revolution? \u00b6 As we can see from the previous discussion, flash memory comes with a higher price tag than one would think from looking at prices for drives for PCs, and it has some other issues also, like durability issues as each memory cell has a very short lifespan in terms of number of rewrites, and the unpredictable slow-down especially under a random small writes load when the drive starts to fill up. Several companies have worked on other types of solid state memory for permanent storage that does not have those issues. Two memory types have been considered that would allow byte level access instead of block level, have much better write endurance and that also could be rewritten at the byte level rather than having to erase whole blocks and copying data round in the drive to free such a big block. HPE (then still called HP) and Sandisk explored a type of memory cell called memristor . Intel and Micron worked together on a type of memory cell which they called 3D-XPoint , sometimes marketed by Intel as Optane (though that name was later recycled for other types of storage also). The memristor never made it to market. 3D-XPoint did result in a couple of products, but they were only competitive for some very special markets. 3D-XPoint memory appeared in several forms. It was used to build datacenter solid state drives with excellent endurance. Even though the I/O performance wasn't stellar on paper, the fine print in the specs really mattered: 3D-XPoint drives coped much better with read and write loads that lacked enough parallelism, while most flash drives must look at a larger number of read and write requests simultaneously and reorder them to get close to their quoted speed. They were also used as a cache for flash drives, buffering writes and keeping some data that was read a lot also available as in some cases it could be read faster from the 3D-XPoint cache. And finally, it could also be plugged in some servers instead of RAM. However, it didn't act as RAM at all as it is still a lot slower than regular RAM, and as its write endurance doesn't come close to that of RAM memory. Instead, it was marketed as memory for large database servers where much of the database could be kept in 3D-XPoint memory yet accessed as if it were RAM, at a lower cost as a fully RAM-equipped system and at a higher reliability in case of, e.g., power problems. The last usage scenario is an example of so-called Storage Class Memory (sometimes abbreviated as SCM): Memory that can operate as (slower) RAM but that just as regular disks storage maintains its state when the system is powered off. However, developing a new memory technology to compete with an already established and very far developed technology is hard and requires extremely deep pockets. In the end, technological evolution created good enough alternatives for many of the use cases of 3D-XPoint memory, or people simply didn't see enough benefits to pay for it, and the development was stopped in 2021 by Micron and 2022 by Intel. High-endurance SSDs are simply much cheaper than 3D-XPoint drives and are a good alternative for all but a few cases where software has a really bad drive access pattern. But then for bigger companies it is probably cheaper to simply rework the software to shine on cheaper drives. The benefit of 3D-XPoint as a cache for SSDs was questionable because of the small size and also because of the way it was implemented, increasing the complexity of the system, and nowadays some drive use some SLC (single bit per cell) flash memory as a cache. The RAM capacity per socket has also increased a lot with more memory channels per socket and larger memory modules. Another technology that would allow larger RAM memories is also coming up. Compute eXpress Link (\\CXL) is an open standard that build upon the PCIe standards to provide an interface that would be suitable for connecting several kinds of components in large systems: CPUs, GPUs, other accelerators with compute capability, additional RAM memory, ... It also builds on the experience with other technologies, as IBM's OpenCAPI, that tried to reach some of these goals though it is not compatible with any of those. It would even be possible to build networks of CXL connections to build a reconfigurable computer: A user can select the number of CPUs, GPUs, memory blocks, etc., and those are connected on the fly the the CXL fabric. This may sound nice but it remains to be seen how useful this will be in practice. It is not possible to make very large switched fabrics as the latency would simply be too large to use this in a way current memory and accelerators are used. On the contrary, as we shall also see in the chapter on accelerators and as we have already seen to some extent in the chapter on memory technology , the needs for many supercomputer applications but also regular applications are exactly the opposite. Large memories are useless if they also come with much higher latency unless applications are reworked to hide the latency and make clever use of the memory hierarchy with nearby faster RAM and more remote larger but slower RAM. As we shall see, the performance improvement that one can obtain from using accelerators can also be limited by the data transfers to and from the accelerators. Not all applications can be reworked to cope with the much higher latency in such a CXL-based reconfigurable system or even just an ordinary server with a large bank of slower memory. In fact, many applications need in fact the opposite, a much closer integration of memory, CPU and accelerators. This is precisely the reason why, e.g., the Apple M-series processors sometimes provide much better performance than one would expect from the chip in applications. Local storage in supercomputer nodes \u00b6 As the speed difference between the processing capacity of a supercomputer node and the storage keeps increasing, there is a renewed interest in adding local storage again to compute nodes, something that certainly the large supercomputers avoided because of reliability and management issues. Modern high-end SSDs have become fairly reliable and as shapes have mostly standardised, it does become possible to build them into water cooled nodes without having a negative impact on the cooling of other components or a performance impact because of a too high temperature. Manufacturers are also working on software to make them more manageable in a supercomputer context and more useful also to parallel programs as those local SSDs cannot be accessed directly from other compute nodes. Intel DAOS was originally developed for the much delayed USA Aurora exascale system where it would work with 3D-XPoint drives in the compute nodes. It is also designed to integrate with the Lustre file system that will be used on Aurora. It is not clear how Intel envisions using DAOS though as it does rely on storage class memory and not only NVMe drives, and was really designed with 3D-XPoint in mind and its server processors with built-in support for that memory. HPE is working a what they call a near-node storage system code-named Rabbits for the third USA exascale computer, El Capitan. It consists of a storage server that sits close to a number of compute nodes with fast dedicated PCIe connection to each of them. The server has its own processor so can work independently from the compute nodes to, e.g., transfer data that was written by the job to the larger remote Lustre file system. Each server has 16 SSDs but also two spares so that it can reconfigure automatically when an SSD fails. These SSDs can be accessed as if they are a directly attached drive, essentially operating as an SSD in the node, or as a network drive acting as a cache to the larger remote Lustre file system. It will work in conjunction with a new scheduler as Slurm cannot easily be made sufficiently aware of the architecture of the attached software to manage it and allocate proper resources.","title":"A storage revolution?"},{"location":"4_Storage/4_04_Revolution/#a-storage-revolution","text":"","title":"A storage revolution?"},{"location":"4_Storage/4_04_Revolution/#why-are-flash-based-ssds-not-the-solution","text":"University of Antwerp-specific The joint bandwidth on the BeeGFS scratch file system at the CalcUA compute service in use in 2022 is on the order of 7 GB/s. At the same time, some NVMe SSDs for PCIe 4 also claim to offer a read bandwidth of 7 GB/s and a write bandwidth that is not that much lower. So one could wonder if we shouldn't use 105 of those drives instead. There is of course the cost of the drives. But also a lot more server hardware would be needed simply to connect the drives and also to support the bandwidth over the interconnect. The following table shows prices and properties for some drives in early 2022: Seagate Exos X20 Seagate Nytro 3732 Seagate Nytro 3332 Samsung 980 Pro Samsung 970 EVO Plus Samsung 870 QVO Technology spinning magnetic disks 3D eTLC NAND flash 3D eTLC NAND flash TLC V-NAND flash TLC V-NAND flash QLC V-NAND Flash Market datacenter (SAS) datacenter (SAS) datacenter (2xSAS) prosumer (NVMe) consumer (NVMe) consumer (SATA) Capacity 20 TB 3.2 TB 15.36 TB 2 TB 2 TB 8 TB Read speed 0.28 GB/s 1.1 GB/s 1.05-2.1 GB/s 7 GB/s 3.5 GB/s 0.56 GB/s Write speed 0.28 GB/s 1 GB/s 0.95-1 GB/s 5.1 GB/s 3.3 GB/s 0.53 GB/s Latency 4,16 ms 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? 20 \u00b5s ??? Endurance ? 58.4 PB 28 PB 1.2 PB 1.2 PB 2.88 PB DWPD ? 10 1 0,33 0.33 0.2 (@5 year) Data written/day ? 32 TB/day 15.3 TB/day 0.66 TB/day 0.66 TB/day 1.5 TB/day Time needed 8h50m 4h15m 2m9s 3m20 s 50 m Price 0.025-0.05 \u20ac/GB 0,85 \u20ac/GB 0,31 \u20ac/GB 0.16 \u20ac/GB 0.12 \u20ac/GB 0.08 \u20ac/GB In this table we compare a popular high-quality hard drive for use in the datacenter with several NAND flash based drives, ordered from the highest to the lowest quality measured in durability first and speed second. The Nytro 3732 is a very high endurance drive but only exists in relatively small capacities. It uses a SAS interface which is very popular in servers as it is a good interface to build large disk systems, where the drives are also further away from the CPU or in this case the drive controller. The 3332 is a somewhat similar drive but with much higher capacity but lower endurance. It has two SAS interfaces that can be used to get double the bandwidth. The Samsung 980 Pro is a NVMe drive in M.2-format, meant to be put in a PCIe 4 slot on the motherboard. This is a much faster interface than the one used in the two Nytro drives, which also explains its very high speed. But it is also a less scalable interface as long distance connections would be expensive, as are the switches that would be needed if the CPU does ot provide enough PCIe connections itself. The Samsung 970 EVO Plus is a slightly lower-end drive with a slower interface. All these drives use so-called TCL NAND, which stands for Triple Level Cell NAND, meaning that it stores 3 bits per memory cell. The last drive, the Samsung 870 QVO differs in two aspects from the other SAMSUNG drives and the datacenter drives: It uses QLC NAND, which stores 4 bits per cell, making it a bit cheaper, but also uses a much slower interface, SATA, which is an older interface that was very popular for hard disks in PCs. It also has a hard disk form factor and is not one that you plug in on the motherboard as the other two Samsung drives. For each of the drive series, we compare the larger capacity SKUs that one can get as after all we are interested in building big storage systems and as they also tend to be relatively cheaper. For SSDs, the larger capacity drives are sometimes also a bit faster than the lower capacity ones in the same series. This is because a flash drive itself is already a highly parallel device using parallelism over multiple banks of flash memory chips to increase the speed. The smaller drives in a given series may not have enough banks of flash memory to fully exploit all the parallelism that the controller chip of the drive (itself a processor more powerful than those in the first smartphones) is capable of using. The main weakness of hard drives and strength of flash drives becomes immediately clean when we look at the rows for (peak) read and write speed and for latency: Hard disks have much lower peak read and write speeds and a latency that is orders of magnitude higher. Note that we did not find precise latency numbers for the flash drives in the table, but the numbers are a reasonable estimate based on other drives for which the data was available. One should be careful interpreting the write bandwidth. Hard disks become slower for both reading and writing as they fill up partly because of mechanical reasons as the outer zones of the drive are also used and the read and write heads have to move more, and partly because of a phenomenon known as drive fragmentation, where data that belongs together gets spread all over the disk instead of stored in a single zone of the disk. There is software to try to correct the latter. But SSDs also get slower the more data is already on them, and this is more pronounced the more bits are stored per memory cell. But SSDs have another problem: Just as regular hard drives, data is written in blocks. But data cannot be erased or overwritten in single blocks. Before overwriting data, the block has to be erased, but this can only be done in clusters of blocks. Therefore, if a drive is getting full and data is written and rewritten all the time, the drive has to reorganise its storage all the time which can lead to write speeds that are much lower than on a well-organised hard disk. Some modern hard disks with very high capacity also have a similar problem, but those are only used for archiving while slightly lower capacity drives that don't have this problem are used for more active storage. And another problem is with drives that store multiple bits per cell, which is currently basically all drives. When the drive is rather empty and only one bit needs to be stored per cell, write speeds are much higher than when the drive is filling up and 3 or 4 bits are stored per cell. The table does however clearly show another problem of SSDs: endurance. Unfortunately endurance for hard disks and SSDs is measure differently so that it is hard to compare in the table. But basically, the Seagate Exos is suitable for near continuous reading and writing and will last for the 5 years of its warranty period. For SSDs the story is very different. In the early years, an SSD memory cell could be erased and rewritten several thousands of times without failing. However, both the miniaturisation and the use of multiple bits per cell have severely lowered that life span. Some flash memory cell can be erased and rewritten only 150 to 500 times without failing. Drives try to compensate for that by very clever write and erase strategies and by keeping some spare storage on the drive that can be used to replace worn out parts, so that a fixed size can be reported to the operating system. This technique is called wear levelling . This complex management of data on the drive to appear as a regular drive to the operating system is also one of the reasons why flash drives have actually rather powerful processors. Endurance of SSDs is measured in Drive Writes Per Day, abbreviated DWPD. In the table above, these have all been normalised to a 5-year life span of the drive. Another measure is the amount of data that can be written per day to have a 5 year life span of the drive. It is obvious that the larger the drive, the more data can be written also. We compare this with how long it would take to write that daily capacity to the drive in the (invalid) assumption that we could write at the maximum write bandwidth. Now we see that the Nytro 3732 is a very good drive. It supports 10 DWPD and even at its relatively small capacity this is still so much data per day that one could probably write almost continuously to it. It is certainly a drive suitable for a scenario with high write loads, as is the case for supercomputer scratch storage. For the Nytro 3332 which is 1 DWPD it is really only the capacity that saves us. It is slow enough that one can still write a lot of data to it. But if we would use a smaller and cheaper 1 DWPD in, e.g., a compute node, we would probably run into problems as that local drive in the compute node would typically be used to contain a temporary copy of large data sets, and hence have a lot of data written to it in each job. The Samsung NVMe drives are meant for a totally different access scenario though. They can only sustain 0.33 DWPD, and if you could write that data at the full write speed, this really means that you could only write to them for a few minutes per day. That high speed is certainly not meant to put a higher write load on the drives. On the contrary, these drives are meant for use in PCs under a typical workload for a PC, where much of the data on the drive is very static and consists of files that may be read often, or are archived for long times. The QVO drive can sustain only 0.2 DWPD, but still ingest quite a lot of data per day due to its high capacity. And due to its slow write speed due largely to the slow SATA interface, it also looks as if one can write for quite some time to it, but also given how much quad level cells can slow down when filling up, it is really a drive for archiving, and for storing data where the speed of the write operations doesn't really matter. This brings us to the last line of the table, the cost of the drives and here we see the big problem of SSDs. The cheaper Samsung drives may only be 2 to 5 times more expensive then the enterprise quality hard drive that we are comparing with, but this is really an apples-and-oranges comparison. Those cheaper SSDs are not really suitable for datacenter use. One could imagine building a storage system with high capacity for high read load scenarios from the QVO drives, and some companies build such storage, but these drives are not suited for the typical load on supercomputer file systems, neither for a local drive or for, e.g., the scratch file system. Some SSDs are suitable (and the Nytro 3732 certainly is) but then the cost is easily 10 times or more higher per byte than for good quality hard drives. This is one of the reasons why SSDs are not used more in supercomputer storage. After all, in supercomputer storage we need storage with a high capacity that can deal with a high write load. Nice to know Around 2010 a file storage expert at KU Leuven looked at how storage was used on their cluster to determine which type of storage should be bought. It turned out that the load was 90% write and only 10% read, which means that most of the data written to disk was even never read... It is hoped that this text makes clear that writing data is all but free.","title":"Why are flash-based SSDs (not) the solution?"},{"location":"4_Storage/4_04_Revolution/#new-memory-types-for-a-revolution","text":"As we can see from the previous discussion, flash memory comes with a higher price tag than one would think from looking at prices for drives for PCs, and it has some other issues also, like durability issues as each memory cell has a very short lifespan in terms of number of rewrites, and the unpredictable slow-down especially under a random small writes load when the drive starts to fill up. Several companies have worked on other types of solid state memory for permanent storage that does not have those issues. Two memory types have been considered that would allow byte level access instead of block level, have much better write endurance and that also could be rewritten at the byte level rather than having to erase whole blocks and copying data round in the drive to free such a big block. HPE (then still called HP) and Sandisk explored a type of memory cell called memristor . Intel and Micron worked together on a type of memory cell which they called 3D-XPoint , sometimes marketed by Intel as Optane (though that name was later recycled for other types of storage also). The memristor never made it to market. 3D-XPoint did result in a couple of products, but they were only competitive for some very special markets. 3D-XPoint memory appeared in several forms. It was used to build datacenter solid state drives with excellent endurance. Even though the I/O performance wasn't stellar on paper, the fine print in the specs really mattered: 3D-XPoint drives coped much better with read and write loads that lacked enough parallelism, while most flash drives must look at a larger number of read and write requests simultaneously and reorder them to get close to their quoted speed. They were also used as a cache for flash drives, buffering writes and keeping some data that was read a lot also available as in some cases it could be read faster from the 3D-XPoint cache. And finally, it could also be plugged in some servers instead of RAM. However, it didn't act as RAM at all as it is still a lot slower than regular RAM, and as its write endurance doesn't come close to that of RAM memory. Instead, it was marketed as memory for large database servers where much of the database could be kept in 3D-XPoint memory yet accessed as if it were RAM, at a lower cost as a fully RAM-equipped system and at a higher reliability in case of, e.g., power problems. The last usage scenario is an example of so-called Storage Class Memory (sometimes abbreviated as SCM): Memory that can operate as (slower) RAM but that just as regular disks storage maintains its state when the system is powered off. However, developing a new memory technology to compete with an already established and very far developed technology is hard and requires extremely deep pockets. In the end, technological evolution created good enough alternatives for many of the use cases of 3D-XPoint memory, or people simply didn't see enough benefits to pay for it, and the development was stopped in 2021 by Micron and 2022 by Intel. High-endurance SSDs are simply much cheaper than 3D-XPoint drives and are a good alternative for all but a few cases where software has a really bad drive access pattern. But then for bigger companies it is probably cheaper to simply rework the software to shine on cheaper drives. The benefit of 3D-XPoint as a cache for SSDs was questionable because of the small size and also because of the way it was implemented, increasing the complexity of the system, and nowadays some drive use some SLC (single bit per cell) flash memory as a cache. The RAM capacity per socket has also increased a lot with more memory channels per socket and larger memory modules. Another technology that would allow larger RAM memories is also coming up. Compute eXpress Link (\\CXL) is an open standard that build upon the PCIe standards to provide an interface that would be suitable for connecting several kinds of components in large systems: CPUs, GPUs, other accelerators with compute capability, additional RAM memory, ... It also builds on the experience with other technologies, as IBM's OpenCAPI, that tried to reach some of these goals though it is not compatible with any of those. It would even be possible to build networks of CXL connections to build a reconfigurable computer: A user can select the number of CPUs, GPUs, memory blocks, etc., and those are connected on the fly the the CXL fabric. This may sound nice but it remains to be seen how useful this will be in practice. It is not possible to make very large switched fabrics as the latency would simply be too large to use this in a way current memory and accelerators are used. On the contrary, as we shall also see in the chapter on accelerators and as we have already seen to some extent in the chapter on memory technology , the needs for many supercomputer applications but also regular applications are exactly the opposite. Large memories are useless if they also come with much higher latency unless applications are reworked to hide the latency and make clever use of the memory hierarchy with nearby faster RAM and more remote larger but slower RAM. As we shall see, the performance improvement that one can obtain from using accelerators can also be limited by the data transfers to and from the accelerators. Not all applications can be reworked to cope with the much higher latency in such a CXL-based reconfigurable system or even just an ordinary server with a large bank of slower memory. In fact, many applications need in fact the opposite, a much closer integration of memory, CPU and accelerators. This is precisely the reason why, e.g., the Apple M-series processors sometimes provide much better performance than one would expect from the chip in applications.","title":"New memory types for a revolution?"},{"location":"4_Storage/4_04_Revolution/#local-storage-in-supercomputer-nodes","text":"As the speed difference between the processing capacity of a supercomputer node and the storage keeps increasing, there is a renewed interest in adding local storage again to compute nodes, something that certainly the large supercomputers avoided because of reliability and management issues. Modern high-end SSDs have become fairly reliable and as shapes have mostly standardised, it does become possible to build them into water cooled nodes without having a negative impact on the cooling of other components or a performance impact because of a too high temperature. Manufacturers are also working on software to make them more manageable in a supercomputer context and more useful also to parallel programs as those local SSDs cannot be accessed directly from other compute nodes. Intel DAOS was originally developed for the much delayed USA Aurora exascale system where it would work with 3D-XPoint drives in the compute nodes. It is also designed to integrate with the Lustre file system that will be used on Aurora. It is not clear how Intel envisions using DAOS though as it does rely on storage class memory and not only NVMe drives, and was really designed with 3D-XPoint in mind and its server processors with built-in support for that memory. HPE is working a what they call a near-node storage system code-named Rabbits for the third USA exascale computer, El Capitan. It consists of a storage server that sits close to a number of compute nodes with fast dedicated PCIe connection to each of them. The server has its own processor so can work independently from the compute nodes to, e.g., transfer data that was written by the job to the larger remote Lustre file system. Each server has 16 SSDs but also two spares so that it can reconfigure automatically when an SSD fails. These SSDs can be accessed as if they are a directly attached drive, essentially operating as an SSD in the node, or as a network drive acting as a cache to the larger remote Lustre file system. It will work in conjunction with a new scheduler as Slurm cannot easily be made sufficiently aware of the architecture of the attached software to manage it and allocate proper resources.","title":"Local storage in supercomputer nodes"},{"location":"4_Storage/4_05_To_remember/","text":"To remember from this chapter \u00b6 Supercomputer file systems, just as their compute capacity, are build from relatively standard but better reliability components. Their speed does not come from novel physics or so but just from clever software. Supercomputers like large files and large reads or writes. Just as with memory, streaming data to and from a file is much faster than random access to data. It may in fact seem that a PC is less sensitive to this but this is only partially true. Even a PC SSD will have much higher bandwidth when streaming sufficiently large amounts of data to it then when using random access inside files and to many files. It is certainly essential to avoid writing many small files. E.g., when running 1000s of small jobs for a parameter study, it would be much better to accumulate the data in fewer files and/or to use a database to process the results. Opening and closing files is also very expensive on a parallel file system and should be avoided where possible. A good step towards improved use of supercomputer file systems is to use appropriate data formats. Libraries such as HDF5 , netCDF and SIONlib help to organise large amounts of data in a small number of files that work well on supercomputers. For some compression formats libraries exist that can be used to read data from the compressed archive directly into memory which may already improve performance when\\ a lot of small files would otherwise be read in, but of course it requires adapting the code to use such a library rather than the functions calls to read from the file system. Another remark that we have not yet really discussed is that it is not a good idea to read or write numeric data in text (ASCII or UTF) format. There is hardly a portability advantage to using text to store numbers as binary data formats have mostly been standardised with really two options remaining and many libraries and tools supporting both (little endian and big endian which defines the byte ordering for multi-byte numbers, but almost all CPUs nowadays use little endian). Storing numbers as ASCII text expands the needed capacity with a factor of approximately three and the conversion to and from binary data is very slow. When discussing scaling a storage system from the size of a PC storage system to the size of a supercomputer storage system, one should realise: Scaling capacity is cheap. Often one only needs to add disks and disk enclosures and not so much servers when using drives with interfaces that are specifically made for that (and in particular SAS drives as we have seen in the drive technology comparison table). Scaling bandwidth is harder and more expensive. Simply adding disks is not enough as we need more bandwidth between the disks and the servers and more bandwidth from the server to the interconnect. As both are limited, we will need to add more servers. Joint bandwidth to a number of files is also easier to scale (at least if those files are spread over the whole system and a sufficient number of requests come in in parallel). The strength of many supercomputers is that they also offer extremely scalable bandwidth to large files. But this also requires large parallel applications using a properly written parallel I/O framework. Scaling the number of I/O operations that the system can process per second (called IOPS) is extremely hard and expensive and is sometimes even physically impossible. This has several causes. Metadata access is much harder to parallelise, especially for access to a single (sub)directory by a single user. Due to the larger physical and logical distance of the application to the storage, the latency of I/O operations will be one or more magnitudes higher. This also means that if the I/O operations are launched sequentially and one has to finish before the next one can start, the application will run a lot slower than on a PC with fast local storage. In fact, it is doable to make supercomputer storage with very high IOPS when there are a lot of parallel I/O requests, from many users to files all over the file system so that metadata and data access can be spread over many servers. But it is not possible to improve IOPS for a single threaded application with synchronous file access.","title":"To remember"},{"location":"4_Storage/4_05_To_remember/#to-remember-from-this-chapter","text":"Supercomputer file systems, just as their compute capacity, are build from relatively standard but better reliability components. Their speed does not come from novel physics or so but just from clever software. Supercomputers like large files and large reads or writes. Just as with memory, streaming data to and from a file is much faster than random access to data. It may in fact seem that a PC is less sensitive to this but this is only partially true. Even a PC SSD will have much higher bandwidth when streaming sufficiently large amounts of data to it then when using random access inside files and to many files. It is certainly essential to avoid writing many small files. E.g., when running 1000s of small jobs for a parameter study, it would be much better to accumulate the data in fewer files and/or to use a database to process the results. Opening and closing files is also very expensive on a parallel file system and should be avoided where possible. A good step towards improved use of supercomputer file systems is to use appropriate data formats. Libraries such as HDF5 , netCDF and SIONlib help to organise large amounts of data in a small number of files that work well on supercomputers. For some compression formats libraries exist that can be used to read data from the compressed archive directly into memory which may already improve performance when\\ a lot of small files would otherwise be read in, but of course it requires adapting the code to use such a library rather than the functions calls to read from the file system. Another remark that we have not yet really discussed is that it is not a good idea to read or write numeric data in text (ASCII or UTF) format. There is hardly a portability advantage to using text to store numbers as binary data formats have mostly been standardised with really two options remaining and many libraries and tools supporting both (little endian and big endian which defines the byte ordering for multi-byte numbers, but almost all CPUs nowadays use little endian). Storing numbers as ASCII text expands the needed capacity with a factor of approximately three and the conversion to and from binary data is very slow. When discussing scaling a storage system from the size of a PC storage system to the size of a supercomputer storage system, one should realise: Scaling capacity is cheap. Often one only needs to add disks and disk enclosures and not so much servers when using drives with interfaces that are specifically made for that (and in particular SAS drives as we have seen in the drive technology comparison table). Scaling bandwidth is harder and more expensive. Simply adding disks is not enough as we need more bandwidth between the disks and the servers and more bandwidth from the server to the interconnect. As both are limited, we will need to add more servers. Joint bandwidth to a number of files is also easier to scale (at least if those files are spread over the whole system and a sufficient number of requests come in in parallel). The strength of many supercomputers is that they also offer extremely scalable bandwidth to large files. But this also requires large parallel applications using a properly written parallel I/O framework. Scaling the number of I/O operations that the system can process per second (called IOPS) is extremely hard and expensive and is sometimes even physically impossible. This has several causes. Metadata access is much harder to parallelise, especially for access to a single (sub)directory by a single user. Due to the larger physical and logical distance of the application to the storage, the latency of I/O operations will be one or more magnitudes higher. This also means that if the I/O operations are launched sequentially and one has to finish before the next one can start, the application will run a lot slower than on a PC with fast local storage. In fact, it is doable to make supercomputer storage with very high IOPS when there are a lot of parallel I/O requests, from many users to files all over the file system so that metadata and data access can be spread over many servers. But it is not possible to improve IOPS for a single threaded application with synchronous file access.","title":"To remember from this chapter"},{"location":"5_Summary1/","text":"Putting it all together \u00b6 Scaling Dennard scaling Transistor cost Three keywords: Streaming, Parallelism, and Hierarchy Andy and Bill's law Software first, not hardware","title":"Putting it all together"},{"location":"5_Summary1/#putting-it-all-together","text":"Scaling Dennard scaling Transistor cost Three keywords: Streaming, Parallelism, and Hierarchy Andy and Bill's law Software first, not hardware","title":"Putting it all together"},{"location":"5_Summary1/05_01_Scaling/","text":"In this chapter we summarise the previous three chapters on processor, memory and storage technology. We also want to stress that HPC stands for High Performance Computing and that a supercomputer is not like a very High-end Personal Computer. Scaling \u00b6 The performance of a computar cannot be understood from a single parameter. Instead many parameters characterise the performance of computer. The clock speed of a CPU is only one of those parameters. There are also many lantencies that need to be taken into account: memory latencies at the various levels of the memory hierarchy and of storage, communication latency, but if you would study into more detail then is possible in these lecture notes how to obtain maximal performance from a computer, also latencies in instruction execution. And there is also the bandwidth of several components that has to be taken into account: bandwidth to the various levels of the memory hierarchy and to storage, bandwidth between CPUs in a shared memory system and bandwidth of the interconnect. And the number of instructions a CPU can execute simultaneously is also a kind of bandwidth. When you're interested in solving big problems, you're also interested in the capacity of memory and storage. Not all these parameters are as cheap to scale, or improve over time at the same rate. As we will also discuss a bit further in this chapter, physical limitations have put a bound to improvments in CPU clock speed and latencies. The finite speed of light and speed of signals in copper wires is just one of those limitations. The growth of the bandwidth of memory, disks and network connections tends to be slower than the growt of the theoretical peak performance of a computer system. As a result of these restrictions it is simply not possible to build a supercomputer were all these parameters would be, e.g., 100 times better than in your PC or smartphone so that your PC software would simply run and run 100x faster. In fact, the opposite may be true. For some applications a High-end PC is unbeatable because of its compact size and (though less so nowadays) thin software layer as it is a personal device, as this guarantees minimal latencies. As we have seen in several examples, \"bigger\" often means higher latencies that need to be hidden. E.g., a bigger memory has to sit physically further from the CPU so access will be slower. A bigger disk system needs a different way of managing it then the SSD that sits just next to the processor in your PC and will be slower. We no longer build processors cores out of multiple chips, not only because it is not really needed anymore, but if we did the clock speed would be low as signals would travel too slowly to the other end of the processor core.","title":"Scaling"},{"location":"5_Summary1/05_01_Scaling/#scaling","text":"The performance of a computar cannot be understood from a single parameter. Instead many parameters characterise the performance of computer. The clock speed of a CPU is only one of those parameters. There are also many lantencies that need to be taken into account: memory latencies at the various levels of the memory hierarchy and of storage, communication latency, but if you would study into more detail then is possible in these lecture notes how to obtain maximal performance from a computer, also latencies in instruction execution. And there is also the bandwidth of several components that has to be taken into account: bandwidth to the various levels of the memory hierarchy and to storage, bandwidth between CPUs in a shared memory system and bandwidth of the interconnect. And the number of instructions a CPU can execute simultaneously is also a kind of bandwidth. When you're interested in solving big problems, you're also interested in the capacity of memory and storage. Not all these parameters are as cheap to scale, or improve over time at the same rate. As we will also discuss a bit further in this chapter, physical limitations have put a bound to improvments in CPU clock speed and latencies. The finite speed of light and speed of signals in copper wires is just one of those limitations. The growth of the bandwidth of memory, disks and network connections tends to be slower than the growt of the theoretical peak performance of a computer system. As a result of these restrictions it is simply not possible to build a supercomputer were all these parameters would be, e.g., 100 times better than in your PC or smartphone so that your PC software would simply run and run 100x faster. In fact, the opposite may be true. For some applications a High-end PC is unbeatable because of its compact size and (though less so nowadays) thin software layer as it is a personal device, as this guarantees minimal latencies. As we have seen in several examples, \"bigger\" often means higher latencies that need to be hidden. E.g., a bigger memory has to sit physically further from the CPU so access will be slower. A bigger disk system needs a different way of managing it then the SSD that sits just next to the processor in your PC and will be slower. We no longer build processors cores out of multiple chips, not only because it is not really needed anymore, but if we did the clock speed would be low as signals would travel too slowly to the other end of the processor core.","title":"Scaling"},{"location":"5_Summary1/05_02_Dennard_scaling/","text":"Dennard scaling \u00b6 For a long time, with every new generation of chip technology, roughly every two years, linear dimensions decreased by 30% (x0.7), and surface dimensions decreased by 50% (x0.7 2 ), i.e., transistor density doubled. Moreover the power density remained practically the same as voltage and currents needed to drive the circuits also lowered proportional to linear dimensions. Circuit delays went down by 30% (x0.7), so frequencies went up by 40% (0.7 -1 ). Though the first cracks already appeared some generations earlier, this really broke down around 2006. No longer did all dimensions of elements on an integrated circuit scale as well and hence transistor density did not grow as fast anymore. Moreover, the threshold voltage of semiconductors, the minimum voltage to let them switch, became more relevant and put a lower boundary on how much voltages can be reduced. Another element in the power consumption, leakage power, became more and more important if not dominant. And capacitances and inductances are such that the clock frequencies don't go up as fast anymore either. As a result of the breakdown of Dennard scaling, chips have become very hot and power consumption of supercomputers has become a major concern. Moreover, there is hardly any further speed increase anymore just from further reducing the component size, and in fact chip designers usually need to chose between a slight speed increase and a slight lowering of the power consumption per transistor. As a result of this designers need to look much harder for architectural improvements than before for further speed increases. The breakdown of dennard scaling is also part of the reason why latencies of various components and subsystems do no longer improve much. Transferring data has become the major source of power consumption in computers, more than doing the actual computations. Nowadays it takes more power to transfer two numbers from one end of a chip to the other end than do a simple arithmetic operation with those numbers. PCs already operate their hardware in the domain of Dennard scaling breakdown so there is no hope that one can design a single processor core that is much faster than the one in a PC in the current state of technology.","title":"Dennard scaling"},{"location":"5_Summary1/05_02_Dennard_scaling/#dennard-scaling","text":"For a long time, with every new generation of chip technology, roughly every two years, linear dimensions decreased by 30% (x0.7), and surface dimensions decreased by 50% (x0.7 2 ), i.e., transistor density doubled. Moreover the power density remained practically the same as voltage and currents needed to drive the circuits also lowered proportional to linear dimensions. Circuit delays went down by 30% (x0.7), so frequencies went up by 40% (0.7 -1 ). Though the first cracks already appeared some generations earlier, this really broke down around 2006. No longer did all dimensions of elements on an integrated circuit scale as well and hence transistor density did not grow as fast anymore. Moreover, the threshold voltage of semiconductors, the minimum voltage to let them switch, became more relevant and put a lower boundary on how much voltages can be reduced. Another element in the power consumption, leakage power, became more and more important if not dominant. And capacitances and inductances are such that the clock frequencies don't go up as fast anymore either. As a result of the breakdown of Dennard scaling, chips have become very hot and power consumption of supercomputers has become a major concern. Moreover, there is hardly any further speed increase anymore just from further reducing the component size, and in fact chip designers usually need to chose between a slight speed increase and a slight lowering of the power consumption per transistor. As a result of this designers need to look much harder for architectural improvements than before for further speed increases. The breakdown of dennard scaling is also part of the reason why latencies of various components and subsystems do no longer improve much. Transferring data has become the major source of power consumption in computers, more than doing the actual computations. Nowadays it takes more power to transfer two numbers from one end of a chip to the other end than do a simple arithmetic operation with those numbers. PCs already operate their hardware in the domain of Dennard scaling breakdown so there is no hope that one can design a single processor core that is much faster than the one in a PC in the current state of technology.","title":"Dennard scaling"},{"location":"5_Summary1/05_03_Cost_transistor/","text":"Cost of transistors \u00b6 The cost per transistor may not be the ideal number to compare chip process generations as chips are actually paid per wafer. And the cost per useful transistor on a wafer will also depend on the die size, how tolerant individual chips on the wafer are to defects, and the defect rate. It is however an easy number to work with. The following graph is based on numbers found in a report of the Marvell Technology Inc 2020 Investor Day. (Marvell Technology Inc is a fabless semiconductor company.) For a long time the cost per gate (or transistor) decreased rapidly with each new process generation. In fact, the cost per wafer did not increase much while the gate density doubled with each new process generation. This ended with the 28nm generation, From the 20nm generation on (which TSMC launched in 2014) the cost per gate did not longer decrease. Instead, the price per wafer started to grow again. This is also the moment that gates on a chip could no longer be made as flat 2D structures, but 3D structures were needed instead to control leakage, requiring more and more process steps as the miniaturisation further progressed. Nowadays processors and computers still become faster with every generation though not at as fast a rate as before (see the breakdown of Dennard scaling), but the cost starts to rise again. It is no coincidence that every new generation of high-end graphics cards or almost every new generation of high-end smartphones is more expensive than the previous one. The cost of high end processors for servers and supercomputers is also exploding the last couple of years. And the price per flop is hardly going down anymore. The only way this can still improve is by architectural innovations that make processors more efficient so that more work can be done per transistor. This also implies that we cannot expect a drastic growth in supercomputer performance in the near future without a matching growth of budgets. It also implies that the only way to get more research results on supercomputers without growth in funding is paying more attention to the quality of the software that is used and the way the supercomputer is used.","title":"Transistor cost"},{"location":"5_Summary1/05_03_Cost_transistor/#cost-of-transistors","text":"The cost per transistor may not be the ideal number to compare chip process generations as chips are actually paid per wafer. And the cost per useful transistor on a wafer will also depend on the die size, how tolerant individual chips on the wafer are to defects, and the defect rate. It is however an easy number to work with. The following graph is based on numbers found in a report of the Marvell Technology Inc 2020 Investor Day. (Marvell Technology Inc is a fabless semiconductor company.) For a long time the cost per gate (or transistor) decreased rapidly with each new process generation. In fact, the cost per wafer did not increase much while the gate density doubled with each new process generation. This ended with the 28nm generation, From the 20nm generation on (which TSMC launched in 2014) the cost per gate did not longer decrease. Instead, the price per wafer started to grow again. This is also the moment that gates on a chip could no longer be made as flat 2D structures, but 3D structures were needed instead to control leakage, requiring more and more process steps as the miniaturisation further progressed. Nowadays processors and computers still become faster with every generation though not at as fast a rate as before (see the breakdown of Dennard scaling), but the cost starts to rise again. It is no coincidence that every new generation of high-end graphics cards or almost every new generation of high-end smartphones is more expensive than the previous one. The cost of high end processors for servers and supercomputers is also exploding the last couple of years. And the price per flop is hardly going down anymore. The only way this can still improve is by architectural innovations that make processors more efficient so that more work can be done per transistor. This also implies that we cannot expect a drastic growth in supercomputer performance in the near future without a matching growth of budgets. It also implies that the only way to get more research results on supercomputers without growth in funding is paying more attention to the quality of the software that is used and the way the supercomputer is used.","title":"Cost of transistors"},{"location":"5_Summary1/05_04_Keywords/","text":"Three keywords: Streaming, Parallelism, and Hierarchy \u00b6 There are three keywords in programming for supercomputers: streaming, parallelism and hierarchy, and we will now discuss each of them separately. Streaming \u00b6 We have already mentioned several problems with data access on computers, not only supercomputers. Data access requires a lot of power. The further the data is from the processing units, the more power it costs to get the data to the processing unit. But unfortunately we can only store a limited amount of data really close to a processing unit. The further the memory is from the processing element, the higher the latency to get it there to process which was the main reason why cache memory was introduced (as it was introduced long before the power associated with data transport became a problem). Hence getting the data flowing smoothly through the computer, all the way from permanent storage to processing, is key to performance. An important way to deal with latency is the use of caches. Some are fully managed in hardware, like the caches between CPU and RAM memory, while others are managed in software, e.g., file systems also have a caching mechanism. It is important to ensure that those caches can work efficiently, and that requires: predictable data accesses, so that prefetching mechanisms can fetch data into the cache before it is needed so that part of the latency can be hidden, and data accesses in sufficiently large chunks to avoid that data in caches is never used and hence wasted and so that the effective bandwidth is not too much reduced by latency. If you'd fetch individual 4-byte numbers from RAM memory and if the memory latency would be 100ns and if there would be no way to have those data accesses overlap as you cannot predict what the next piece of data would be, then you effectively get at most a bandwidth of 4 bytes / 100ns / 1024^3 = 0,037 GByte/s which is only a fraction of the bandwidth one can get from RAM memory. Random access to small blocks of data is bad at all levels in computers, not only supercomputers. Only the definition of \"small\" varies depending on the type of memory or storage. For RAM memory \"small\" is on the order of a cache line or 64 bytes on many popular CPUs, for permanent storage \"small\" is measured in kilobytes or even 10s or 100s of kilobytes for shared parallel file systems. We haven't discussed the architecture of main memory in much detail, ub there also streaming is important to get the fastest performance as internally memory is also accessed and buffered in larger blocks than a single request, and a subsequent request to data that is already in that larger buffer will be quicker than an access to data that is not. This is also not a new message. Some level of streaming has been important in supercomputers ever since the 70s, and when it comes to permanent storage it has been important on PCs ever since the first PCs were build. One may have the impression that it has become less important with modern flash memory based SSDs, but that is only partly true. The latency is extremely small compared to any storage device that uses mechanically moving parts, but even then if you only access data through small files with size in the order of kilobytes so that the caching mechanisms in file systems cannot work, you will only reach a fraction of the theoretical bandwidth of those devices, and this again becomes more and more pronounced with every new generation as the peak bandwidth of SSDs improves quickly while the latency stays about the same. Parallelism \u00b6 Parallelism is important at all levels in supercomputers. We have thoroughly discussed the 4 levels of parallelism in a the processing units in a supercomputer: Instruction Level Parallelism and vectorisation (and in fact, recently matrix computations) in the core, the use of multiple cores in shared memory setup, and distributed memory parallel computing. We've also seen that parallelism is important in the design of storage systems for supercomputers, with even specialised shared storage systems. However, as we have mentioned, in fact all but the most basic SSDs in PCs and smartphones also rely on parallelism to reach the high bandwidth and high number of IO operations per second they promise to be capable of. We haven't discussed main memory in that much detail, but in fact modern computer memory also relies on parallelism. modern processor packages offer multiple memory controllers that work in parallel, and usually each memory controller already controls more than one memory channel. And even within a modern memory module there is already a level of parallelism. Most parallelism is not automatic. Instruction level parallelism does not require much work from a user though there are some programming techniques that improve and others that harm instruction level parallelism. Exploiting vector computation already requires more work from the programmer as we shall see in the following chapter, and matrix units are even harder to exploit. Shared and distributed memory parallelism almost always requires a significant effort from the programmer. Both can of course be exploited by simply running multiple binaries in a capacity computing context but that does not improve the time-to-solution for a single problem. Similarly, good performance of central storage is all but a given and requires careful thinking about how data is stored. Some storage systems may be more forgiving than other storage systems, but even on an SSD in a PC a bad data access pattern will leave a lot of potential of the storage system unused. This is also not a new lesson. Supercomputers have relied on forms of parallelism that require help from the programmer since the '70s. In PCs some form of vector computing returned in the '90s and became essential for floating point performance with the Pentium 4 processor in 2002. The first regular PCs with more than one core appeared already in 2006, and nowadays 4 cores or more are common in even laptops and the cheapest smartphones. In PCs single thread performance still improves over every generation but at the expense of steeply rising power consumption. In servers and supercomputers the single thread performance has been stagnating for several years already as they are more optimised for performance per Watt. Even though every new generation of core claims to do more work per clock cycle (better instruction level parallelism), the clock speed is often also lowered so that the net gain is practically zero. Just as streaming is almost as important on PCs as on supercomputers, the same also holds for parallelism. There is only one level of parallelism that is specific to supercomputers: distributed memory parallelism. And at the file system levels PCs also tend to be a lot more forgiving for bad access patterns, even though you will still be exploiting only 10 percent or less of the potential of your storage. Hierarchy \u00b6 Hierarchies appear in different ways in supercomputers, but also more and more in regular PCs. Memory is organised in a hierarchical way with typically 3 levels of cache where the first two levels tend to be organised per core while the third level is a cache that is shared by multiple if not all cores, and then often two or more levels of RAM memory. Many supercomputers are build out of building blocks with two processor sockets and accessing memory that is physically attached to the other socket is slower than accessing local main memory, but even within a socket not all memory might be equal. The latter is already the case on the AMD Epyc server processors and will also happen on some versions of the Intel Sapphire Rapids series that became available in early 2023. PCs also have the same cache hierarchy, but the main memory has only one level unless you opt for some workstation-class systems that are really built using variants of processors for servers. There is also a hierarchy in the levels of parallelism for processing. Instruction level parallelism and vectorisation is parallelism at a very fine scale as it is done in the instruction stream itself and does not require any special communication or synchronisation. Shared memory parallelism is the next level in the hierarchy. It comes with a startup cost and cost to synchronise the threads for some operations. It requires bigger chunks of work to be executed in parallel before the gain from parallelism offsets the startup and synchronisation costs. Distributed memory parallelism tends to be even coarser as the startup cost is higher and the communication between the processes is a lot more complicated then the communication between threads in a shared memory program. Discussing the hardware architecture and software parallelism models of GPUs in much detail is outside the scope of this two-lecture introduction. However, bot the hardware and the lower-level programming models for GPUs are very hierarchical. We can expect that parallel storage will also become more hierarchical than it is today as supercomputer manufacturers are looking for ways to bring some storage again closer to the processing elements without losing too much of the manageability, and as flash storage will remain too expensive in the forseeable future to build an all flash file system and hence can only be used for an extra level in the hierarchy. Exploiting the memory hierarchy is extremely important for performance as will also be illustrated later in this tutorial. Mapping threads and processes in shared and distributed memory parallel computing on that hierarchy is for many codes also important for performance. One may of course wish that it would be different, but in practice some understanding of the hardware architecture is needed even if you are only using parallel computers and not programming them.","title":"3 keywords"},{"location":"5_Summary1/05_04_Keywords/#three-keywords-streaming-parallelism-and-hierarchy","text":"There are three keywords in programming for supercomputers: streaming, parallelism and hierarchy, and we will now discuss each of them separately.","title":"Three keywords: Streaming, Parallelism, and Hierarchy"},{"location":"5_Summary1/05_04_Keywords/#streaming","text":"We have already mentioned several problems with data access on computers, not only supercomputers. Data access requires a lot of power. The further the data is from the processing units, the more power it costs to get the data to the processing unit. But unfortunately we can only store a limited amount of data really close to a processing unit. The further the memory is from the processing element, the higher the latency to get it there to process which was the main reason why cache memory was introduced (as it was introduced long before the power associated with data transport became a problem). Hence getting the data flowing smoothly through the computer, all the way from permanent storage to processing, is key to performance. An important way to deal with latency is the use of caches. Some are fully managed in hardware, like the caches between CPU and RAM memory, while others are managed in software, e.g., file systems also have a caching mechanism. It is important to ensure that those caches can work efficiently, and that requires: predictable data accesses, so that prefetching mechanisms can fetch data into the cache before it is needed so that part of the latency can be hidden, and data accesses in sufficiently large chunks to avoid that data in caches is never used and hence wasted and so that the effective bandwidth is not too much reduced by latency. If you'd fetch individual 4-byte numbers from RAM memory and if the memory latency would be 100ns and if there would be no way to have those data accesses overlap as you cannot predict what the next piece of data would be, then you effectively get at most a bandwidth of 4 bytes / 100ns / 1024^3 = 0,037 GByte/s which is only a fraction of the bandwidth one can get from RAM memory. Random access to small blocks of data is bad at all levels in computers, not only supercomputers. Only the definition of \"small\" varies depending on the type of memory or storage. For RAM memory \"small\" is on the order of a cache line or 64 bytes on many popular CPUs, for permanent storage \"small\" is measured in kilobytes or even 10s or 100s of kilobytes for shared parallel file systems. We haven't discussed the architecture of main memory in much detail, ub there also streaming is important to get the fastest performance as internally memory is also accessed and buffered in larger blocks than a single request, and a subsequent request to data that is already in that larger buffer will be quicker than an access to data that is not. This is also not a new message. Some level of streaming has been important in supercomputers ever since the 70s, and when it comes to permanent storage it has been important on PCs ever since the first PCs were build. One may have the impression that it has become less important with modern flash memory based SSDs, but that is only partly true. The latency is extremely small compared to any storage device that uses mechanically moving parts, but even then if you only access data through small files with size in the order of kilobytes so that the caching mechanisms in file systems cannot work, you will only reach a fraction of the theoretical bandwidth of those devices, and this again becomes more and more pronounced with every new generation as the peak bandwidth of SSDs improves quickly while the latency stays about the same.","title":"Streaming"},{"location":"5_Summary1/05_04_Keywords/#parallelism","text":"Parallelism is important at all levels in supercomputers. We have thoroughly discussed the 4 levels of parallelism in a the processing units in a supercomputer: Instruction Level Parallelism and vectorisation (and in fact, recently matrix computations) in the core, the use of multiple cores in shared memory setup, and distributed memory parallel computing. We've also seen that parallelism is important in the design of storage systems for supercomputers, with even specialised shared storage systems. However, as we have mentioned, in fact all but the most basic SSDs in PCs and smartphones also rely on parallelism to reach the high bandwidth and high number of IO operations per second they promise to be capable of. We haven't discussed main memory in that much detail, but in fact modern computer memory also relies on parallelism. modern processor packages offer multiple memory controllers that work in parallel, and usually each memory controller already controls more than one memory channel. And even within a modern memory module there is already a level of parallelism. Most parallelism is not automatic. Instruction level parallelism does not require much work from a user though there are some programming techniques that improve and others that harm instruction level parallelism. Exploiting vector computation already requires more work from the programmer as we shall see in the following chapter, and matrix units are even harder to exploit. Shared and distributed memory parallelism almost always requires a significant effort from the programmer. Both can of course be exploited by simply running multiple binaries in a capacity computing context but that does not improve the time-to-solution for a single problem. Similarly, good performance of central storage is all but a given and requires careful thinking about how data is stored. Some storage systems may be more forgiving than other storage systems, but even on an SSD in a PC a bad data access pattern will leave a lot of potential of the storage system unused. This is also not a new lesson. Supercomputers have relied on forms of parallelism that require help from the programmer since the '70s. In PCs some form of vector computing returned in the '90s and became essential for floating point performance with the Pentium 4 processor in 2002. The first regular PCs with more than one core appeared already in 2006, and nowadays 4 cores or more are common in even laptops and the cheapest smartphones. In PCs single thread performance still improves over every generation but at the expense of steeply rising power consumption. In servers and supercomputers the single thread performance has been stagnating for several years already as they are more optimised for performance per Watt. Even though every new generation of core claims to do more work per clock cycle (better instruction level parallelism), the clock speed is often also lowered so that the net gain is practically zero. Just as streaming is almost as important on PCs as on supercomputers, the same also holds for parallelism. There is only one level of parallelism that is specific to supercomputers: distributed memory parallelism. And at the file system levels PCs also tend to be a lot more forgiving for bad access patterns, even though you will still be exploiting only 10 percent or less of the potential of your storage.","title":"Parallelism"},{"location":"5_Summary1/05_04_Keywords/#hierarchy","text":"Hierarchies appear in different ways in supercomputers, but also more and more in regular PCs. Memory is organised in a hierarchical way with typically 3 levels of cache where the first two levels tend to be organised per core while the third level is a cache that is shared by multiple if not all cores, and then often two or more levels of RAM memory. Many supercomputers are build out of building blocks with two processor sockets and accessing memory that is physically attached to the other socket is slower than accessing local main memory, but even within a socket not all memory might be equal. The latter is already the case on the AMD Epyc server processors and will also happen on some versions of the Intel Sapphire Rapids series that became available in early 2023. PCs also have the same cache hierarchy, but the main memory has only one level unless you opt for some workstation-class systems that are really built using variants of processors for servers. There is also a hierarchy in the levels of parallelism for processing. Instruction level parallelism and vectorisation is parallelism at a very fine scale as it is done in the instruction stream itself and does not require any special communication or synchronisation. Shared memory parallelism is the next level in the hierarchy. It comes with a startup cost and cost to synchronise the threads for some operations. It requires bigger chunks of work to be executed in parallel before the gain from parallelism offsets the startup and synchronisation costs. Distributed memory parallelism tends to be even coarser as the startup cost is higher and the communication between the processes is a lot more complicated then the communication between threads in a shared memory program. Discussing the hardware architecture and software parallelism models of GPUs in much detail is outside the scope of this two-lecture introduction. However, bot the hardware and the lower-level programming models for GPUs are very hierarchical. We can expect that parallel storage will also become more hierarchical than it is today as supercomputer manufacturers are looking for ways to bring some storage again closer to the processing elements without losing too much of the manageability, and as flash storage will remain too expensive in the forseeable future to build an all flash file system and hence can only be used for an extra level in the hierarchy. Exploiting the memory hierarchy is extremely important for performance as will also be illustrated later in this tutorial. Mapping threads and processes in shared and distributed memory parallel computing on that hierarchy is for many codes also important for performance. One may of course wish that it would be different, but in practice some understanding of the hardware architecture is needed even if you are only using parallel computers and not programming them.","title":"Hierarchy"},{"location":"5_Summary1/05_05_Andy_Bill/","text":"Andy and Bill's law \u00b6 What Andy giveth, Bill taketh away The law originates from a humorous one-liner told in the 1990s during computing conferences. Andy in this law is Andy Grove, CEO of Intel and later Chairman of the board between 1987 and 2004. In those days Dennard scaling and the rapid evolution of semiconductor technology made a rapid growth of processing power for PCs possible and the single chip designs from Intel were quickly catching up with the much more expensive often multi-chip designs for workstation and server processors. Bill in this law is Bill Gates, founder of Micorosft and its CEO and Chairman between 1975 and 2000. Especially in the '90s, when GUIs became popular, PC software rapidly expanded and always managed to use all available processing power, sometimes not feeling any faster than the previous version of a package on a previous generation of hardware. With some packages it felt as if you didn't really get that much more work done quickly even though your hardware became faster and faster. However, beyond those law is also a frustration of Andy Grove who felt that Bill Gates wasn't always making proper use of new performance-enhancing features of the new processors and hence not using the new hardware to the full potential. Intel introduced the 80286 in 1982 and it was first used by IBM in the IBM AT in 1984, but it took until 1987 before an OS appeared that was aimed at regular PC users and fully exploited the 80286, with OS/2, which was in fact mostly built by IBM. Though Microsoft should not be entirely to blame for this as it was not possible to use the extended mode (actually called protected mode) while keeping compatibility with older DOS software also, except via an undocumented trick (which is what OS/2 used). The 80386 was the first processor of Intel that offered a 32-bit instruction set, and it was also designed to run old 16-bit DOS programs well together with 32-bit software. Though there were some Unix variants that supported the CPU in 32-bit mode and a 32-bit version of OS/2 in early 1992, it took Microsoft until Windows NT 3.1 in July 1993 to come with an OS for typical PC use that fully exploited the 32-bit features of the 80386 (by then succeeded by the 80486 and Pentium). It used to be common practice in much of the scientific computing community to ridicule Microsoft and to claim that UNIX and UNIX-derived operating systems are superior. A large part of the scientific computing community isn't doing any better though and we can think of several laws equivalent to Andy and Bill's law that apply to scientific computing. What Andy giveth, Cleve taketh away where Cleve is Cleve Moler who really started the development of efficient linear algebra libraries such as LINPACK and EISPACK, predecessors to LAPACK, but then also developed MATLAB as a user-friendly environment to experiment with those libraries and in 1984 was one of the founders of MathWorks, the company that went on to develop Matlab into what it is today. Matlab evolved into an excellent system to prototype numerical algorithms. However, its language is not nearly as efficient as traditional programming languages when it comes to execution efficiency and hence is a good way to slow down modern hardware. What Andy giveth, James taketh away where James is James Gosling, the main developer of the Java programming language. Java, and many other programming languages from that area, may have good ideas to improve programmer productivity or make it easier to run code on multiple machines, but this also came at a cost of performance. The first versions of the Java virtual machine were just slow, and even later versions based on a just-in-time compiler are not that spectacular. Designers of just-in-time compilers have long promised us better performance than regular compilers as they can use runtime information to further improve the generated code, but the reality is that gathering that information and using it properly to improve the generated code is too expensive and cumbersome. Abstracting away too much of the memory system is also not a good idea as making proper use of the memory hierarchy is essential for performance and one cannot expect compilers to do be able to do the necessary code transformations on their own. Integrating with code written in other programming languages that make it easier to write high-performance library routines is also very cumbersome. And getting garbage collection to work well in a distributed memory context also requires build-in support in the virtual machines for this type of parallelism and cannot be done via a simple library add-on (there has been an effort do do MPI for Java but that didn't work well because of this). Granted not everything about Java is bad though. The language did support concurrency in the base language and was hence ready for shared memory execution. And in theory the just-in-time compiler concept also allows to quickly adapt to new processor architectures, if it were not that the language lacked the features to enable to compiler to easily vectorise code, one of the features that influences performance of modern processors most on scientific code. At some point Java gained some popularity in scientific computing basically because it became the first programming language taught at many universities and hence the one that beginning researchers were most familiar with, but it is mostly given here as an example of languages that try to abstract away to much of the underlying system architecture and hence tend to run with less than optimal efficiency. What Andy giveth, Guido taketh away where Guido is Guido van Rossum, the original developer of the Python scripting language (back in 1989 already). Python for a long time was a scripting language only known by system administrators and the like, but from 2005 on, with the advent of NumPy, became more and more popular in scientific computing. The Python ecosystem exploded with lots of small and often badly tested packages, and the language designers got in the habit to break code with every minor release every 18 months. Moreover, the language is usually interpreted and the interpreter is extremely inefficient on much code. In fact, the Python designers aren't really to blame for this as the language was developed with a completely different purpose in mind and did a good job at that for a very long time. There have been several efforts to develop just-in-time compilers (and an ahead-of-time compiler to C) for Python, but as of today there is still no JIT that does well on most Python code, and several companies that invested in the development of one have given up, though all compilers will probably come with examples where they offer a 100x or 1000x speed increase over naively written pure Python code for small specific fragments. Cython is an example of an ahead-of-time compiler that needs some help as regular Python code doesn't really offer enough type information to generate efficient code. Numba and PyPy are two examples of just-in-time compilers wheve Numba seems to do best with code that heavily uses NumPy data structures while PyPy works better on non-NumPy code. Never mind that we also tend install Python and its packages from repositories that often only contain generic binaries compiled to run on as large a range of hardware as possible rather than binaries that exploit specific features of each processor to optimise performance. Of course it is easy to write bad performing code in any programming language, but the point is that there are languages where it is near impossible or even just impossible to write truly efficient code. One used to get away with that in the days that the performance-for-money ratio improved a lot with every new generation of hardware. But as we have discussed, these days are over for now and it may be a long time before they return. For now, progress will have to come from better, more efficient software that better exploits the features of current hardware. There may be a lot of discussion today about how quantum computers or computers with optical components will solve all problems. This is just discussion by the hopeful. The reality today is that the quantum computer is still looking for an application in which it will excel, and currently needs to be attached to a rather powerful traditional computer also simply to turn the information that it produces in something useful. The cooling needed for a quantum computer is also extremely expensive as it needs to be cooled to nearly the absolute zero. EVery fraction of a Kelvin above the absolute zero makes the results worse because of noise (which manifests itself as errors in the computation). Don't be fooled by the pictures you see of quantum computers: Most of these pictures don't even show the quantum computer nor all the hardware that is needed to measure the quantum state. The most popular pictures tend to show the nice-looking multistage cooling system. The same holds for optical computers. The reality there is that we are still very far from integrated optical circuits, let alone ones that are powerful enough to do computations quicker than our current computers. The amount of money needed to develop that technology into something useful may turn out to be prohibitive unless we really hit barriers very hard. In fact, we have seen this happening before. As we have discussed, flash memory has serious problems. Longevity is a big problem. And furthermore it is still a block-oriented medium limiting the ways in which it can be used. There were two promising technologies to replace it over time: the memristor and phase change memory. A form of the latter was brought to market as 3D XPoint by Micron and Intel in 2017. However, it was expensive compared to flash memory, partly also because there was a huge development cost that needed to be paid back with the relatively low initial volumes, and it would still have required a lot of funding to become truly price-volume competitive with flash memory. The technology was abandoned in 2022 because of that.","title":"Andy and Bill's law"},{"location":"5_Summary1/05_05_Andy_Bill/#andy-and-bills-law","text":"What Andy giveth, Bill taketh away The law originates from a humorous one-liner told in the 1990s during computing conferences. Andy in this law is Andy Grove, CEO of Intel and later Chairman of the board between 1987 and 2004. In those days Dennard scaling and the rapid evolution of semiconductor technology made a rapid growth of processing power for PCs possible and the single chip designs from Intel were quickly catching up with the much more expensive often multi-chip designs for workstation and server processors. Bill in this law is Bill Gates, founder of Micorosft and its CEO and Chairman between 1975 and 2000. Especially in the '90s, when GUIs became popular, PC software rapidly expanded and always managed to use all available processing power, sometimes not feeling any faster than the previous version of a package on a previous generation of hardware. With some packages it felt as if you didn't really get that much more work done quickly even though your hardware became faster and faster. However, beyond those law is also a frustration of Andy Grove who felt that Bill Gates wasn't always making proper use of new performance-enhancing features of the new processors and hence not using the new hardware to the full potential. Intel introduced the 80286 in 1982 and it was first used by IBM in the IBM AT in 1984, but it took until 1987 before an OS appeared that was aimed at regular PC users and fully exploited the 80286, with OS/2, which was in fact mostly built by IBM. Though Microsoft should not be entirely to blame for this as it was not possible to use the extended mode (actually called protected mode) while keeping compatibility with older DOS software also, except via an undocumented trick (which is what OS/2 used). The 80386 was the first processor of Intel that offered a 32-bit instruction set, and it was also designed to run old 16-bit DOS programs well together with 32-bit software. Though there were some Unix variants that supported the CPU in 32-bit mode and a 32-bit version of OS/2 in early 1992, it took Microsoft until Windows NT 3.1 in July 1993 to come with an OS for typical PC use that fully exploited the 32-bit features of the 80386 (by then succeeded by the 80486 and Pentium). It used to be common practice in much of the scientific computing community to ridicule Microsoft and to claim that UNIX and UNIX-derived operating systems are superior. A large part of the scientific computing community isn't doing any better though and we can think of several laws equivalent to Andy and Bill's law that apply to scientific computing. What Andy giveth, Cleve taketh away where Cleve is Cleve Moler who really started the development of efficient linear algebra libraries such as LINPACK and EISPACK, predecessors to LAPACK, but then also developed MATLAB as a user-friendly environment to experiment with those libraries and in 1984 was one of the founders of MathWorks, the company that went on to develop Matlab into what it is today. Matlab evolved into an excellent system to prototype numerical algorithms. However, its language is not nearly as efficient as traditional programming languages when it comes to execution efficiency and hence is a good way to slow down modern hardware. What Andy giveth, James taketh away where James is James Gosling, the main developer of the Java programming language. Java, and many other programming languages from that area, may have good ideas to improve programmer productivity or make it easier to run code on multiple machines, but this also came at a cost of performance. The first versions of the Java virtual machine were just slow, and even later versions based on a just-in-time compiler are not that spectacular. Designers of just-in-time compilers have long promised us better performance than regular compilers as they can use runtime information to further improve the generated code, but the reality is that gathering that information and using it properly to improve the generated code is too expensive and cumbersome. Abstracting away too much of the memory system is also not a good idea as making proper use of the memory hierarchy is essential for performance and one cannot expect compilers to do be able to do the necessary code transformations on their own. Integrating with code written in other programming languages that make it easier to write high-performance library routines is also very cumbersome. And getting garbage collection to work well in a distributed memory context also requires build-in support in the virtual machines for this type of parallelism and cannot be done via a simple library add-on (there has been an effort do do MPI for Java but that didn't work well because of this). Granted not everything about Java is bad though. The language did support concurrency in the base language and was hence ready for shared memory execution. And in theory the just-in-time compiler concept also allows to quickly adapt to new processor architectures, if it were not that the language lacked the features to enable to compiler to easily vectorise code, one of the features that influences performance of modern processors most on scientific code. At some point Java gained some popularity in scientific computing basically because it became the first programming language taught at many universities and hence the one that beginning researchers were most familiar with, but it is mostly given here as an example of languages that try to abstract away to much of the underlying system architecture and hence tend to run with less than optimal efficiency. What Andy giveth, Guido taketh away where Guido is Guido van Rossum, the original developer of the Python scripting language (back in 1989 already). Python for a long time was a scripting language only known by system administrators and the like, but from 2005 on, with the advent of NumPy, became more and more popular in scientific computing. The Python ecosystem exploded with lots of small and often badly tested packages, and the language designers got in the habit to break code with every minor release every 18 months. Moreover, the language is usually interpreted and the interpreter is extremely inefficient on much code. In fact, the Python designers aren't really to blame for this as the language was developed with a completely different purpose in mind and did a good job at that for a very long time. There have been several efforts to develop just-in-time compilers (and an ahead-of-time compiler to C) for Python, but as of today there is still no JIT that does well on most Python code, and several companies that invested in the development of one have given up, though all compilers will probably come with examples where they offer a 100x or 1000x speed increase over naively written pure Python code for small specific fragments. Cython is an example of an ahead-of-time compiler that needs some help as regular Python code doesn't really offer enough type information to generate efficient code. Numba and PyPy are two examples of just-in-time compilers wheve Numba seems to do best with code that heavily uses NumPy data structures while PyPy works better on non-NumPy code. Never mind that we also tend install Python and its packages from repositories that often only contain generic binaries compiled to run on as large a range of hardware as possible rather than binaries that exploit specific features of each processor to optimise performance. Of course it is easy to write bad performing code in any programming language, but the point is that there are languages where it is near impossible or even just impossible to write truly efficient code. One used to get away with that in the days that the performance-for-money ratio improved a lot with every new generation of hardware. But as we have discussed, these days are over for now and it may be a long time before they return. For now, progress will have to come from better, more efficient software that better exploits the features of current hardware. There may be a lot of discussion today about how quantum computers or computers with optical components will solve all problems. This is just discussion by the hopeful. The reality today is that the quantum computer is still looking for an application in which it will excel, and currently needs to be attached to a rather powerful traditional computer also simply to turn the information that it produces in something useful. The cooling needed for a quantum computer is also extremely expensive as it needs to be cooled to nearly the absolute zero. EVery fraction of a Kelvin above the absolute zero makes the results worse because of noise (which manifests itself as errors in the computation). Don't be fooled by the pictures you see of quantum computers: Most of these pictures don't even show the quantum computer nor all the hardware that is needed to measure the quantum state. The most popular pictures tend to show the nice-looking multistage cooling system. The same holds for optical computers. The reality there is that we are still very far from integrated optical circuits, let alone ones that are powerful enough to do computations quicker than our current computers. The amount of money needed to develop that technology into something useful may turn out to be prohibitive unless we really hit barriers very hard. In fact, we have seen this happening before. As we have discussed, flash memory has serious problems. Longevity is a big problem. And furthermore it is still a block-oriented medium limiting the ways in which it can be used. There were two promising technologies to replace it over time: the memristor and phase change memory. A form of the latter was brought to market as 3D XPoint by Micron and Intel in 2017. However, it was expensive compared to flash memory, partly also because there was a huge development cost that needed to be paid back with the relatively low initial volumes, and it would still have required a lot of funding to become truly price-volume competitive with flash memory. The technology was abandoned in 2022 because of that.","title":"Andy and Bill's law"},{"location":"5_Summary1/05_06_Software_not_hardware/","text":"Software first, not hardware \u00b6 It is the software more than the hardware that defines a supercomputer. In the early 60s computers were still mostly build from individual transistors. There were already smaller slower and bigger faster computers, but architectures differed vastly. Powers of two for number of bits, or binary number representations were not yet standard. The first programming languages, Fortran for technical computing and Cobol for business computingv, already existed. In the late 60s and early 70s computer were build from 100s or 1000s of small integrated circuits (thoguh some already appeared earlier). In this era we saw smaller minicomputers and large mainframes. The minicomputers were more scaled-down versions of the mainframes. By the second half of the 70s a very specific supercomputer architecture that was not very well suited for general computing appeared on the market: the vector computers build by Cray and CDC. Specialised hardware meant that software also had to be adapted: The compilers did need some assistance to generate proper vector code, and the first numeric libraries that helped exploit the vector architecture also appeared. By the second half of the 80s the next big paradigm shift showed up. It had become prohibitively expensive to design supercomputer hardware. Vector machines were still fairly popular, but keeping developing them had become very expensive, while a market for smaller workstations that offered decent performance for the time appeared. Hence labs and Intel started to experiment with building supercomputers that would reuse some of that workstation hardware. The first computer of that kind was probably the Cosmic Cube developed at Caltech which was based not even on a workstation processor but the two-chip 8086/8087 combo that was also used in personal computers. 64 of those were linked together using a special-purpose network. This research later led to the Intel iPSC product line of supercomputer build out of regular 80386 and later the Intel i860 RISC processors. Other manufacturers also picked up the idea, e.g., IBM with the SP line using processors developed for their POWER workstations in 1993 (IBM also build a vector computer in the late 80s). By 1995 distributed memory computers based on processors designed for workstations or PSs already took a lot of the top spots in the Top 500 list, a list of fastest supercomputers on the Linpack benchmark. Supercomputer manufacturers differentiated more in their designs for the interconnect and in the software than in the processor hardware as the latter was shared with less capable workstations or even came from another manufacturer. This trend only became even more pronounced from the late 90s on. These are the days when Linux started to become more popular among researchers and when Intel processors for PCs had fully caught up with typical workstation processors in terms of performance, gradually pushing the latter out of the market as the growing design costs had to be amortised on a too low volumes. Modern supercomputers try to minimise the hardware cost by reusing technologies that have other larger volume applications also and in some cases even reusing more volume hardware. Even custom network technologies, for a long time the feature that vendors used to distinguish their supercomputers from more regular computers, have largely disappeared from the market in favour of InfiniBand, a network technology originally designed for other applications in the data centre. Cray and Fujitsu are notable exceptions, both still investing in their home-grown interconnect technologies and still doing so today, though Cray has been bought by HPE. One can argue though that the most recent Cray network technology, called SlingShot, is largely derived from Ethernet with some customisations that are in fact also partly software. More than ever before does the system and application software make the supercomputer: programming models implemented in libraries and compilers to deal with distributed computing, parallel file systems to turn huge arrays of disks and servers into a high bandwidth file system capable of efficiently serving multi-terabyte data files to applications, applications that exploit all levels of parallelism and the hierarchical structure of modern supercomputers, ... In fact, without all that software a high-end gaming PC could very well be faster for your application than a supercomputer! This evolution is only normal. Designing hardware is expensive and hardware also needs a lot of testing before it can be brought to market as many errors are hard to correct afterwards. Moreover there is also a high production cost associated with hardware, and the cost is higher for hardware that can only be used in small volumes. Software may not be cheap to design either, but it is easier to bring to market as it is easy to correct errors afterwards, and the cost of distributing it is low compared to the costs for hardware. As it is easy to continue improving software it is possible to upgrade a system and make it better during its lifetime. It is also largely the software that makes a supercomputer still a different infrastructure from a server farm or a cloud infrastructure (though there is one element, the interconnect, that still remains very important and tend to differ from those infrastructures also). Supercomputers focus on latency and staying close to \"bare metal\" to be able to get the maximum performance out of the hardware and to enable thousands of processors to work together on solving big problems that cannot be split up in hundreds of almost independent tasks that can be executed in parallel. Supercomputers focus on scalability for capability applications and everything that compromises that scalability is not implemented. Cloud infrastructures on the other hand focus more on isolating even small users from one another, security and the creation of a personal environment. They are typically only suited for fairly coarse grained parallelism and applications where different tasks that can be executed in parallel are fairly independent of each other (think of the thousands of requests coming in on web servers). Supercomputers and cloud infrastructures have also very different exploitation models. That partly results from the different type of environment they intend to offer, but is also partly the result of the fact that many supercomputers are installed in supercomputer centres that serve mostly academia or in academic institutions themselves, while the largest cloud infrastructures are commercial. Users think completely differently about resource use if they get an allocation upfront and don't see the bill then when they actually have to pay full cost for resources at a commercial cloud infrastructure. Supercomputers tend to focus on a rapid succession of jobs of different users, relying mostly on shared storage as managing bare-metal node based storage in a multi-user environment can be hard. Modern Linux may offer some low-overhead solutions to ease some of those problems, for some reason they have not become popular yet on supercomputers. The focus on rapid succession of jobs is only normal on a cluster where compute time is not billed in terms of money as there is not enough incentive for a user to carefully consider if it makes sense to keep the same resources unused for a short while to save on some other startup cost instead. This is different on a cloud infrastructure where there is a financial incentive to think economically and where it is not uncommon to keep some servers a bit longer to, e.g., save on the data transport cost to bring data from a remote data store service to a local (virtualised) disk. The full virtualisation that is often used also makes it a lot easier for the system to clean up afterwards, making it easier to offer local storage in a manageable way (and cloud servers tend to be larger also which also eases offering local storage, while supercomputers are built as compact as possible for better internode communication performance and to be able to use cheaper cabling).","title":"Software, not hardware"},{"location":"5_Summary1/05_06_Software_not_hardware/#software-first-not-hardware","text":"It is the software more than the hardware that defines a supercomputer. In the early 60s computers were still mostly build from individual transistors. There were already smaller slower and bigger faster computers, but architectures differed vastly. Powers of two for number of bits, or binary number representations were not yet standard. The first programming languages, Fortran for technical computing and Cobol for business computingv, already existed. In the late 60s and early 70s computer were build from 100s or 1000s of small integrated circuits (thoguh some already appeared earlier). In this era we saw smaller minicomputers and large mainframes. The minicomputers were more scaled-down versions of the mainframes. By the second half of the 70s a very specific supercomputer architecture that was not very well suited for general computing appeared on the market: the vector computers build by Cray and CDC. Specialised hardware meant that software also had to be adapted: The compilers did need some assistance to generate proper vector code, and the first numeric libraries that helped exploit the vector architecture also appeared. By the second half of the 80s the next big paradigm shift showed up. It had become prohibitively expensive to design supercomputer hardware. Vector machines were still fairly popular, but keeping developing them had become very expensive, while a market for smaller workstations that offered decent performance for the time appeared. Hence labs and Intel started to experiment with building supercomputers that would reuse some of that workstation hardware. The first computer of that kind was probably the Cosmic Cube developed at Caltech which was based not even on a workstation processor but the two-chip 8086/8087 combo that was also used in personal computers. 64 of those were linked together using a special-purpose network. This research later led to the Intel iPSC product line of supercomputer build out of regular 80386 and later the Intel i860 RISC processors. Other manufacturers also picked up the idea, e.g., IBM with the SP line using processors developed for their POWER workstations in 1993 (IBM also build a vector computer in the late 80s). By 1995 distributed memory computers based on processors designed for workstations or PSs already took a lot of the top spots in the Top 500 list, a list of fastest supercomputers on the Linpack benchmark. Supercomputer manufacturers differentiated more in their designs for the interconnect and in the software than in the processor hardware as the latter was shared with less capable workstations or even came from another manufacturer. This trend only became even more pronounced from the late 90s on. These are the days when Linux started to become more popular among researchers and when Intel processors for PCs had fully caught up with typical workstation processors in terms of performance, gradually pushing the latter out of the market as the growing design costs had to be amortised on a too low volumes. Modern supercomputers try to minimise the hardware cost by reusing technologies that have other larger volume applications also and in some cases even reusing more volume hardware. Even custom network technologies, for a long time the feature that vendors used to distinguish their supercomputers from more regular computers, have largely disappeared from the market in favour of InfiniBand, a network technology originally designed for other applications in the data centre. Cray and Fujitsu are notable exceptions, both still investing in their home-grown interconnect technologies and still doing so today, though Cray has been bought by HPE. One can argue though that the most recent Cray network technology, called SlingShot, is largely derived from Ethernet with some customisations that are in fact also partly software. More than ever before does the system and application software make the supercomputer: programming models implemented in libraries and compilers to deal with distributed computing, parallel file systems to turn huge arrays of disks and servers into a high bandwidth file system capable of efficiently serving multi-terabyte data files to applications, applications that exploit all levels of parallelism and the hierarchical structure of modern supercomputers, ... In fact, without all that software a high-end gaming PC could very well be faster for your application than a supercomputer! This evolution is only normal. Designing hardware is expensive and hardware also needs a lot of testing before it can be brought to market as many errors are hard to correct afterwards. Moreover there is also a high production cost associated with hardware, and the cost is higher for hardware that can only be used in small volumes. Software may not be cheap to design either, but it is easier to bring to market as it is easy to correct errors afterwards, and the cost of distributing it is low compared to the costs for hardware. As it is easy to continue improving software it is possible to upgrade a system and make it better during its lifetime. It is also largely the software that makes a supercomputer still a different infrastructure from a server farm or a cloud infrastructure (though there is one element, the interconnect, that still remains very important and tend to differ from those infrastructures also). Supercomputers focus on latency and staying close to \"bare metal\" to be able to get the maximum performance out of the hardware and to enable thousands of processors to work together on solving big problems that cannot be split up in hundreds of almost independent tasks that can be executed in parallel. Supercomputers focus on scalability for capability applications and everything that compromises that scalability is not implemented. Cloud infrastructures on the other hand focus more on isolating even small users from one another, security and the creation of a personal environment. They are typically only suited for fairly coarse grained parallelism and applications where different tasks that can be executed in parallel are fairly independent of each other (think of the thousands of requests coming in on web servers). Supercomputers and cloud infrastructures have also very different exploitation models. That partly results from the different type of environment they intend to offer, but is also partly the result of the fact that many supercomputers are installed in supercomputer centres that serve mostly academia or in academic institutions themselves, while the largest cloud infrastructures are commercial. Users think completely differently about resource use if they get an allocation upfront and don't see the bill then when they actually have to pay full cost for resources at a commercial cloud infrastructure. Supercomputers tend to focus on a rapid succession of jobs of different users, relying mostly on shared storage as managing bare-metal node based storage in a multi-user environment can be hard. Modern Linux may offer some low-overhead solutions to ease some of those problems, for some reason they have not become popular yet on supercomputers. The focus on rapid succession of jobs is only normal on a cluster where compute time is not billed in terms of money as there is not enough incentive for a user to carefully consider if it makes sense to keep the same resources unused for a short while to save on some other startup cost instead. This is different on a cloud infrastructure where there is a financial incentive to think economically and where it is not uncommon to keep some servers a bit longer to, e.g., save on the data transport cost to bring data from a remote data store service to a local (virtualised) disk. The full virtualisation that is often used also makes it a lot easier for the system to clean up afterwards, making it easier to offer local storage in a manageable way (and cloud servers tend to be larger also which also eases offering local storage, while supercomputers are built as compact as possible for better internode communication performance and to be able to use cheaper cabling).","title":"Software first, not hardware"},{"location":"6_Middleware/","text":"Middleware: Turning the hardware into a usable supercomputer \u00b6","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"6_Middleware/#middleware-turning-the-hardware-into-a-usable-supercomputer","text":"","title":"Middleware: Turning the hardware into a usable supercomputer"},{"location":"7_Expectations/","text":"What can we expect? \u00b6","title":"What can we expect?"},{"location":"7_Expectations/#what-can-we-expect","text":"","title":"What can we expect?"},{"location":"8_Accelerators/","text":"Accelerators \u00b6 What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading","title":"Accelerators"},{"location":"8_Accelerators/#accelerators","text":"What are accelerators? Offloading CPUs with accelerator features Accelerator programming Status of GPU computing Further reading","title":"Accelerators"},{"location":"8_Accelerators/8_01_What_are_accelerators/","text":"What are accelerators? \u00b6 In short \u00b6 We restrict ourselves to the most common types of compute accelerators used in supercomputing and do not cover, e.g., accelerators in the network to process certain MPI calls. An accelerator is usually a coprocessor that accelerates some computations that a CPU might be capable of doing, but that can be done much faster on more specialised hardware, so that it becomes interesting to offload that work to specialised hardware. An accelerator is not a full-featured general purpose processor that can run a regular operating system, etc. Hence an accelerator always has to work together with the regular CPU of the system, which can lead to very complicated programming with a host program that then offloads certain routines to the accelerator, and also needs to manage the transport of data to and from the accelerator. Some history \u00b6 Accelerators have been around for a long time in large computers, and in particular in mainframes, very specialised machines mostly used for administrative work. However, accelerators also appeared in the PC world starting in the '90s. The first true programmable accelerators probably appeared in the form of high-end sound cards. They contained one or more so-called Digital Signal Processor (DSP) chips for all the digital processing of the sound. Graphic cards were originally very specialised fixed-function hardware that was not really programmable but this changed in the early '00s with graphics cards as the NVIDIA GeForce 3 and ATI Radeon 9300 (ATI was later acquired by AMD which still uses the Radeon brand). It didn't take long before scientist looking for more and cheaper compute power took note and started experimenting with using that programmability to accelerate certain scientific computations. Manufacturers, and certainly NVIDIA, took note and started adding features specifically for broader use. This led to the birth of NVIDIA CUDA 1.0 in 2007, the first successful platform and programming model for programming graphics cards that were now called Graphics Processing Units (or GPU) as they became real programmable processors. And the term GPGPU for General-Purpose GPU is also used for hardware that is particularly suited to be used for non-graphics work also. GPGPU programming quickly became popular, and even a bit overhyped, as not all applications are suitable for GPGPU computing. Types of accelerators \u00b6 The most popular type of accelerators are accelerators for vector computing . All modern GPUs fall in this family. Examples are NVIDIA Data Center series (previously also called the Tesla series). These started of as basically a more reliable version of the NVIDIA GeForce and Quadro GPUs, but currently, with the Ada Lovelace GPUs and Hopper GPGPUs these lines start to diverge a bit (and even before, the cards really meant for supercomputing had more hardware on them for double precision floating point computations). Strictly speaking the NVIDIA architecture is a single instruction multiple data (SIMD) architecture but not one that uses explicit vector instructions, but the resulting capabilities are not really different from more regular vector computers (but then vector computers with an instruction set that also support scatter/gather instructions and predication, features that are missing from, e.g., the AVX/AVX2 instruction set). AMD has the Instinct series for GPGPU computing. They employ a separate architecture for their compute cards, called CDNA, while their current graphics cards use various generations of the RDNA architecture. The CDNA architecture is a further evolution of their previous graphics architecture GCN though (used in, e.g., the Vega cards). AMD Instinct GPUs are used in the first USA exaflop computer Frontier (fastest system in the Top500 ranking of June 2022) and in the European LUMI system (fastest European system in the Top500 ranking of June 2022). These computers use the CDNA2 architecture. A future USA exascale system, El Capitan, planned for 2023 (or possibly 2024 with the supply chain disruptions largely due to Covid), will employ a future version of the architecture, RDNA3, which will bring CPU and GPU very close together. Intel is also moving into the market of GPGPU computing with their Xe graphics products. They have supported using their integrated GPUs for computations for many years already, with even support in their C/C++/Fortran compilers, but are now making a separate products for the supercomputing market with the Xe HPC product lines which support additional data formats that are very needed for scientific computing applications. The first product in this line is the GPU code named Ponte Vecchio that will be used in the USA Aurora supercomputer, which should become the second USA exaflop computer. A future European pre-exascale system will also have a compute section with the successor of that chip, Rialto Bridge. The NEC SX Aurora TSUBASA has a more traditional vector computing architecture, but is physically also an expansion card that is put in a regular Intel-compatible server. It is special in the sense that the original idea was that applications would fully run on the vector processor and hence not use a host programming with offloading, while under the hood the OS libraries would offload OS operations to the Intel-compatible server, but in practice it is more and more used as a regular accelerator with a host program running on the Intel-compatible server offloading work to the vector processor. A second type of accelerator that became very popular recently, are accelerators for matrix operations , and in particular matrix multiplication or rank-k update. They were originally designed to speed up operations in certain types of neural networks, but were quickly gained support for additional data types that makes them useful for a range of AI and other HPC applications. Some are integrated on GPGPUs while others are specialised accelerators. The ones integrated in GPUs are most popular for supercomputing though: NVIDIA Tensor cores in the V100 and later generations. AMD matrix cores in the MI100 and later chips. The MI200 generation may be a little behind the similar-generation A100 NVIDIA cards when it comes to low-precision formats used in some AI applications, but it shines in higher-precision data formats (single and double precision floating point). Intel includes their so-called Matrix Engines in the Ponte Vecchio GPGPUs. The NVIDIA tensor cores, AMD matrix cores and Intel matrix engines are all integrated very closely with the vector processing hardware on their GPGPUs, However, there are also dedicated matrix computing accelerators, in particular in accelerators specifically designed for AI, such as the Google TPU (Tensor Processing Unit). Most neural network accelerators on smartphone processors also fall in this category as they are usually in a physically distinct area from the GPU hardware in the SOC (though not a separate chip or die). A third and so far less popular accelerator in supercomputing is an FPGA accelerator, which stands for Field Programmable Gate Array. This is hardware designed to be fully configured after manufacturing, allowing the user to create a specialised processor for their application. One could, e.g., imagine creating specialised 2-bit CPUs for working with generic data.","title":"What are accelerators?"},{"location":"8_Accelerators/8_01_What_are_accelerators/#what-are-accelerators","text":"","title":"What are accelerators?"},{"location":"8_Accelerators/8_01_What_are_accelerators/#in-short","text":"We restrict ourselves to the most common types of compute accelerators used in supercomputing and do not cover, e.g., accelerators in the network to process certain MPI calls. An accelerator is usually a coprocessor that accelerates some computations that a CPU might be capable of doing, but that can be done much faster on more specialised hardware, so that it becomes interesting to offload that work to specialised hardware. An accelerator is not a full-featured general purpose processor that can run a regular operating system, etc. Hence an accelerator always has to work together with the regular CPU of the system, which can lead to very complicated programming with a host program that then offloads certain routines to the accelerator, and also needs to manage the transport of data to and from the accelerator.","title":"In short"},{"location":"8_Accelerators/8_01_What_are_accelerators/#some-history","text":"Accelerators have been around for a long time in large computers, and in particular in mainframes, very specialised machines mostly used for administrative work. However, accelerators also appeared in the PC world starting in the '90s. The first true programmable accelerators probably appeared in the form of high-end sound cards. They contained one or more so-called Digital Signal Processor (DSP) chips for all the digital processing of the sound. Graphic cards were originally very specialised fixed-function hardware that was not really programmable but this changed in the early '00s with graphics cards as the NVIDIA GeForce 3 and ATI Radeon 9300 (ATI was later acquired by AMD which still uses the Radeon brand). It didn't take long before scientist looking for more and cheaper compute power took note and started experimenting with using that programmability to accelerate certain scientific computations. Manufacturers, and certainly NVIDIA, took note and started adding features specifically for broader use. This led to the birth of NVIDIA CUDA 1.0 in 2007, the first successful platform and programming model for programming graphics cards that were now called Graphics Processing Units (or GPU) as they became real programmable processors. And the term GPGPU for General-Purpose GPU is also used for hardware that is particularly suited to be used for non-graphics work also. GPGPU programming quickly became popular, and even a bit overhyped, as not all applications are suitable for GPGPU computing.","title":"Some history"},{"location":"8_Accelerators/8_01_What_are_accelerators/#types-of-accelerators","text":"The most popular type of accelerators are accelerators for vector computing . All modern GPUs fall in this family. Examples are NVIDIA Data Center series (previously also called the Tesla series). These started of as basically a more reliable version of the NVIDIA GeForce and Quadro GPUs, but currently, with the Ada Lovelace GPUs and Hopper GPGPUs these lines start to diverge a bit (and even before, the cards really meant for supercomputing had more hardware on them for double precision floating point computations). Strictly speaking the NVIDIA architecture is a single instruction multiple data (SIMD) architecture but not one that uses explicit vector instructions, but the resulting capabilities are not really different from more regular vector computers (but then vector computers with an instruction set that also support scatter/gather instructions and predication, features that are missing from, e.g., the AVX/AVX2 instruction set). AMD has the Instinct series for GPGPU computing. They employ a separate architecture for their compute cards, called CDNA, while their current graphics cards use various generations of the RDNA architecture. The CDNA architecture is a further evolution of their previous graphics architecture GCN though (used in, e.g., the Vega cards). AMD Instinct GPUs are used in the first USA exaflop computer Frontier (fastest system in the Top500 ranking of June 2022) and in the European LUMI system (fastest European system in the Top500 ranking of June 2022). These computers use the CDNA2 architecture. A future USA exascale system, El Capitan, planned for 2023 (or possibly 2024 with the supply chain disruptions largely due to Covid), will employ a future version of the architecture, RDNA3, which will bring CPU and GPU very close together. Intel is also moving into the market of GPGPU computing with their Xe graphics products. They have supported using their integrated GPUs for computations for many years already, with even support in their C/C++/Fortran compilers, but are now making a separate products for the supercomputing market with the Xe HPC product lines which support additional data formats that are very needed for scientific computing applications. The first product in this line is the GPU code named Ponte Vecchio that will be used in the USA Aurora supercomputer, which should become the second USA exaflop computer. A future European pre-exascale system will also have a compute section with the successor of that chip, Rialto Bridge. The NEC SX Aurora TSUBASA has a more traditional vector computing architecture, but is physically also an expansion card that is put in a regular Intel-compatible server. It is special in the sense that the original idea was that applications would fully run on the vector processor and hence not use a host programming with offloading, while under the hood the OS libraries would offload OS operations to the Intel-compatible server, but in practice it is more and more used as a regular accelerator with a host program running on the Intel-compatible server offloading work to the vector processor. A second type of accelerator that became very popular recently, are accelerators for matrix operations , and in particular matrix multiplication or rank-k update. They were originally designed to speed up operations in certain types of neural networks, but were quickly gained support for additional data types that makes them useful for a range of AI and other HPC applications. Some are integrated on GPGPUs while others are specialised accelerators. The ones integrated in GPUs are most popular for supercomputing though: NVIDIA Tensor cores in the V100 and later generations. AMD matrix cores in the MI100 and later chips. The MI200 generation may be a little behind the similar-generation A100 NVIDIA cards when it comes to low-precision formats used in some AI applications, but it shines in higher-precision data formats (single and double precision floating point). Intel includes their so-called Matrix Engines in the Ponte Vecchio GPGPUs. The NVIDIA tensor cores, AMD matrix cores and Intel matrix engines are all integrated very closely with the vector processing hardware on their GPGPUs, However, there are also dedicated matrix computing accelerators, in particular in accelerators specifically designed for AI, such as the Google TPU (Tensor Processing Unit). Most neural network accelerators on smartphone processors also fall in this category as they are usually in a physically distinct area from the GPU hardware in the SOC (though not a separate chip or die). A third and so far less popular accelerator in supercomputing is an FPGA accelerator, which stands for Field Programmable Gate Array. This is hardware designed to be fully configured after manufacturing, allowing the user to create a specialised processor for their application. One could, e.g., imagine creating specialised 2-bit CPUs for working with generic data.","title":"Types of accelerators"},{"location":"8_Accelerators/8_02_Offloading/","text":"Offloading \u00b6 In early accelerator generations (and this is still the case in 2022) a CPU cannot directly operate on data in the memory of the GPU and vice-versa. Instead data needs to be copied between the memory spaces. Multiple accelerators in a system may or may not share a memory space. NVIDIA NVLink for instance is a technology that can be used to link graphics cards together and create a shared memory space, but even then it is crucial for performance that data is as much as possible in the memory directly attached to a GPU when being used. Many modern GPUs for scientific computing include support for unified memory where CPU and GPUs in the system can share a single logical address space, but under the hood the data still needs to be copied. This feature has been present in, e.g., the NVIDIA Pascal and later generations, where a page fault mechanism would be used to trigger a migration mechanism. Control-wise the program running on the CPU orchestrates the work but passes control to the accelerator to execute those code blocks that can be accelerated by the accelerator. The main problem with this model is that all the data copying that is needed, whether explicitly triggered by the application or implicitly through the unified memory model of some modern GPUs, can really nullify the gain from the accelerator. This is no different from what we saw for distributed computing. Just as for distributed computing, the fraction of the algorithm that cannot be parallelized limits the speed-up, as does the communication overhead, for accelerators the fraction of the application and the overhead to pass data to and from the accelerator will limit the speed-up that can be obtained from the accelerator. Hence it is clear that for future generations of accelerators, the main attention will be in making them more versatile to reduce the fraction that cannot be accelerated, and in integrating them closer with the CPU to reduce or eliminate the overhead in passing data between CPU and GPU. The first signs of this evolution were in some USA pre-exascale systems Summit and Sierra that used a IBM POWER9 CPUs and NVIDIA V100 GPUs. Those GPUs were connected to the CPU through NVLink, NVIDIA's interconnect for GPUs, rather than only PCIe links, so that the memory spaces of CPU and GPU become physically merged, with all data directly addressable from GPU and CPU and cache coherency. This technology of the Summit and Sierra supercomputers is carried over to the first three planned exascale systems of the USA. Frontier, and the European supercomputer LUMI, use the MI250X variant of the AMD CDNA2 architecture. The nodes in these systems consist of 4 MI250X GPUs, with each GPU consisting of two dies, and a custom Zen3-based CPU code named Trento. AMD's InfinityFabric is not only used internally in each package to connect the dies (and the zen3 chiplets with the I/O die in the Trento CPU), but also to connect the CPU packages to each other and the CPUs to the GPUs, hence creating a unified coherent memory space. This allows each CPU chiplet to access the memory of the closest attached GPU die with full cache coherency, but it is not clear if coherency is usable over the full system. The Aurora supercomputer which uses Intel Sapphire Rapids CPUs and Ponte Vecchio GPUs will also support a unified and cache-coherent memory space. NVIDIA was lagging a bit with the Ampere generation that has no longer a corresponding CPU that supports its NVLink connection, but will return with its own ARM-based CPU code named Grace and the Hopper GPU generation. The Grace Hopper superchip will combine a Grace CPU die and a Hopper GPU die with a coherent interconnect between them, creating a coherent memory space between the CPU and GPU in the same package, though the available whitepapers suggest no coherency between different CPU-GPU packages in the system. Placing a CPU and GPU in the same package also not only ensures much higher bandwidth between both, but also a much lower energy consumption for the data transfers. Future generation products will go even further. The AMD MI300 is technically speaking no longer a GPU but rather an Accelerated Processing Unit or APU as it will integrate CDNA3 GPU dies, Zen4 CPU dies and memory in a single package. The CPU and GPU dies will no longer have physically distinct memory and AMD claims that copy operations between CPU and GPU in a package will be completely unnecessary. MI300-based products should start appearing towards the end of 2023 or early 2024. Intel has also hinted at the Falcon Shores successor to Ponte Vecchio and Rialto Bridge, likely a 2024 or 2025 product, that will also combine CPU and GPU chiplets in a single package with a fully unified memory space. Note that current Intel or AMD-based PC's with integrated GPUs are not better in this respect. The CPU and integrated GPU share the physical RAM in the system, but each has its own reserved memory space and data still needs to be migrated between those spaces. The Apple M1 and later chips on the other hand so seem to have a truly unified memory space, though given the limited information that Apple provides, it is hard to see if it is indeed a fully hardware based solution. The fact that the M1 can be so fast in photo- and video processing apps though indicates that the M1 does indeed have an architecture that allows to make use of all accelerators in a very efficient way.","title":"Offloading"},{"location":"8_Accelerators/8_02_Offloading/#offloading","text":"In early accelerator generations (and this is still the case in 2022) a CPU cannot directly operate on data in the memory of the GPU and vice-versa. Instead data needs to be copied between the memory spaces. Multiple accelerators in a system may or may not share a memory space. NVIDIA NVLink for instance is a technology that can be used to link graphics cards together and create a shared memory space, but even then it is crucial for performance that data is as much as possible in the memory directly attached to a GPU when being used. Many modern GPUs for scientific computing include support for unified memory where CPU and GPUs in the system can share a single logical address space, but under the hood the data still needs to be copied. This feature has been present in, e.g., the NVIDIA Pascal and later generations, where a page fault mechanism would be used to trigger a migration mechanism. Control-wise the program running on the CPU orchestrates the work but passes control to the accelerator to execute those code blocks that can be accelerated by the accelerator. The main problem with this model is that all the data copying that is needed, whether explicitly triggered by the application or implicitly through the unified memory model of some modern GPUs, can really nullify the gain from the accelerator. This is no different from what we saw for distributed computing. Just as for distributed computing, the fraction of the algorithm that cannot be parallelized limits the speed-up, as does the communication overhead, for accelerators the fraction of the application and the overhead to pass data to and from the accelerator will limit the speed-up that can be obtained from the accelerator. Hence it is clear that for future generations of accelerators, the main attention will be in making them more versatile to reduce the fraction that cannot be accelerated, and in integrating them closer with the CPU to reduce or eliminate the overhead in passing data between CPU and GPU. The first signs of this evolution were in some USA pre-exascale systems Summit and Sierra that used a IBM POWER9 CPUs and NVIDIA V100 GPUs. Those GPUs were connected to the CPU through NVLink, NVIDIA's interconnect for GPUs, rather than only PCIe links, so that the memory spaces of CPU and GPU become physically merged, with all data directly addressable from GPU and CPU and cache coherency. This technology of the Summit and Sierra supercomputers is carried over to the first three planned exascale systems of the USA. Frontier, and the European supercomputer LUMI, use the MI250X variant of the AMD CDNA2 architecture. The nodes in these systems consist of 4 MI250X GPUs, with each GPU consisting of two dies, and a custom Zen3-based CPU code named Trento. AMD's InfinityFabric is not only used internally in each package to connect the dies (and the zen3 chiplets with the I/O die in the Trento CPU), but also to connect the CPU packages to each other and the CPUs to the GPUs, hence creating a unified coherent memory space. This allows each CPU chiplet to access the memory of the closest attached GPU die with full cache coherency, but it is not clear if coherency is usable over the full system. The Aurora supercomputer which uses Intel Sapphire Rapids CPUs and Ponte Vecchio GPUs will also support a unified and cache-coherent memory space. NVIDIA was lagging a bit with the Ampere generation that has no longer a corresponding CPU that supports its NVLink connection, but will return with its own ARM-based CPU code named Grace and the Hopper GPU generation. The Grace Hopper superchip will combine a Grace CPU die and a Hopper GPU die with a coherent interconnect between them, creating a coherent memory space between the CPU and GPU in the same package, though the available whitepapers suggest no coherency between different CPU-GPU packages in the system. Placing a CPU and GPU in the same package also not only ensures much higher bandwidth between both, but also a much lower energy consumption for the data transfers. Future generation products will go even further. The AMD MI300 is technically speaking no longer a GPU but rather an Accelerated Processing Unit or APU as it will integrate CDNA3 GPU dies, Zen4 CPU dies and memory in a single package. The CPU and GPU dies will no longer have physically distinct memory and AMD claims that copy operations between CPU and GPU in a package will be completely unnecessary. MI300-based products should start appearing towards the end of 2023 or early 2024. Intel has also hinted at the Falcon Shores successor to Ponte Vecchio and Rialto Bridge, likely a 2024 or 2025 product, that will also combine CPU and GPU chiplets in a single package with a fully unified memory space. Note that current Intel or AMD-based PC's with integrated GPUs are not better in this respect. The CPU and integrated GPU share the physical RAM in the system, but each has its own reserved memory space and data still needs to be migrated between those spaces. The Apple M1 and later chips on the other hand so seem to have a truly unified memory space, though given the limited information that Apple provides, it is hard to see if it is indeed a fully hardware based solution. The fact that the M1 can be so fast in photo- and video processing apps though indicates that the M1 does indeed have an architecture that allows to make use of all accelerators in a very efficient way.","title":"Offloading"},{"location":"8_Accelerators/8_03_CPUs_accelerator_features/","text":"CPUs with accelerator features \u00b6 In the (relatively young) history of personal computers and their microprocessors, successful accelerators were often integrated in CPUs by means of extensions of the CPU instruction set. Though never as performant as a dedicated accelerator, it was often a \"good enough\" solution to the extent that some accelerators even disappeared from the market, and as these are now part of the instruction set of the CPU, programming is also greatly simplified as there is no need to pass data and control to a coprocessor. Vector accelerators have long had an influence on CPUs. The original Intel MMX instructions (which is rumoured to stand for MultiMedia eXtensions) were designed to compete with the DSPs used in sound cards in the second half of the '90s. They were introduced in an update of the Pentium architecture in 1997. This instruction set reused 64-bit registers from the floating point unit, so both could not be used together. Two years later, in 1999, intel introduced the first version of SSE, which used new 128-bit registers. The MMX and SSE instruction sets made it feasible to process audio on the CPU and quickly erased the market of higher-end sound cards with DSPs. The SSE instruction set continued to evolve for several generations and also adopted support for floating point computing. It became essential to get the full performance out of Intel CPUs in scientific codes. The various editions of the SSE instruction set where superseded by the AVX and later AVX2 instruction sets, both of which use 256-bit registers, defining vector operations working on 4 double precision or 8 single precision floating point number simultaneously. Maybe less known is that Intel's current vector instruction set for scientific computing, AVX512, which as the name implies uses 512-bit registers that can hold 8 double precision or 16 single precision floating point numbers, has its origin in a failed project to build a GPU with a simplified x86 architecture, code-named Larrabee. The Larrabee design was recycled as the Xeon Phi, a chip for supercomputing meant to compete with the NVIDIA GPUs, and in a second generation Xeon Phi product the instruction set was thoroughly revised to become the AVX512 instruction set which is the first x86 vector extension with good support for scatter and gather operations and predication. Another interesting processor design is the ARM-based Fujitsu A64fx which is used in the Japanese Fugaku supercomputer which held the crown of fastest computer in the world from June 2020 until June 2022 when it was finally surpassed by the Frontier supercomputer. The A64fx processor was built specifically for supercomputers. Together with ARM, a new vector extension to the ARM instruction set was developed, Scalable Vector Extensions or SVE, which later became an official part of an update of the ARM architecture. The A64fx combines 48 or 52 cores on a chip with a very high bandwidth but relatively small memory system that uses the same technology as the supercomputer GPUs of NVIDIA. It could be used to build supercomputers that were not only as fast as GPU-based systems, but also almost as power-efficient, and performance was good even on some applications that are less suited for vectorisation but also don't run very good on traditional CPUs due to the lack of memory bandwidth in the latter. Matrix accelerators, although fairly new in the market, are also already starting to influence CPU instruction sets. IBM has added matrix instructions for both AI and linear algebra (the latter requiring single and double precision floating point) to the POWER10 processor. Intel has already some instructions in some CPUs but only for low-precision inference, but will add a new instruction set extension called AMX in Sapphire Rapids, a server CPU that should come out in late 2022 or early 2023. It is still only meant for AI applications, supporting 4 and 8-bit integers and Googles bfloat16 data format. Similarly the ARM V9-A instruction set adds Scalable Matrix Extensions to the architecture. As is the case for Intel, these extensions are also aimed at AI applications, supporting 8-bit integer and bfloat16 data formats. Though these CPU instructions certainly don't make CPUs so fast that they beat GPUs, they also have two advantages over accelerators: there is no need to pass control to an remote processor, which can save some time, and there are no issues with data needing to be moved. Also, one should not forget that a single GPU card for a supercomputer easily costs three times or more as much as a single CPU socket with memory (and even more if the lower end SKUs in the CPU line are used), so a CPU doesn't need to be as fast as a GPU to be the most economical solution.","title":"CPUs with accelerator features"},{"location":"8_Accelerators/8_03_CPUs_accelerator_features/#cpus-with-accelerator-features","text":"In the (relatively young) history of personal computers and their microprocessors, successful accelerators were often integrated in CPUs by means of extensions of the CPU instruction set. Though never as performant as a dedicated accelerator, it was often a \"good enough\" solution to the extent that some accelerators even disappeared from the market, and as these are now part of the instruction set of the CPU, programming is also greatly simplified as there is no need to pass data and control to a coprocessor. Vector accelerators have long had an influence on CPUs. The original Intel MMX instructions (which is rumoured to stand for MultiMedia eXtensions) were designed to compete with the DSPs used in sound cards in the second half of the '90s. They were introduced in an update of the Pentium architecture in 1997. This instruction set reused 64-bit registers from the floating point unit, so both could not be used together. Two years later, in 1999, intel introduced the first version of SSE, which used new 128-bit registers. The MMX and SSE instruction sets made it feasible to process audio on the CPU and quickly erased the market of higher-end sound cards with DSPs. The SSE instruction set continued to evolve for several generations and also adopted support for floating point computing. It became essential to get the full performance out of Intel CPUs in scientific codes. The various editions of the SSE instruction set where superseded by the AVX and later AVX2 instruction sets, both of which use 256-bit registers, defining vector operations working on 4 double precision or 8 single precision floating point number simultaneously. Maybe less known is that Intel's current vector instruction set for scientific computing, AVX512, which as the name implies uses 512-bit registers that can hold 8 double precision or 16 single precision floating point numbers, has its origin in a failed project to build a GPU with a simplified x86 architecture, code-named Larrabee. The Larrabee design was recycled as the Xeon Phi, a chip for supercomputing meant to compete with the NVIDIA GPUs, and in a second generation Xeon Phi product the instruction set was thoroughly revised to become the AVX512 instruction set which is the first x86 vector extension with good support for scatter and gather operations and predication. Another interesting processor design is the ARM-based Fujitsu A64fx which is used in the Japanese Fugaku supercomputer which held the crown of fastest computer in the world from June 2020 until June 2022 when it was finally surpassed by the Frontier supercomputer. The A64fx processor was built specifically for supercomputers. Together with ARM, a new vector extension to the ARM instruction set was developed, Scalable Vector Extensions or SVE, which later became an official part of an update of the ARM architecture. The A64fx combines 48 or 52 cores on a chip with a very high bandwidth but relatively small memory system that uses the same technology as the supercomputer GPUs of NVIDIA. It could be used to build supercomputers that were not only as fast as GPU-based systems, but also almost as power-efficient, and performance was good even on some applications that are less suited for vectorisation but also don't run very good on traditional CPUs due to the lack of memory bandwidth in the latter. Matrix accelerators, although fairly new in the market, are also already starting to influence CPU instruction sets. IBM has added matrix instructions for both AI and linear algebra (the latter requiring single and double precision floating point) to the POWER10 processor. Intel has already some instructions in some CPUs but only for low-precision inference, but will add a new instruction set extension called AMX in Sapphire Rapids, a server CPU that should come out in late 2022 or early 2023. It is still only meant for AI applications, supporting 4 and 8-bit integers and Googles bfloat16 data format. Similarly the ARM V9-A instruction set adds Scalable Matrix Extensions to the architecture. As is the case for Intel, these extensions are also aimed at AI applications, supporting 8-bit integer and bfloat16 data formats. Though these CPU instructions certainly don't make CPUs so fast that they beat GPUs, they also have two advantages over accelerators: there is no need to pass control to an remote processor, which can save some time, and there are no issues with data needing to be moved. Also, one should not forget that a single GPU card for a supercomputer easily costs three times or more as much as a single CPU socket with memory (and even more if the lower end SKUs in the CPU line are used), so a CPU doesn't need to be as fast as a GPU to be the most economical solution.","title":"CPUs with accelerator features"},{"location":"8_Accelerators/8_04_Programming_accelerators/","text":"Accelerator programming \u00b6 We will restrict ourselves to GPGPU programming as these are currently the most widely used accelerators in supercomputers. The current state \u00b6 Accelerator programming is, also due to the early stage of the technology, still a mess, and standardisation still has to set in. This is not uncommon in the world of supercomputing: The same happened with message passing where there were several proprietary technologies until the needs were sufficiently well understood to come to a standardisation. Currently there are three competing ecosystems growing: NVIDIA was the first to market with a fairly complete ecosystem for GPGPU programming. They chose to make large parts of their technology proprietary to create a vendor lock-in and keep hardware prices high. CUDA is the main NVIDIA ecosystem. NVIDIA together with partners also created a set of compiler pragma's for C/C++ and Fortran for GPGPU programming with an open consortium, but many other compiler vendors are hesitant to pick those up. The NVIDIA toolset also offers some support for open, broadly standardised technologies, but the support is usually inferior to that for their proprietary technologies or standards that are in practice largely controlled by them. AMD is a latecomer to the market. Their software stack is called ROCm, and they open sourced all components to gain more traction in the market. They basically focus on two technologies that we will discuss: HIP and Open MP. Intel calls its ecosystem oneAPI, as it tries to unify CPU and GPGPU programming, with libraries that make it easy to switch between a CPU-only version and a GPGPU-accelerated version of an application. The oneAPI ecosystem is largely based on Data Parallel C++, an extension of SYCL, and Open MP compiler pragmas. The oneAPI is partly open-sourced. In some cases, only the API is available and vendors should make their own implementation, but, e.g., the sources for their clang/LLVM based Data Parallel C++ compiler are available to all and this has been used already to make ports to NVIDIA and AMD GPU hardware. Lower-level models \u00b6 These models use separate code for the host and the accelerator devices that are then linked together to create the application binary. CUDA \u00b6 CUDA is the best known environment in the HPC world. It is however a proprietary NVIDIA technology. It is possible to write GPGPU kernels using subsets of C, C++ and Fortran. CUDA also comes with many libraries with optimised routines for linear algebra, FFT, neural networks, etc. This makes it by far the most extensive of the lower-level models. CUDA, which launched in 2007, has gone through a rapid evolution though it is now reaching a level of maturity with less frequent major updates. HIP \u00b6 HIP or H eterogeneous Computing I nterface for P ortability is AMD's alternative to CUDA. It tries to be as close as possible as legally possible. The HIP API maps one-to-one on the CUDA API, making it possible to recompile HIP code with the CUDA tools using just a header file. In theory this process is without loss of performance, but there is one caveat: A kernel that is efficient on an AMD GPU may not be the most efficient option on a NVIDIA GPU as, e.g., the vector size is different. So careful programming is needed to ensure efficiency on both platforms. HIP is purely based on C++ with currently no support for Fortran GPU kernels. Feature-wise it is roughly comparable to CUDA 7 or 8, but features that cannot yet be supported by AMD hardware are missing. The ROCm platform also comes with a lot of libraries that provide APIs that are also similar to the APIs provided by their CUDA counterpart to further ease porting code from the CUDA ecosystem to the AMD ROCm ecosystem. HIP also comes with two tools that help in reworking CUDA code into HIP code, though both typically need some manual intervention. OpenCL \u00b6 OpenCL or Open Compute Language is a framework for heterogeneous computing developed by the non-profit, member-driven technology consortium Khronos Group which manages many standards for GPU software (including also OpenGL and its offsprings and Vulkan). It develops vendor-neutral standards. OpenCL is a C and C++-based technology to develop code that not only runs on GPU accelerators, but the code can also be recompiled for CPU only, and some DSPs and FPGAs are also supported. This makes it a very versatile standard and it was for a long time a very attractive standard for commercial software development. It is by far not as advanced as CUD or not even HIP. The evolution of the standard has been very slow lately, with vendors hardly picking up many of the features of the 2.x versions. This lead to version 3.0 of the standard that defined a clear baseline and made the other features explicitly optional so that programmers at least know what they can expect. OpenCL is in principle supported on NVIDIA, AMD and Intel hardware, but the quality of the implementation is not always spectacular. There are also some open source implementations with varying hardware support. It has been used in supercomputing software though it was not always the only model offered as several packages also contained specialised CUDA code for better performance on NVIDIA hardware. The molecular dynamics package GROMACS is one example, and they will be switching away from OpenCL to newer technologies to support non-NVIDIA hardware. OpenCL is largely superseded by newer technologies developed by the Khronos Group. Vulkan is their effort to create an API that unifies 3D graphics and computing and is mostly oriented towards game programming etc., but less towards supercomputing (as several GPUs used for supercomputing even start omitting graphics-specific circuits) while SYCL, which we will discuss later in these notes, is a new initiative for GPGPU programming. Compiler directives \u00b6 In these models, there are no separate sources for the GPU that are compiled with a separate compiler. Instead, there is a single base of code with compiler directives instructing the compiler how certain parts of the code can be offloaded to an accelerator. These directives appear as comments in a Fortran code to compilers that do not support the technology or use the #pragma directive in C/C++ to make clear that they are directives to compilers that support those. OpenACC \u00b6 OpenACC was the first successful compiler directive model for GPGPU computing. It supports C, C++ and Fortran programming. The first version of standard made its debut at the supercomputing computing conference SC'11 in November 2011. The technology was largely developed by 4 companies: the GPU company NVIDIA, the compiler company Portland Group which was later renamed PGI and later bought by NVIDIA, the supercomputer manufacturer Cray which got bought by HPE and largely lost interest in OpenACC, now more promoting an alternative, and CAPS Entreprises, a French company and university spin-off creating tools for accelerator programming that failed in 2013. The OpenACC standard is currently controlled by the OpenACC Organization, and though it does count other companies that develop GPUs among its members, it still seems rather dominated by NVIDIA which may explain why other companies are somewhat hesitant to pick up the standard. The standard is at the time of writing of this section at version 3.1, released in November 2021, but is updated almost annually in November. OpenACC is well supported on NVIDIA hardware through the NVIDIA HPC compilers (which is the new name of the PGI compilers adopted after the integration of PGI in NVIDIA). GCC offers some support on NVIDIA and some AMD hardware for version 2.6 of the standard (the November 2017 version) since version 10 of the GCC compilers., but the evolution is slow and performance is often not that great. More promising for the future is the work going on in the clang/LLVM community to support OpenACC, as this effort is largely driven by the USA national labs who want to avoid having to port all OpenACC-based code developed for the NVIDIA based pre-exascale supercomputers to other models to run on the new AMD and Intel based exascale supercomputers. In fact, the clang/LLVM ecosystem is the future for scientific computing and not the GNU Compiler Collection ecosystem as most compiler vendors already base their compilers on that technology. The NVIDIA, AMD and new Intel compilers are all based on LLVM and the clang frontend for C and C++-support, with NVIDIA and AMD still using a Fortran front-end developed by PGI and donated to the LLVM project while Intel is already experimenting with a new community-developed more modern front-end for Fortran. OpenACC support in the LLVM ecosystem will build upon the OpenMP support, the technology that we will discuss next, using extensions for those OpenACC features that still have no equivalent in OpenMP. However, as of version 15 (summer 2022) this support is still very incomplete and experimental. OpenMP \u00b6 We've already discussed OpenMP as a technology for shared memory parallelism and for vectorisation (the latter since version 4.0 of the standard). But OpenMP is nowadays even more versatile. Since version 4.0 of the standard, released in July 2013, there is also support for offloading to accelerators. That support was greatly improved in version 5.0 of the standard which was released at the SC'18 supercomputer conference. It became a more descriptive and less prescriptive model (offering the compiler enough information to decide what it should do rather then enforcing the compiler to do something in a particular way), with the prescriptive nature being criticised a lot by the OpenACC community who claimed superiority because of this. It also contained much better support for debuggers, performance monitoring tools, etc. OpenMP has since had minor extensions i the form of version 5.1 at SC'20 and 5.2 at SC'21. The standard is controlled by a much larger consortium than the OpenACC standard. OpenMP is an important technology in the AMD ROCm and Intel oneAPI ecosystems. Intel has in fact supported OpenMP offload to some of its own GPUs for many years, long before establishing the oneAPI ecosystem. GCC has had some support for OpenMP offload to NVIDIA hardware since version 7 and to some AMD hardware since version 10. However, as is the case for OpenACC, it may not be the most performant option on the market. The clang/LLVM ecosystem is working hard for full support of the newest OpenMP standards. The AMD and new oneAPI Intel compilers are in fact fully based on clang and LLVM using some of their own plug-ins to offer additional features, and the NVIDIA HPC compiler also largely seems to be based on this technology. NVIDIA and AMD also use the LLVM backend to compile CUDA and HIP kernels respectively, showing once more that this is the compiler ecosystem for the future of scientific computing. C++ extensions \u00b6 SYCL \u00b6 SYCL is a programming model based on recent versions of the C++ standard. The earlier drafts of the standard go back to the 2014-2015 time frame, but SYCL really took off with the 2020 version of the standard. SYCL is also a logical successor to OpenCL, but now making it possible to target CPU, GPU, FPGA and possibly other types of accelerators from a single code base. Though the programmer is of course free to provide alternative implementations for different hardware for better performance, it is not needed by design. SYCL is heavily based on C++ template libraries, but to generate code for accelerators it still needs a specific compiler. There are several compilers in development with varying levels of support for the standard, targeting not only GPUs, but also, e.g., the NEC SX Aurora Tsubasa vector boards. Most if not all of these implementations are again based on Clang and LLVM. One implementation worth mentioning is hipSYCL, which as its name suggests targets hip for the backend and hence can support both AMD and NVIDIA GPUs, but can also target pure CPU systems and now even contains experimental support for Intel GPUs. DPC++ or Data Parallel C++ \u00b6 Data Parallel C++ is the oneAPI implementation of SYCL. It is basically a project by Intel to bring SYCL into LLVM, and all code is open sourced. The implementation does not only cover CPUs and Intel GPUs, but with the help of others (including the company Codeplay that has since been acquired by Intel) it also adds support for NVIDIA and AMD GPUs and even Intel FPGAs, the latter through a backend based on OpenCL and SPIR. That support is not included in the binaries that Intel distributes though. When DPC++ initially launched, it was really an extension of the then-current SYCL standard, which is why it gets a separate subsection in these notes. However, it is currently promoted as a SYCL 2020 implementation. C++AMP \u00b6 C++AMP or C++ Accelerated Massive Parallelism is a programming model developed by Microsoft. It consists of C++ libraries and a minor extension to the language. There are experimental implementations for non-Microsoft environments, but these are not really popular and the technology is not really taking off in scientific computing. The technology is now deprecated by Microsoft. Yet we want to mention it in these notes as it was a source of inspiration for SYCL. Frameworks \u00b6 There exist also several C++ frameworks or abstraction libraries that support creating code that is portable to regular CPUs and GPU systems of various vendors. They let you exploit all levels of parallelism in a supercomputer except distributed computing. Kokkos is a framework developed by Sandia National Labs and probably the most popular one of the frameworks mentioned here. It was first released in 2011 already but grew to a complete ecosystem with tools to support debugging, profiling and tuning also, and now even some support for distributed computing also. Kokkos already supports backends for CUDA and ROCm, and there are experimental backends that can also support the Intel GPUs that will be used in the Aurora supercomputer. RAJA is a framework developed at Lawrence Livermore National Laboratory, based on standard C++11. Just as Kokkos, RAJA has several backends supporting SIMD, threading through the TBB library or OpenMP, but also GPU computing through NVIDIA CUDA, AMD HIP and OpenMP offload, though not all back-ends are as mature or support all features.In particular the TBB and OpenMP target offload (the latter needed for Intel GPUs) are still experimental at the time this section was written (October 2022). Alpaka is a framework developed by CASUS - Center for Advanced Systems Understanding of the Helmholtz Zentrum Dresden Rossendorf. Alpaka also supports various backends, including a CUDA back-end for NVIDIA GPUs. There is also work going on on a HIP backend for AMD GPUs, with support for Intel GPUs coming through an OpenMP offload backend. Libraries \u00b6 One can often rely on already existing libraries when developing software for GPUs. NVIDIA CUDA comes with a wide range of libraries. AMD ROCm provides several libraries that mimic (subsets of) libraries in the CUDA ecosystem, also easing porting from NVIDIA to AMD hardware. Intel has adapted several of its CPU libraries to use GPU acceleration also in its oneAPI platform. However, there are also several vendor-neutral libraries, e.g., MAGMA which stands for Matrix Algebra on GPU and Multicore Architectures, which is an early example of such a library. heFFTe is a library for FFT","title":"Accelerator prorgramming"},{"location":"8_Accelerators/8_04_Programming_accelerators/#accelerator-programming","text":"We will restrict ourselves to GPGPU programming as these are currently the most widely used accelerators in supercomputers.","title":"Accelerator programming"},{"location":"8_Accelerators/8_04_Programming_accelerators/#the-current-state","text":"Accelerator programming is, also due to the early stage of the technology, still a mess, and standardisation still has to set in. This is not uncommon in the world of supercomputing: The same happened with message passing where there were several proprietary technologies until the needs were sufficiently well understood to come to a standardisation. Currently there are three competing ecosystems growing: NVIDIA was the first to market with a fairly complete ecosystem for GPGPU programming. They chose to make large parts of their technology proprietary to create a vendor lock-in and keep hardware prices high. CUDA is the main NVIDIA ecosystem. NVIDIA together with partners also created a set of compiler pragma's for C/C++ and Fortran for GPGPU programming with an open consortium, but many other compiler vendors are hesitant to pick those up. The NVIDIA toolset also offers some support for open, broadly standardised technologies, but the support is usually inferior to that for their proprietary technologies or standards that are in practice largely controlled by them. AMD is a latecomer to the market. Their software stack is called ROCm, and they open sourced all components to gain more traction in the market. They basically focus on two technologies that we will discuss: HIP and Open MP. Intel calls its ecosystem oneAPI, as it tries to unify CPU and GPGPU programming, with libraries that make it easy to switch between a CPU-only version and a GPGPU-accelerated version of an application. The oneAPI ecosystem is largely based on Data Parallel C++, an extension of SYCL, and Open MP compiler pragmas. The oneAPI is partly open-sourced. In some cases, only the API is available and vendors should make their own implementation, but, e.g., the sources for their clang/LLVM based Data Parallel C++ compiler are available to all and this has been used already to make ports to NVIDIA and AMD GPU hardware.","title":"The current state"},{"location":"8_Accelerators/8_04_Programming_accelerators/#lower-level-models","text":"These models use separate code for the host and the accelerator devices that are then linked together to create the application binary.","title":"Lower-level models"},{"location":"8_Accelerators/8_04_Programming_accelerators/#cuda","text":"CUDA is the best known environment in the HPC world. It is however a proprietary NVIDIA technology. It is possible to write GPGPU kernels using subsets of C, C++ and Fortran. CUDA also comes with many libraries with optimised routines for linear algebra, FFT, neural networks, etc. This makes it by far the most extensive of the lower-level models. CUDA, which launched in 2007, has gone through a rapid evolution though it is now reaching a level of maturity with less frequent major updates.","title":"CUDA"},{"location":"8_Accelerators/8_04_Programming_accelerators/#hip","text":"HIP or H eterogeneous Computing I nterface for P ortability is AMD's alternative to CUDA. It tries to be as close as possible as legally possible. The HIP API maps one-to-one on the CUDA API, making it possible to recompile HIP code with the CUDA tools using just a header file. In theory this process is without loss of performance, but there is one caveat: A kernel that is efficient on an AMD GPU may not be the most efficient option on a NVIDIA GPU as, e.g., the vector size is different. So careful programming is needed to ensure efficiency on both platforms. HIP is purely based on C++ with currently no support for Fortran GPU kernels. Feature-wise it is roughly comparable to CUDA 7 or 8, but features that cannot yet be supported by AMD hardware are missing. The ROCm platform also comes with a lot of libraries that provide APIs that are also similar to the APIs provided by their CUDA counterpart to further ease porting code from the CUDA ecosystem to the AMD ROCm ecosystem. HIP also comes with two tools that help in reworking CUDA code into HIP code, though both typically need some manual intervention.","title":"HIP"},{"location":"8_Accelerators/8_04_Programming_accelerators/#opencl","text":"OpenCL or Open Compute Language is a framework for heterogeneous computing developed by the non-profit, member-driven technology consortium Khronos Group which manages many standards for GPU software (including also OpenGL and its offsprings and Vulkan). It develops vendor-neutral standards. OpenCL is a C and C++-based technology to develop code that not only runs on GPU accelerators, but the code can also be recompiled for CPU only, and some DSPs and FPGAs are also supported. This makes it a very versatile standard and it was for a long time a very attractive standard for commercial software development. It is by far not as advanced as CUD or not even HIP. The evolution of the standard has been very slow lately, with vendors hardly picking up many of the features of the 2.x versions. This lead to version 3.0 of the standard that defined a clear baseline and made the other features explicitly optional so that programmers at least know what they can expect. OpenCL is in principle supported on NVIDIA, AMD and Intel hardware, but the quality of the implementation is not always spectacular. There are also some open source implementations with varying hardware support. It has been used in supercomputing software though it was not always the only model offered as several packages also contained specialised CUDA code for better performance on NVIDIA hardware. The molecular dynamics package GROMACS is one example, and they will be switching away from OpenCL to newer technologies to support non-NVIDIA hardware. OpenCL is largely superseded by newer technologies developed by the Khronos Group. Vulkan is their effort to create an API that unifies 3D graphics and computing and is mostly oriented towards game programming etc., but less towards supercomputing (as several GPUs used for supercomputing even start omitting graphics-specific circuits) while SYCL, which we will discuss later in these notes, is a new initiative for GPGPU programming.","title":"OpenCL"},{"location":"8_Accelerators/8_04_Programming_accelerators/#compiler-directives","text":"In these models, there are no separate sources for the GPU that are compiled with a separate compiler. Instead, there is a single base of code with compiler directives instructing the compiler how certain parts of the code can be offloaded to an accelerator. These directives appear as comments in a Fortran code to compilers that do not support the technology or use the #pragma directive in C/C++ to make clear that they are directives to compilers that support those.","title":"Compiler directives"},{"location":"8_Accelerators/8_04_Programming_accelerators/#openacc","text":"OpenACC was the first successful compiler directive model for GPGPU computing. It supports C, C++ and Fortran programming. The first version of standard made its debut at the supercomputing computing conference SC'11 in November 2011. The technology was largely developed by 4 companies: the GPU company NVIDIA, the compiler company Portland Group which was later renamed PGI and later bought by NVIDIA, the supercomputer manufacturer Cray which got bought by HPE and largely lost interest in OpenACC, now more promoting an alternative, and CAPS Entreprises, a French company and university spin-off creating tools for accelerator programming that failed in 2013. The OpenACC standard is currently controlled by the OpenACC Organization, and though it does count other companies that develop GPUs among its members, it still seems rather dominated by NVIDIA which may explain why other companies are somewhat hesitant to pick up the standard. The standard is at the time of writing of this section at version 3.1, released in November 2021, but is updated almost annually in November. OpenACC is well supported on NVIDIA hardware through the NVIDIA HPC compilers (which is the new name of the PGI compilers adopted after the integration of PGI in NVIDIA). GCC offers some support on NVIDIA and some AMD hardware for version 2.6 of the standard (the November 2017 version) since version 10 of the GCC compilers., but the evolution is slow and performance is often not that great. More promising for the future is the work going on in the clang/LLVM community to support OpenACC, as this effort is largely driven by the USA national labs who want to avoid having to port all OpenACC-based code developed for the NVIDIA based pre-exascale supercomputers to other models to run on the new AMD and Intel based exascale supercomputers. In fact, the clang/LLVM ecosystem is the future for scientific computing and not the GNU Compiler Collection ecosystem as most compiler vendors already base their compilers on that technology. The NVIDIA, AMD and new Intel compilers are all based on LLVM and the clang frontend for C and C++-support, with NVIDIA and AMD still using a Fortran front-end developed by PGI and donated to the LLVM project while Intel is already experimenting with a new community-developed more modern front-end for Fortran. OpenACC support in the LLVM ecosystem will build upon the OpenMP support, the technology that we will discuss next, using extensions for those OpenACC features that still have no equivalent in OpenMP. However, as of version 15 (summer 2022) this support is still very incomplete and experimental.","title":"OpenACC"},{"location":"8_Accelerators/8_04_Programming_accelerators/#openmp","text":"We've already discussed OpenMP as a technology for shared memory parallelism and for vectorisation (the latter since version 4.0 of the standard). But OpenMP is nowadays even more versatile. Since version 4.0 of the standard, released in July 2013, there is also support for offloading to accelerators. That support was greatly improved in version 5.0 of the standard which was released at the SC'18 supercomputer conference. It became a more descriptive and less prescriptive model (offering the compiler enough information to decide what it should do rather then enforcing the compiler to do something in a particular way), with the prescriptive nature being criticised a lot by the OpenACC community who claimed superiority because of this. It also contained much better support for debuggers, performance monitoring tools, etc. OpenMP has since had minor extensions i the form of version 5.1 at SC'20 and 5.2 at SC'21. The standard is controlled by a much larger consortium than the OpenACC standard. OpenMP is an important technology in the AMD ROCm and Intel oneAPI ecosystems. Intel has in fact supported OpenMP offload to some of its own GPUs for many years, long before establishing the oneAPI ecosystem. GCC has had some support for OpenMP offload to NVIDIA hardware since version 7 and to some AMD hardware since version 10. However, as is the case for OpenACC, it may not be the most performant option on the market. The clang/LLVM ecosystem is working hard for full support of the newest OpenMP standards. The AMD and new oneAPI Intel compilers are in fact fully based on clang and LLVM using some of their own plug-ins to offer additional features, and the NVIDIA HPC compiler also largely seems to be based on this technology. NVIDIA and AMD also use the LLVM backend to compile CUDA and HIP kernels respectively, showing once more that this is the compiler ecosystem for the future of scientific computing.","title":"OpenMP"},{"location":"8_Accelerators/8_04_Programming_accelerators/#c-extensions","text":"","title":"C++ extensions"},{"location":"8_Accelerators/8_04_Programming_accelerators/#sycl","text":"SYCL is a programming model based on recent versions of the C++ standard. The earlier drafts of the standard go back to the 2014-2015 time frame, but SYCL really took off with the 2020 version of the standard. SYCL is also a logical successor to OpenCL, but now making it possible to target CPU, GPU, FPGA and possibly other types of accelerators from a single code base. Though the programmer is of course free to provide alternative implementations for different hardware for better performance, it is not needed by design. SYCL is heavily based on C++ template libraries, but to generate code for accelerators it still needs a specific compiler. There are several compilers in development with varying levels of support for the standard, targeting not only GPUs, but also, e.g., the NEC SX Aurora Tsubasa vector boards. Most if not all of these implementations are again based on Clang and LLVM. One implementation worth mentioning is hipSYCL, which as its name suggests targets hip for the backend and hence can support both AMD and NVIDIA GPUs, but can also target pure CPU systems and now even contains experimental support for Intel GPUs.","title":"SYCL"},{"location":"8_Accelerators/8_04_Programming_accelerators/#dpc-or-data-parallel-c","text":"Data Parallel C++ is the oneAPI implementation of SYCL. It is basically a project by Intel to bring SYCL into LLVM, and all code is open sourced. The implementation does not only cover CPUs and Intel GPUs, but with the help of others (including the company Codeplay that has since been acquired by Intel) it also adds support for NVIDIA and AMD GPUs and even Intel FPGAs, the latter through a backend based on OpenCL and SPIR. That support is not included in the binaries that Intel distributes though. When DPC++ initially launched, it was really an extension of the then-current SYCL standard, which is why it gets a separate subsection in these notes. However, it is currently promoted as a SYCL 2020 implementation.","title":"DPC++ or Data Parallel C++"},{"location":"8_Accelerators/8_04_Programming_accelerators/#camp","text":"C++AMP or C++ Accelerated Massive Parallelism is a programming model developed by Microsoft. It consists of C++ libraries and a minor extension to the language. There are experimental implementations for non-Microsoft environments, but these are not really popular and the technology is not really taking off in scientific computing. The technology is now deprecated by Microsoft. Yet we want to mention it in these notes as it was a source of inspiration for SYCL.","title":"C++AMP"},{"location":"8_Accelerators/8_04_Programming_accelerators/#frameworks","text":"There exist also several C++ frameworks or abstraction libraries that support creating code that is portable to regular CPUs and GPU systems of various vendors. They let you exploit all levels of parallelism in a supercomputer except distributed computing. Kokkos is a framework developed by Sandia National Labs and probably the most popular one of the frameworks mentioned here. It was first released in 2011 already but grew to a complete ecosystem with tools to support debugging, profiling and tuning also, and now even some support for distributed computing also. Kokkos already supports backends for CUDA and ROCm, and there are experimental backends that can also support the Intel GPUs that will be used in the Aurora supercomputer. RAJA is a framework developed at Lawrence Livermore National Laboratory, based on standard C++11. Just as Kokkos, RAJA has several backends supporting SIMD, threading through the TBB library or OpenMP, but also GPU computing through NVIDIA CUDA, AMD HIP and OpenMP offload, though not all back-ends are as mature or support all features.In particular the TBB and OpenMP target offload (the latter needed for Intel GPUs) are still experimental at the time this section was written (October 2022). Alpaka is a framework developed by CASUS - Center for Advanced Systems Understanding of the Helmholtz Zentrum Dresden Rossendorf. Alpaka also supports various backends, including a CUDA back-end for NVIDIA GPUs. There is also work going on on a HIP backend for AMD GPUs, with support for Intel GPUs coming through an OpenMP offload backend.","title":"Frameworks"},{"location":"8_Accelerators/8_04_Programming_accelerators/#libraries","text":"One can often rely on already existing libraries when developing software for GPUs. NVIDIA CUDA comes with a wide range of libraries. AMD ROCm provides several libraries that mimic (subsets of) libraries in the CUDA ecosystem, also easing porting from NVIDIA to AMD hardware. Intel has adapted several of its CPU libraries to use GPU acceleration also in its oneAPI platform. However, there are also several vendor-neutral libraries, e.g., MAGMA which stands for Matrix Algebra on GPU and Multicore Architectures, which is an early example of such a library. heFFTe is a library for FFT","title":"Libraries"},{"location":"8_Accelerators/8_05_Status_GPU_computing/","text":"Status of GPU computing \u00b6 Too much hype \u00b6 Even though GPU computing, or accelerator computing in general, is definitely here to stay and important for the future of not only supercomputing but computing in general as what can be done with a given amount of power is important in many markets (also on mobiles that have to get their power from a battery), it does not mean that it works for all applications. Benchmarking of supercomputers with accelerators is often more benchmark et ing. GPU computing is often overhyped using incomplete benchmarks (basically only benchmark that part of the code that can be properly accelerated), marketing by numbers (redefine common terms to get bigger numbers, something that in particular NVIDIA is very good at) and comparing apples and oranges by comparing systems with a very different price or total cost of ownership, e.g., comparing a server with multiple accelerators costing 60k EURO or more with a standard dual socket server costing only 10k EURO and using only one fifth of the power. The NVIDIA vendor lock-in and its success in the market have made accelerators very expensive. At the current price point, GPU computing only makes sense from a price point of view if the speed-up at the application level is a factor of 2.5 or more per accelerator card compared to a standard medium-sized dual socket node. As we have seen, some features of accelerators have been carried over to traditional CPUs in the form of new instructions supporting vector and nowadays even matrix computing, and in some cases they may just be the better choice as CPUs have more memory readily available and as programming is easier. Problems and solutions \u00b6 There are several problems with current GPU designs: The amount of memory that a GPU can address directly and has fast enough access to, is limited. 2020 GPUs were limited to 32-48 GB of memory, in early 2021 a 80 GB GPU appeared on the market, and in 2022 it is technically possible to have 128 GB on a single package. But this is still small to typical memory sizes on a regular dual socket server that is much slower than the GPU. Programming bottleneck: Having to organise all data transport manually and working with separate memory spaces is a pain. The link between the CPU and the GPU is a bottleneck. The PCIe buss that typically links the CPU to the GPU has a rather limited bandwidth compared to either the bandwidth of the CPU memory of the bandwidth of GPU memory. The GPU is rather slow for serial code, so that code often has to run on the host. Which is then an issue since it may require additional copying between the CPU and GPU. However, there is hope. The amount of memory that can be used by a GPU will increase a lot the coming years. Both the memory packages are getting bigger by stacking more dies in 3D, and the number of memory packages that can be integrated in the overall GPU package is increasing. As of 2022, 8 memory stacks in a GPU package is feasible as is shown by the AMD MI250(X) GPUs, while in 2023-2024 12 memory stacks in a single GPU package should become a possibility. The two combined may make memory sizes of 192 and likely even 384 GB possible by 2024 or 2025. The programming bottleneck can already be partially solved by unified memory, using memory pointers that work on both CPU and GPU, and further hardware support for virtual memory that can then trigger software that migrates memory pages under the hood. NVIDIA GPUs have had some of those features since the Pascal generation in 2017. However, one can do even better by physically sharing memory spaces between GPU and CPU, and supporting some level of coherency so that the CPU can access the GPU memory without risk of inconsistent data, or even the GPU can access the memory of the CPU, though that is less interesting as the CPU can never provide the memory bandwidth that the GPU needs to perform well. NUMA style shared memory spaces were first explored in the Sierra and Summit USA pre-exascale systems (as we discussed before) but is now also seen in the MI250X GPU from AMD which is a special version of the MI200 family connecting to the CPU through InfinityFabric, the same interconnect that AMD uses internally in its sockets to link the CPU dies to the I/O die, and also uses to connect CPU sockets or to connect GPUs to each other in the MI100 and MI200 generations. The Intel Ponte Vecchio GPU combined with the Sapphire Rapids CPU that will be used in the Aurora supercomputer supports a similar feature, as does the NVIDIA Grace CPU and Hopper GPU integrated in a single package. The physical sharing of memory spaces with a level of cache coherency is also the first step in solving the problem of copying data back and forth all the time. E.g., if a CPU can access memory attached to the GPU without risks of coherency problems, then there is less need to copy full memory pages and also not to copy those back, as the link that is used in those GPU systems to connect the CPU to GPU is as fast as the links that are typically used to connect two CPU sockets. The NVIDIA Grace Hopper \"superchip\" shows the next step. By integrating the CPU and GPU in the same package, it is possible to have a much higher bandwidth between both, reducing the copying bottleneck in cases where copying is still needed. However, with the AMD MI300 and Intel Falcon Shores and without doubt a future unannounced NVIDIA product, we will see even closer integration where CPU and GPU chiplets share the memory controllers. The Apple M series chips give an indication of what can be obtained with such a system, as these systems perform way better in some applications that use acceleration than one would expect from looking at systems with discrete GPUs with similar theoretical performance. We will discuss this evolution in some more detail in the next section. Evolution of GPU nodes \u00b6 GPU subsystem connected via PCIe \u00b6 Around 2016, a typical GPU compute node consisted of a dual socket server with 1-4 GPUs attached to the CPUs. A typical design would have been: The red line between the two CPUs denotes a fully cache coherent link between the CPUs. In 2016 this would very likely have been either Intel CPUs or IBM POWER CPUs, and both had proprietary fully cache coherent links to link CPUs in a shared memory system. The red link between the GPUs denotes a similarly proprietary connection between the CPUs for easy data transfer between the CPUs at typically a much higher bandwidth than that offered by the connections between the CPU and GPU. However, not all systems used such a link between graphics cards. A CPU was connected to the GPUs using PCIe, and similarly a network interface would also be connected to a CPU using PCIe. Later designs tried to move the network interconnect closer to the GPUs, as they perform the majority of the calculations and hence also contain the data that should be send to other nodes. A typical 4-GPU node based on the NVIDIA Ampere A100 GPU launched in 2020 would look similar to: There are many variants of quad GPU designs with the A100 GPU, with single and dual socket CPU servers. However, it is often advantageous to have all GPUs connected to a single CPU, but only the single socket AMD Epyc CPU has enough PCIe connections to do so and still be able to also attach one or two network interface cards. The above design solves this in a different way. It uses two PCIe switches (the magenta circles), and each PCIe switch connects the CPU to two of the GPUs and a network interface card (the latter denoted by the dark purple line). This also brings the network very close to the GPU. Cache-coherent interconnect between CPU and GPU \u00b6 The next evolution of this design is used in the USA pre-exascale systems Sierra and Summit, both using IBM POWER9 CPUs and NVIDIA Volta V100 GPUs. More recently, the idea is revived in the GPU compute nodes of the Frontier and LUMI supercomputers based on the MI250X GPU. A simplified diagram of the LUMI and Frontier GPU nodes is: The GPU compute nodes of LUMI and Frontier use a special variant of the Zen3-based AMD Epyc processor. In this variant, the PCIe lanes are replaced by InfinityFabric connections. In an MI250X, the 4 GPU packages are connected with each other and with the CPU through Infinity Fabric links. Each GPU package connects to its own quarter of the AMD Epyc processor (remember from earlier that the I/O die is subdivided in 4 quarters, in this case each connected to two CPU chiplets with 8 cores each). This creates a coherent memory space. Also noteworthy is that the interconnect is no longer connected to the CPU, but the 4 network cards are each connected to a GPU package (through a PCIe interface). This makes this compute node really a GPU-first system, almost a system where the CPU is only used for those parts of the code that cannot be accelerated at all by a GPU and to run the Linux operating system. The true picture of the MI250X GPU node is a bit more complicated though. Each GPU package contains two GPU dies, and these are connected to each other through some Infinity Fabric links. Each GPU die connects to 4 memory packages, with 64 GB of memory per GPU die. However, the connection between two GPU dies is sufficiently bandwidth-starved that programming wise a single GPU package should be considered as two separate GPUs. Each individual GPU die has its own InfinityFabric link to the CPU and seems to have a preferential CPU chiplet. Even though the connection between the GPU packages appears to be an all-to-all connection, this is not true when one looks at the connections between the GPU dies. Integration of CPU and GPU in a single package: NVIDIA GH100 \u00b6 Even though the MI250X has a level of cache coherency, using the memory in a unified matter is still a problem, partly because of the extra latency introduced by the fairly long distance between the CPU and GPU, partly also because of the limited memory bandwidth on the CPU side. NVIDIA's Grace Hopper Superchip, expected in 2023, works on those two problems by integrating the CPU and GPU on a single package and not only putting the GPU memory, but also the CPU memory on that package. The CPU part is called Grace and is a CPU based on the ARMv9 architecture, the newest version of the ARM architecture at the time of design of the Grace processor. The GPU part of the package is the Hopper architecture and similar to the one in the SXM and PCIe cards released in the fall of 2022, but with all 6 memory controllers enabled. The CPU and GPU still both have their own memory controllers and basically behave as separate NUMA domains, but as the connection between the two has been brought on-chip the bandwidth between CPU and GPU is a lot higher than in the MI250X architecture OR the Summit and Sierra systems with IBM POWER9 and NVIDIA V100 chips. The CPU memory is not provided through external DIMMs, but through a number of internal LPDDR5X modules integrated in the CPU-GPU package in a similar way as the GPU memory has been integrated in the package for a number of generations already. Integration of this memory type is popular in smartphones where it saves both space and power, and is also used in the Apple Silicon M-series chips, where in addition to space and power savings it also provides higher bandwidth. In the Grace chip it enables a very high memory bandwidth for the CPU, even 20% better than what the AMD EPYC 4 generation offers per socket (but not as much as the Intel Sapphire Rapids MAX chips that incorporate a small amount of GPU-style memory in the package) while still offering a high memory capacity. Pre-release material claims up to 512 GB per CPU, but this number has to be taken with a grain of salt as it does not correspond to the available memory modules that can be found in catalogues of memory providers in early 2023. The Grace Hopper superchip provides two types of external connections. There are a number of regular PCIe connections that come from the CPU die. They can be used to attach, e.g., network cards (including a HPC interconnect) and NVMe drives. There are also a number of NVLINK connections coming from the GPU die. These connections are similar to the ones already used in previous generation NVIDIA GPUs to link GPU packages and offer a much higher bandwidth interconnect. They can be used to interconnect a number of Grace Hopper superchips in a NUMA shared memory way. This makes the Grace Hopper superchip a very flexible building block. Depending on the needs those chips can be combined in different ways. One can connect individual single package nodes through any HPC interconnect in a distributed memory way. It is also possible to use NVLINK technology to connect multiple superchips into a single system where each CPU and each GPU appears as a NUMA node to the OS. The bandwidth of this connection is much higher than the typical inter-socket interconnect in CPU-based servers, but still a lot lower than the memory bandwidth that the GPU memory system can offer. So it is very important that applications exploit the NUMA structure of the memory. It is also possible to combine both approaches: Build supercomputer nodes with up to 8 superchips with a single NVLINK switch level, and link those nodes together using a traditional HPC interconnect. A possible layout of such a node is shown in the following figure: In this figure we combine 4 Grace Hopper packages in a single node and have chose to connect each package directly to the interconnect for additional bandwidth. Fully unified CPU and GPU: AMD MI300 and Intel Falcon Shores \u00b6 The AMD MI250X is really just a transition to the MI300 series and the Intel Falcon Shores architecture, that go one step further beyond the integration that the NVIDIA Grace Hopper architecture offers. In that generation, expected to come to market in late 2023 in the case of AMD and 2024 or 2025 in the case of Intel, the CPU and GPU will merge completely and share on-package memory. In fact, the reality is that memory outside the package is also starting to limit CPU performance as an increasing number of CPU codes becomes memory bandwidth bound, so even for the CPU it makes sense to switch to smaller but much higher bandwidth memory in the package. The AMD MI300 and Intel Falcon Shores will fully integrate the CPU and GPU chiplets and memory controllers with memory in a single package. Whereas the MI250x has cache coherent memory but still a lot of overhead when the GPU wants to access CPU memory or the other way around, in the MI300 generation the CPU and GPU memory is fully unified (physical and virtual), with both sharing the same memory controllers and memory, which will enable to fully eliminate redundant memory copies. MI300 was first mentioned at the AMD Financial Analyst Day in June 2022 and at CES'2023 (early January) , where a full package was shown, but still with very little detail. It was announced the one MI300 package will combine a powerful GPU with 24 CPU cores (presumably 3 Zen4 chiplets) and 8 HBM3 modules for a total of 128 GB RAM. The chip will consists of 13 chiplets stacked in two layers, with 4 chiplets at the bottom (presumably the memory controllers as they produce less heat) and 9 chiplets at the top (which could be 3 CPU chiplets and 6 CDNA3 GPU dies?). A supercomputer node based on this chip could look a bit like Here we see four packages integrating one or more CPU chiplets, one or more GPU dies and memory in a single packages. The four packages have an all-to-all connection likely using a new generation of InfinityFabric, and each GPU packages also connects to a network card using PCIe. It is expected that the techniques to connect dies will have evolved enough that the GPU dies in a single package will work as a single GPU. In fact, those improved connections will also be needed to have equal access to memory from the GPU and CPU chiplets. It does look though that the total memory capacity of such a node may be rather limited, unless there would be some unannounced feature to attach slower external memory, e.g., through additional DIMM slots or through (even slower) CXL memory boards. But for applications that don't need those large memory capacities and scale nicely over NUMA domains and then further over distributed memory nodes, the more uniform architecture will certainly make life easier and offer great performance benefits.","title":"Status of GPU computing"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#status-of-gpu-computing","text":"","title":"Status of GPU computing"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#too-much-hype","text":"Even though GPU computing, or accelerator computing in general, is definitely here to stay and important for the future of not only supercomputing but computing in general as what can be done with a given amount of power is important in many markets (also on mobiles that have to get their power from a battery), it does not mean that it works for all applications. Benchmarking of supercomputers with accelerators is often more benchmark et ing. GPU computing is often overhyped using incomplete benchmarks (basically only benchmark that part of the code that can be properly accelerated), marketing by numbers (redefine common terms to get bigger numbers, something that in particular NVIDIA is very good at) and comparing apples and oranges by comparing systems with a very different price or total cost of ownership, e.g., comparing a server with multiple accelerators costing 60k EURO or more with a standard dual socket server costing only 10k EURO and using only one fifth of the power. The NVIDIA vendor lock-in and its success in the market have made accelerators very expensive. At the current price point, GPU computing only makes sense from a price point of view if the speed-up at the application level is a factor of 2.5 or more per accelerator card compared to a standard medium-sized dual socket node. As we have seen, some features of accelerators have been carried over to traditional CPUs in the form of new instructions supporting vector and nowadays even matrix computing, and in some cases they may just be the better choice as CPUs have more memory readily available and as programming is easier.","title":"Too much hype"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#problems-and-solutions","text":"There are several problems with current GPU designs: The amount of memory that a GPU can address directly and has fast enough access to, is limited. 2020 GPUs were limited to 32-48 GB of memory, in early 2021 a 80 GB GPU appeared on the market, and in 2022 it is technically possible to have 128 GB on a single package. But this is still small to typical memory sizes on a regular dual socket server that is much slower than the GPU. Programming bottleneck: Having to organise all data transport manually and working with separate memory spaces is a pain. The link between the CPU and the GPU is a bottleneck. The PCIe buss that typically links the CPU to the GPU has a rather limited bandwidth compared to either the bandwidth of the CPU memory of the bandwidth of GPU memory. The GPU is rather slow for serial code, so that code often has to run on the host. Which is then an issue since it may require additional copying between the CPU and GPU. However, there is hope. The amount of memory that can be used by a GPU will increase a lot the coming years. Both the memory packages are getting bigger by stacking more dies in 3D, and the number of memory packages that can be integrated in the overall GPU package is increasing. As of 2022, 8 memory stacks in a GPU package is feasible as is shown by the AMD MI250(X) GPUs, while in 2023-2024 12 memory stacks in a single GPU package should become a possibility. The two combined may make memory sizes of 192 and likely even 384 GB possible by 2024 or 2025. The programming bottleneck can already be partially solved by unified memory, using memory pointers that work on both CPU and GPU, and further hardware support for virtual memory that can then trigger software that migrates memory pages under the hood. NVIDIA GPUs have had some of those features since the Pascal generation in 2017. However, one can do even better by physically sharing memory spaces between GPU and CPU, and supporting some level of coherency so that the CPU can access the GPU memory without risk of inconsistent data, or even the GPU can access the memory of the CPU, though that is less interesting as the CPU can never provide the memory bandwidth that the GPU needs to perform well. NUMA style shared memory spaces were first explored in the Sierra and Summit USA pre-exascale systems (as we discussed before) but is now also seen in the MI250X GPU from AMD which is a special version of the MI200 family connecting to the CPU through InfinityFabric, the same interconnect that AMD uses internally in its sockets to link the CPU dies to the I/O die, and also uses to connect CPU sockets or to connect GPUs to each other in the MI100 and MI200 generations. The Intel Ponte Vecchio GPU combined with the Sapphire Rapids CPU that will be used in the Aurora supercomputer supports a similar feature, as does the NVIDIA Grace CPU and Hopper GPU integrated in a single package. The physical sharing of memory spaces with a level of cache coherency is also the first step in solving the problem of copying data back and forth all the time. E.g., if a CPU can access memory attached to the GPU without risks of coherency problems, then there is less need to copy full memory pages and also not to copy those back, as the link that is used in those GPU systems to connect the CPU to GPU is as fast as the links that are typically used to connect two CPU sockets. The NVIDIA Grace Hopper \"superchip\" shows the next step. By integrating the CPU and GPU in the same package, it is possible to have a much higher bandwidth between both, reducing the copying bottleneck in cases where copying is still needed. However, with the AMD MI300 and Intel Falcon Shores and without doubt a future unannounced NVIDIA product, we will see even closer integration where CPU and GPU chiplets share the memory controllers. The Apple M series chips give an indication of what can be obtained with such a system, as these systems perform way better in some applications that use acceleration than one would expect from looking at systems with discrete GPUs with similar theoretical performance. We will discuss this evolution in some more detail in the next section.","title":"Problems and solutions"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#evolution-of-gpu-nodes","text":"","title":"Evolution of GPU nodes"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#gpu-subsystem-connected-via-pcie","text":"Around 2016, a typical GPU compute node consisted of a dual socket server with 1-4 GPUs attached to the CPUs. A typical design would have been: The red line between the two CPUs denotes a fully cache coherent link between the CPUs. In 2016 this would very likely have been either Intel CPUs or IBM POWER CPUs, and both had proprietary fully cache coherent links to link CPUs in a shared memory system. The red link between the GPUs denotes a similarly proprietary connection between the CPUs for easy data transfer between the CPUs at typically a much higher bandwidth than that offered by the connections between the CPU and GPU. However, not all systems used such a link between graphics cards. A CPU was connected to the GPUs using PCIe, and similarly a network interface would also be connected to a CPU using PCIe. Later designs tried to move the network interconnect closer to the GPUs, as they perform the majority of the calculations and hence also contain the data that should be send to other nodes. A typical 4-GPU node based on the NVIDIA Ampere A100 GPU launched in 2020 would look similar to: There are many variants of quad GPU designs with the A100 GPU, with single and dual socket CPU servers. However, it is often advantageous to have all GPUs connected to a single CPU, but only the single socket AMD Epyc CPU has enough PCIe connections to do so and still be able to also attach one or two network interface cards. The above design solves this in a different way. It uses two PCIe switches (the magenta circles), and each PCIe switch connects the CPU to two of the GPUs and a network interface card (the latter denoted by the dark purple line). This also brings the network very close to the GPU.","title":"GPU subsystem connected via PCIe"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#cache-coherent-interconnect-between-cpu-and-gpu","text":"The next evolution of this design is used in the USA pre-exascale systems Sierra and Summit, both using IBM POWER9 CPUs and NVIDIA Volta V100 GPUs. More recently, the idea is revived in the GPU compute nodes of the Frontier and LUMI supercomputers based on the MI250X GPU. A simplified diagram of the LUMI and Frontier GPU nodes is: The GPU compute nodes of LUMI and Frontier use a special variant of the Zen3-based AMD Epyc processor. In this variant, the PCIe lanes are replaced by InfinityFabric connections. In an MI250X, the 4 GPU packages are connected with each other and with the CPU through Infinity Fabric links. Each GPU package connects to its own quarter of the AMD Epyc processor (remember from earlier that the I/O die is subdivided in 4 quarters, in this case each connected to two CPU chiplets with 8 cores each). This creates a coherent memory space. Also noteworthy is that the interconnect is no longer connected to the CPU, but the 4 network cards are each connected to a GPU package (through a PCIe interface). This makes this compute node really a GPU-first system, almost a system where the CPU is only used for those parts of the code that cannot be accelerated at all by a GPU and to run the Linux operating system. The true picture of the MI250X GPU node is a bit more complicated though. Each GPU package contains two GPU dies, and these are connected to each other through some Infinity Fabric links. Each GPU die connects to 4 memory packages, with 64 GB of memory per GPU die. However, the connection between two GPU dies is sufficiently bandwidth-starved that programming wise a single GPU package should be considered as two separate GPUs. Each individual GPU die has its own InfinityFabric link to the CPU and seems to have a preferential CPU chiplet. Even though the connection between the GPU packages appears to be an all-to-all connection, this is not true when one looks at the connections between the GPU dies.","title":"Cache-coherent interconnect between CPU and GPU"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#integration-of-cpu-and-gpu-in-a-single-package-nvidia-gh100","text":"Even though the MI250X has a level of cache coherency, using the memory in a unified matter is still a problem, partly because of the extra latency introduced by the fairly long distance between the CPU and GPU, partly also because of the limited memory bandwidth on the CPU side. NVIDIA's Grace Hopper Superchip, expected in 2023, works on those two problems by integrating the CPU and GPU on a single package and not only putting the GPU memory, but also the CPU memory on that package. The CPU part is called Grace and is a CPU based on the ARMv9 architecture, the newest version of the ARM architecture at the time of design of the Grace processor. The GPU part of the package is the Hopper architecture and similar to the one in the SXM and PCIe cards released in the fall of 2022, but with all 6 memory controllers enabled. The CPU and GPU still both have their own memory controllers and basically behave as separate NUMA domains, but as the connection between the two has been brought on-chip the bandwidth between CPU and GPU is a lot higher than in the MI250X architecture OR the Summit and Sierra systems with IBM POWER9 and NVIDIA V100 chips. The CPU memory is not provided through external DIMMs, but through a number of internal LPDDR5X modules integrated in the CPU-GPU package in a similar way as the GPU memory has been integrated in the package for a number of generations already. Integration of this memory type is popular in smartphones where it saves both space and power, and is also used in the Apple Silicon M-series chips, where in addition to space and power savings it also provides higher bandwidth. In the Grace chip it enables a very high memory bandwidth for the CPU, even 20% better than what the AMD EPYC 4 generation offers per socket (but not as much as the Intel Sapphire Rapids MAX chips that incorporate a small amount of GPU-style memory in the package) while still offering a high memory capacity. Pre-release material claims up to 512 GB per CPU, but this number has to be taken with a grain of salt as it does not correspond to the available memory modules that can be found in catalogues of memory providers in early 2023. The Grace Hopper superchip provides two types of external connections. There are a number of regular PCIe connections that come from the CPU die. They can be used to attach, e.g., network cards (including a HPC interconnect) and NVMe drives. There are also a number of NVLINK connections coming from the GPU die. These connections are similar to the ones already used in previous generation NVIDIA GPUs to link GPU packages and offer a much higher bandwidth interconnect. They can be used to interconnect a number of Grace Hopper superchips in a NUMA shared memory way. This makes the Grace Hopper superchip a very flexible building block. Depending on the needs those chips can be combined in different ways. One can connect individual single package nodes through any HPC interconnect in a distributed memory way. It is also possible to use NVLINK technology to connect multiple superchips into a single system where each CPU and each GPU appears as a NUMA node to the OS. The bandwidth of this connection is much higher than the typical inter-socket interconnect in CPU-based servers, but still a lot lower than the memory bandwidth that the GPU memory system can offer. So it is very important that applications exploit the NUMA structure of the memory. It is also possible to combine both approaches: Build supercomputer nodes with up to 8 superchips with a single NVLINK switch level, and link those nodes together using a traditional HPC interconnect. A possible layout of such a node is shown in the following figure: In this figure we combine 4 Grace Hopper packages in a single node and have chose to connect each package directly to the interconnect for additional bandwidth.","title":"Integration of CPU and GPU in a single package: NVIDIA GH100"},{"location":"8_Accelerators/8_05_Status_GPU_computing/#fully-unified-cpu-and-gpu-amd-mi300-and-intel-falcon-shores","text":"The AMD MI250X is really just a transition to the MI300 series and the Intel Falcon Shores architecture, that go one step further beyond the integration that the NVIDIA Grace Hopper architecture offers. In that generation, expected to come to market in late 2023 in the case of AMD and 2024 or 2025 in the case of Intel, the CPU and GPU will merge completely and share on-package memory. In fact, the reality is that memory outside the package is also starting to limit CPU performance as an increasing number of CPU codes becomes memory bandwidth bound, so even for the CPU it makes sense to switch to smaller but much higher bandwidth memory in the package. The AMD MI300 and Intel Falcon Shores will fully integrate the CPU and GPU chiplets and memory controllers with memory in a single package. Whereas the MI250x has cache coherent memory but still a lot of overhead when the GPU wants to access CPU memory or the other way around, in the MI300 generation the CPU and GPU memory is fully unified (physical and virtual), with both sharing the same memory controllers and memory, which will enable to fully eliminate redundant memory copies. MI300 was first mentioned at the AMD Financial Analyst Day in June 2022 and at CES'2023 (early January) , where a full package was shown, but still with very little detail. It was announced the one MI300 package will combine a powerful GPU with 24 CPU cores (presumably 3 Zen4 chiplets) and 8 HBM3 modules for a total of 128 GB RAM. The chip will consists of 13 chiplets stacked in two layers, with 4 chiplets at the bottom (presumably the memory controllers as they produce less heat) and 9 chiplets at the top (which could be 3 CPU chiplets and 6 CDNA3 GPU dies?). A supercomputer node based on this chip could look a bit like Here we see four packages integrating one or more CPU chiplets, one or more GPU dies and memory in a single packages. The four packages have an all-to-all connection likely using a new generation of InfinityFabric, and each GPU packages also connects to a network card using PCIe. It is expected that the techniques to connect dies will have evolved enough that the GPU dies in a single package will work as a single GPU. In fact, those improved connections will also be needed to have equal access to memory from the GPU and CPU chiplets. It does look though that the total memory capacity of such a node may be rather limited, unless there would be some unannounced feature to attach slower external memory, e.g., through additional DIMM slots or through (even slower) CXL memory boards. But for applications that don't need those large memory capacities and scale nicely over NUMA domains and then further over distributed memory nodes, the more uniform architecture will certainly make life easier and offer great performance benefits.","title":"Fully unified CPU and GPU: AMD MI300 and Intel Falcon Shores"},{"location":"8_Accelerators/8_06_Further_reading/","text":"Further reading \u00b6 AMD GPUs Whitepaper: Introducing AMD CDNA TM 2 Architecture which includes information on how the MI250X connects to the CPU, and on the vector and matrix cores of the GPU. HPCwire article on the AMD MI300 APU that will be used in the El Capitan supercomputer. AnandTech article on the MI300 , based on information from AMD's 2022 Investors Day. The article shows some not very detailed slides about the memory architecture. GPU comuting at the AMD Financial Analyst Day 2022 on YouTube (not an official AMD recording) . The CDNA3 part starts at 1:07:50. MI300 @ AMD CES'2023 keynote (YouTube) . The MI300 part starts at 1:29:42. NVIDIA GPUs Whitepaper: Summit and Sierra Supercomputers: An Inside Look at the U.S. Department of Energy\u2019s New Pre-Exascale Systems for information on the physically unified memory space made possible by the NVLink connections between the CPUs and GPUs. Whitepaper: NVIDIA Grace Hopper Superchip Architecture for all information on the combination of the Grace ARM-based CPU and the Hopper GPUs. NVIDIA CUDA NVIDIA CUDA toolkit AMD ROCm AMD ROCm information portal AMD HIP fundamentals HIP API Guides Intel oneAPI Intel oneAPI overview OpenCL Khronos Group OpenCL documentation OpenACC The OpenACC Organization Clacc, OpenACC in LLVM project page OpenMP OpenMP Architecture Review Board web site SYCL Khronos Group SYCL documentation hipSYCL Intel oneAPI DPC++/C++ compiler Intel oneAPI DPC++ compiler GitHub Kokkos Kokkos ecosystem home page RAJA framework RAJA home page RAJA documentation Alpaka abstraction library Alpaka information on the CASUS web site Alpaka information on GitHub Alpaka training from 2020 on YouTube Libraries MAGMA heFFTe","title":"Further reading"},{"location":"8_Accelerators/8_06_Further_reading/#further-reading","text":"AMD GPUs Whitepaper: Introducing AMD CDNA TM 2 Architecture which includes information on how the MI250X connects to the CPU, and on the vector and matrix cores of the GPU. HPCwire article on the AMD MI300 APU that will be used in the El Capitan supercomputer. AnandTech article on the MI300 , based on information from AMD's 2022 Investors Day. The article shows some not very detailed slides about the memory architecture. GPU comuting at the AMD Financial Analyst Day 2022 on YouTube (not an official AMD recording) . The CDNA3 part starts at 1:07:50. MI300 @ AMD CES'2023 keynote (YouTube) . The MI300 part starts at 1:29:42. NVIDIA GPUs Whitepaper: Summit and Sierra Supercomputers: An Inside Look at the U.S. Department of Energy\u2019s New Pre-Exascale Systems for information on the physically unified memory space made possible by the NVLink connections between the CPUs and GPUs. Whitepaper: NVIDIA Grace Hopper Superchip Architecture for all information on the combination of the Grace ARM-based CPU and the Hopper GPUs. NVIDIA CUDA NVIDIA CUDA toolkit AMD ROCm AMD ROCm information portal AMD HIP fundamentals HIP API Guides Intel oneAPI Intel oneAPI overview OpenCL Khronos Group OpenCL documentation OpenACC The OpenACC Organization Clacc, OpenACC in LLVM project page OpenMP OpenMP Architecture Review Board web site SYCL Khronos Group SYCL documentation hipSYCL Intel oneAPI DPC++/C++ compiler Intel oneAPI DPC++ compiler GitHub Kokkos Kokkos ecosystem home page RAJA framework RAJA home page RAJA documentation Alpaka abstraction library Alpaka information on the CASUS web site Alpaka information on GitHub Alpaka training from 2020 on YouTube Libraries MAGMA heFFTe","title":"Further reading"},{"location":"9_Conclusions/","text":"Conclusions \u00b6 Cost-concious computing \u00b6 Supercomputing does not come cheap. Some really big runs run on bigger supercomputers may easily cost 10k EURO. Academic users typically don't see the full bill. They either pay a minor contribution, or have to submit a proposal to get the compute time, but someone has to pay the bill. Given the high cost, it is clear that a supercomputer should be used efficiently. Unfortunately, using a supercomputer efficiently is not that trivial. But as we have seen, even your work that you do on a regular PC or workstation may benefit from it as after all, a modern PC has all the levels of parallelism in a supercomputer with the exception of the distributed memory level and probably the NUMA characteristic of the memory. It is important to realise that a supercomputer is not a machine that wil automatically and in a magical way run every program faster than a PC, though at first it may look like there is a high degree of compatibility with Linux PCs. (And that even works in the other direction: it is not hard to install some of the middleware used on a supercomputer also on your PC to, e.g., test distributed memory programs.) A supercomputer is no excuse for lousy programming. One needs to first optimise a program and then, if the cheaper hardware is still not capable of running it properly, move to a (larger) supercomputer, and not the other way around, first move to a supercomputer and then if it still doesn't work think about developing better code. Cost-concious computing as a software user \u00b6 It is important to select the software that you use with care and to follow the evolutions in your field. Hardware evolves, and some software packages evolve with the hardware while others stay behind. EuroHPC, the European initiative for supercomputer, adn the USA funding agencies and in particular the national labs, invest a lot of resources into improving or rewriting popular scientific software packages, and these are often available as open source or free to license for academic research. The most hyped package or technology may not always be the best one for your needs. But the package that your Ph.D. advisor used 25 years ago for their research may not be adequate anymore either. It is also important to learn to use the software packages efficiently, with a balance between execution efficiency and time-to-solution. Most packages, and this is even more true for the free ones, have no auto-tune facility. Users need to do that work instead. For a numerical simulation it is also important to understand the limits of the models (accuracy-wise) and of the numerics. Asking for more precision than makes sense given the errors in the model and the stability of the numerical algorithms may come at a very high cost or even failed runs. Cost-concious computing as a developer \u00b6 Prototype languages such as Matlab are really meant for prototyping algorithms but not for production runs at scale, though the situations is somewhat improving with some languages having decent just-in-time compilation features or being more designed for efficient execution on modern hardware than only easy of programming. Julia is a modern language developed at MIT that offers much of the ease-of-use of Matlab but where being able to compile the program to efficient computer code was taken into account when designing the language. In fact, some users who have translated existing code to Julia mention that they hardly lost speed compared to Fortran while their Matlab and Python codes run orders of magnitude faster when rewritten in Julia. The same holds for languages meant for scripting, with Python the most used one in scientific computing. Such languages were never designed for efficiency and for tasks that execute continuously and require a lot of CPU time. Python can be acceptable when it is used to bind modules together that themselves each are highly optimised C/C++ code, but it is very hard to get pure Python code to work efficiently. There have been several efforts to try to write just-in-time compilers that can increase the efficiency of pure Python code, but none of the efforts has succeeded in making one that is fairly universal. An example that is somewhat popular in scientific computing is Numba , that seems to work well especially with code that uses data structures from NumPy. (Numba is in fact based on the LLVM compiler back-end that was mentioned so often in these nodes.) Here again, Julia can be a nice alternative. Why should you care? \u00b6 A typical reaction from a Ph.D. student could be: \"Why should I care? I need to write my Ph.D. as fast as possible, produce papers, and by the way, it's (almost) free, isn't it?\" Resources are not infinite and most clusters run at a fairly high load. On bigger machines, a user typically gets a certain allocation (in terms of core-hours etc.) and can only use that much. On smaller supercomputers, at the level of a university, a fair share policy is often implemented to come to a somewhat reasonable distribution of resources. A fair share policy typically implies that users or groups who have used a lot of computer time recently, will get a lower priority. Few if no supercomputers will use a first come, first served policy which would allow you to simply dump a lot of work onto the machine and not care at all. Moreover, funding agencies in the end have to pay the bill and they will also look at the outcome of the research done on a supercomputer before deciding to fund the next one. So the user community can better optimise the scientific return and often economic return of a supercomputer to ensure funding for the next machine. The way in which research can be made usable for companies is becomeing more and more important. And for companies, compute time comes at a high cost so they are not often interested in using inefficient procedures to solve their problems. We also live in a time where energy comes with an increasing cost. One should realise that supercomputers do consume a lot of energy. A supercomputer consuming only 100 kW is still small, and the biggest machines on earth at the moment are closer to 30 MW. The supercomputer world is looking at many ways to lower the energy consumption. Software that uses hardware more efficiently or uses more efficient hardware (such as accelerators) is often the easiest route to go... Hardware designers have already developed ways to cool supercomputers with water that is so warm even when it enters the computer that \"free cooling\" can be used to cool the hot water that leaves the computer, i.e., just radiators and fans to pass the air over the radiator. Some centres are experimenting with heat recovery for the purpose of heating their buildings. The waste hear of LUMI, a European pre-exascale machine installed in Kajaani, Finland, is used to produce heat for the city of Kajaani . Another reaction one sometimes hears is \"But by the time I've reworked my code or rolled out a better application in my research, faster computers will be around so I don't need to care anymore.\" This was to some extent true from roughly 1980 to 2005 if you look at personal computers. It was as if Intel engineers were optimising your program while you were having lunch or going to a party, by designing better, faster processors. In those days, code would run considerably faster on a new processor even without recompiling, though recompiling would improve that even more. However, these days are long over. Around 2005, further increasing the clock speed became very hard. We've moved from 4 MHz in 1980 to almost 4 GHz in 2005, but from then on evolution slowed down (and took even a step back for a while) and now we are somewhere around 5.5 GHz for processors that are really optimised for sequential code and consume a lot of power. It also became harder and harder to get gains from increased instruction level parallelism. Instead of that, we saw growing popularity of vector instructions and the use of multiple cores. 8 to 16 cores is not uncommon anymore in regular PC's, and the first 24-core PC processor was announced in late 2022. So the further speed increases almost exclusively came from other levels of parallelism that require more effort from the programmer: vector instructions and more cores in shared memory computing, and in supercomputing, more nodes in distributed memory. Nowadays even that is slowing down. The number of transistors that can be put on a chip is no longer growing as fast, the power consumption per transistor is no longer decreasing much with every new generation, and the cost of a transistor isn't really decreasing anymore, but in fact is starting to increase again when moving to a smaller manufacturing process. The focus in computing is more and more on performance per Watt as energy bills become prohibitively high for supercomputer centres and centres operating large server farms or as portability matters as in smartphones, where you want more and more performance while the battery capacity isn't increasing that quickly. This implies more but slower cores, so more need to exploit parallelism and less single-thread performance. There are only a few markets that don't care much about this, but those markets are becoming too small to carry the cost of designing faster and faster processors with limited parallelism.","title":"Conclusions"},{"location":"9_Conclusions/#conclusions","text":"","title":"Conclusions"},{"location":"9_Conclusions/#cost-concious-computing","text":"Supercomputing does not come cheap. Some really big runs run on bigger supercomputers may easily cost 10k EURO. Academic users typically don't see the full bill. They either pay a minor contribution, or have to submit a proposal to get the compute time, but someone has to pay the bill. Given the high cost, it is clear that a supercomputer should be used efficiently. Unfortunately, using a supercomputer efficiently is not that trivial. But as we have seen, even your work that you do on a regular PC or workstation may benefit from it as after all, a modern PC has all the levels of parallelism in a supercomputer with the exception of the distributed memory level and probably the NUMA characteristic of the memory. It is important to realise that a supercomputer is not a machine that wil automatically and in a magical way run every program faster than a PC, though at first it may look like there is a high degree of compatibility with Linux PCs. (And that even works in the other direction: it is not hard to install some of the middleware used on a supercomputer also on your PC to, e.g., test distributed memory programs.) A supercomputer is no excuse for lousy programming. One needs to first optimise a program and then, if the cheaper hardware is still not capable of running it properly, move to a (larger) supercomputer, and not the other way around, first move to a supercomputer and then if it still doesn't work think about developing better code.","title":"Cost-concious computing"},{"location":"9_Conclusions/#cost-concious-computing-as-a-software-user","text":"It is important to select the software that you use with care and to follow the evolutions in your field. Hardware evolves, and some software packages evolve with the hardware while others stay behind. EuroHPC, the European initiative for supercomputer, adn the USA funding agencies and in particular the national labs, invest a lot of resources into improving or rewriting popular scientific software packages, and these are often available as open source or free to license for academic research. The most hyped package or technology may not always be the best one for your needs. But the package that your Ph.D. advisor used 25 years ago for their research may not be adequate anymore either. It is also important to learn to use the software packages efficiently, with a balance between execution efficiency and time-to-solution. Most packages, and this is even more true for the free ones, have no auto-tune facility. Users need to do that work instead. For a numerical simulation it is also important to understand the limits of the models (accuracy-wise) and of the numerics. Asking for more precision than makes sense given the errors in the model and the stability of the numerical algorithms may come at a very high cost or even failed runs.","title":"Cost-concious computing as a software user"},{"location":"9_Conclusions/#cost-concious-computing-as-a-developer","text":"Prototype languages such as Matlab are really meant for prototyping algorithms but not for production runs at scale, though the situations is somewhat improving with some languages having decent just-in-time compilation features or being more designed for efficient execution on modern hardware than only easy of programming. Julia is a modern language developed at MIT that offers much of the ease-of-use of Matlab but where being able to compile the program to efficient computer code was taken into account when designing the language. In fact, some users who have translated existing code to Julia mention that they hardly lost speed compared to Fortran while their Matlab and Python codes run orders of magnitude faster when rewritten in Julia. The same holds for languages meant for scripting, with Python the most used one in scientific computing. Such languages were never designed for efficiency and for tasks that execute continuously and require a lot of CPU time. Python can be acceptable when it is used to bind modules together that themselves each are highly optimised C/C++ code, but it is very hard to get pure Python code to work efficiently. There have been several efforts to try to write just-in-time compilers that can increase the efficiency of pure Python code, but none of the efforts has succeeded in making one that is fairly universal. An example that is somewhat popular in scientific computing is Numba , that seems to work well especially with code that uses data structures from NumPy. (Numba is in fact based on the LLVM compiler back-end that was mentioned so often in these nodes.) Here again, Julia can be a nice alternative.","title":"Cost-concious computing as a developer"},{"location":"9_Conclusions/#why-should-you-care","text":"A typical reaction from a Ph.D. student could be: \"Why should I care? I need to write my Ph.D. as fast as possible, produce papers, and by the way, it's (almost) free, isn't it?\" Resources are not infinite and most clusters run at a fairly high load. On bigger machines, a user typically gets a certain allocation (in terms of core-hours etc.) and can only use that much. On smaller supercomputers, at the level of a university, a fair share policy is often implemented to come to a somewhat reasonable distribution of resources. A fair share policy typically implies that users or groups who have used a lot of computer time recently, will get a lower priority. Few if no supercomputers will use a first come, first served policy which would allow you to simply dump a lot of work onto the machine and not care at all. Moreover, funding agencies in the end have to pay the bill and they will also look at the outcome of the research done on a supercomputer before deciding to fund the next one. So the user community can better optimise the scientific return and often economic return of a supercomputer to ensure funding for the next machine. The way in which research can be made usable for companies is becomeing more and more important. And for companies, compute time comes at a high cost so they are not often interested in using inefficient procedures to solve their problems. We also live in a time where energy comes with an increasing cost. One should realise that supercomputers do consume a lot of energy. A supercomputer consuming only 100 kW is still small, and the biggest machines on earth at the moment are closer to 30 MW. The supercomputer world is looking at many ways to lower the energy consumption. Software that uses hardware more efficiently or uses more efficient hardware (such as accelerators) is often the easiest route to go... Hardware designers have already developed ways to cool supercomputers with water that is so warm even when it enters the computer that \"free cooling\" can be used to cool the hot water that leaves the computer, i.e., just radiators and fans to pass the air over the radiator. Some centres are experimenting with heat recovery for the purpose of heating their buildings. The waste hear of LUMI, a European pre-exascale machine installed in Kajaani, Finland, is used to produce heat for the city of Kajaani . Another reaction one sometimes hears is \"But by the time I've reworked my code or rolled out a better application in my research, faster computers will be around so I don't need to care anymore.\" This was to some extent true from roughly 1980 to 2005 if you look at personal computers. It was as if Intel engineers were optimising your program while you were having lunch or going to a party, by designing better, faster processors. In those days, code would run considerably faster on a new processor even without recompiling, though recompiling would improve that even more. However, these days are long over. Around 2005, further increasing the clock speed became very hard. We've moved from 4 MHz in 1980 to almost 4 GHz in 2005, but from then on evolution slowed down (and took even a step back for a while) and now we are somewhere around 5.5 GHz for processors that are really optimised for sequential code and consume a lot of power. It also became harder and harder to get gains from increased instruction level parallelism. Instead of that, we saw growing popularity of vector instructions and the use of multiple cores. 8 to 16 cores is not uncommon anymore in regular PC's, and the first 24-core PC processor was announced in late 2022. So the further speed increases almost exclusively came from other levels of parallelism that require more effort from the programmer: vector instructions and more cores in shared memory computing, and in supercomputing, more nodes in distributed memory. Nowadays even that is slowing down. The number of transistors that can be put on a chip is no longer growing as fast, the power consumption per transistor is no longer decreasing much with every new generation, and the cost of a transistor isn't really decreasing anymore, but in fact is starting to increase again when moving to a smaller manufacturing process. The focus in computing is more and more on performance per Watt as energy bills become prohibitively high for supercomputer centres and centres operating large server farms or as portability matters as in smartphones, where you want more and more performance while the battery capacity isn't increasing that quickly. This implies more but slower cores, so more need to exploit parallelism and less single-thread performance. There are only a few markets that don't care much about this, but those markets are becoming too small to carry the cost of designing faster and faster processors with limited parallelism.","title":"Why should you care?"}]}